{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auther: Tzu-Han Hsu\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) 2022, Anywhere Door Lab (ADL) and Tzu-Han Hsu\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "#    list of conditions and the following disclaimer.\n",
    "\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "#    this list of conditions and the following disclaimer in the documentation\n",
    "#    and/or other materials provided with the distribution.\n",
    "\n",
    "# 3. Neither the name of the copyright holder nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived from\n",
    "#    this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientNetb5_stn1 with 101 classes running on: caltech101\n",
      "Dataset size: Train: 6277, Valid: 1200, Test: 1200\n",
      "{'Faces': 0, 'Faces_easy': 1, 'Leopards': 2, 'Motorbikes': 3, 'accordion': 4, 'airplanes': 5, 'anchor': 6, 'ant': 7, 'barrel': 8, 'bass': 9, 'beaver': 10, 'binocular': 11, 'bonsai': 12, 'brain': 13, 'brontosaurus': 14, 'buddha': 15, 'butterfly': 16, 'camera': 17, 'cannon': 18, 'car_side': 19, 'ceiling_fan': 20, 'cellphone': 21, 'chair': 22, 'chandelier': 23, 'cougar_body': 24, 'cougar_face': 25, 'crab': 26, 'crayfish': 27, 'crocodile': 28, 'crocodile_head': 29, 'cup': 30, 'dalmatian': 31, 'dollar_bill': 32, 'dolphin': 33, 'dragonfly': 34, 'electric_guitar': 35, 'elephant': 36, 'emu': 37, 'euphonium': 38, 'ewer': 39, 'ferry': 40, 'flamingo': 41, 'flamingo_head': 42, 'garfield': 43, 'gerenuk': 44, 'gramophone': 45, 'grand_piano': 46, 'hawksbill': 47, 'headphone': 48, 'hedgehog': 49, 'helicopter': 50, 'ibis': 51, 'inline_skate': 52, 'joshua_tree': 53, 'kangaroo': 54, 'ketch': 55, 'lamp': 56, 'laptop': 57, 'llama': 58, 'lobster': 59, 'lotus': 60, 'mandolin': 61, 'mayfly': 62, 'menorah': 63, 'metronome': 64, 'minaret': 65, 'nautilus': 66, 'octopus': 67, 'okapi': 68, 'pagoda': 69, 'panda': 70, 'pigeon': 71, 'pizza': 72, 'platypus': 73, 'pyramid': 74, 'revolver': 75, 'rhino': 76, 'rooster': 77, 'saxophone': 78, 'schooner': 79, 'scissors': 80, 'scorpion': 81, 'sea_horse': 82, 'snoopy': 83, 'soccer_ball': 84, 'stapler': 85, 'starfish': 86, 'stegosaurus': 87, 'stop_sign': 88, 'strawberry': 89, 'sunflower': 90, 'tick': 91, 'trilobite': 92, 'umbrella': 93, 'watch': 94, 'water_lilly': 95, 'wheelchair': 96, 'wild_cat': 97, 'windsor_chair': 98, 'wrench': 99, 'yin_yang': 100}\n",
      "torch.Size([3, 456, 456])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "from math import ceil\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#vital params\n",
    "\n",
    "\n",
    "dataset_name=\"caltech101\"\n",
    " \n",
    "model_name=\"EfficientNetb5_stn1\"\n",
    "version = \"b5\"\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "phi, res, drop_rate = phi_values[version]\n",
    "#hyperparameters\n",
    "batch_size=10\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "image_size=(res,res)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "elif dataset_name == \"caltech101\":\n",
    "    num_classes=101\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n",
    "\n",
    "\n",
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "elif dataset_name == \"caltech101\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/caltech101\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [7477, 1200])\n",
    "    train_set, test_set = data.random_split(train_set,[6277, 1200])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "    \n",
    "    \n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "print(head_train_set.class_to_idx)\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b5d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU() # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4, # squeeze excitation\n",
    "            survival_prob=0.8, # for stochastic depth\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,\n",
    "            ),\n",
    "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            return self.stochastic_depth(self.conv(x)) + inputs\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = ceil(1280 * width_factor)\n",
    "        self.localization = nn.Sequential(\n",
    "            #nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.Conv2d(3,8,kernel_size=3),\n",
    "            nn.Conv2d(8,8,kernel_size=3),\n",
    "            nn.Conv2d(8,8,kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            \n",
    "            #nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.Conv2d(8,10, kernel_size = 3),\n",
    "            nn.Conv2d(10,10,kernel_size = 3),\n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10*110*110, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10*110*110)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        \n",
    "        \n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate\n",
    "\n",
    "    def create_features(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4*ceil(int(channels*width_factor) / 4)\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride = stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size//2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.stn(x)\n",
    "        x = self.pool(self.features(x))\n",
    "        \n",
    "        return self.classifier(x.view(x.shape[0], -1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee819dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.SiLU'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.CNNBlock'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Sigmoid'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.SqueezeExcitation'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.InvertedResidualBlock'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.EfficientNet'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[34mThe model requires: 42.0209 GFLOPS\n",
      "\u001b[0m\n",
      "torch.Size([10, 3, 456, 456])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 101])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 454, 454]             224\n",
      "            Conv2d-2          [-1, 8, 452, 452]             584\n",
      "            Conv2d-3          [-1, 8, 450, 450]             584\n",
      "         MaxPool2d-4          [-1, 8, 225, 225]               0\n",
      "              ReLU-5          [-1, 8, 225, 225]               0\n",
      "            Conv2d-6         [-1, 10, 223, 223]             730\n",
      "            Conv2d-7         [-1, 10, 221, 221]             910\n",
      "         MaxPool2d-8         [-1, 10, 110, 110]               0\n",
      "              ReLU-9         [-1, 10, 110, 110]               0\n",
      "           Linear-10                   [-1, 32]       3,872,032\n",
      "             ReLU-11                   [-1, 32]               0\n",
      "           Linear-12                    [-1, 6]             198\n",
      "           Conv2d-13         [-1, 46, 228, 228]           1,242\n",
      "      BatchNorm2d-14         [-1, 46, 228, 228]              92\n",
      "             SiLU-15         [-1, 46, 228, 228]               0\n",
      "         CNNBlock-16         [-1, 46, 228, 228]               0\n",
      "           Conv2d-17         [-1, 46, 228, 228]             414\n",
      "      BatchNorm2d-18         [-1, 46, 228, 228]              92\n",
      "             SiLU-19         [-1, 46, 228, 228]               0\n",
      "         CNNBlock-20         [-1, 46, 228, 228]               0\n",
      "AdaptiveAvgPool2d-21             [-1, 46, 1, 1]               0\n",
      "           Conv2d-22             [-1, 11, 1, 1]             517\n",
      "             SiLU-23             [-1, 11, 1, 1]               0\n",
      "           Conv2d-24             [-1, 46, 1, 1]             552\n",
      "          Sigmoid-25             [-1, 46, 1, 1]               0\n",
      "SqueezeExcitation-26         [-1, 46, 228, 228]               0\n",
      "           Conv2d-27         [-1, 24, 228, 228]           1,104\n",
      "      BatchNorm2d-28         [-1, 24, 228, 228]              48\n",
      "InvertedResidualBlock-29         [-1, 24, 228, 228]               0\n",
      "           Conv2d-30         [-1, 24, 228, 228]             216\n",
      "      BatchNorm2d-31         [-1, 24, 228, 228]              48\n",
      "             SiLU-32         [-1, 24, 228, 228]               0\n",
      "         CNNBlock-33         [-1, 24, 228, 228]               0\n",
      "AdaptiveAvgPool2d-34             [-1, 24, 1, 1]               0\n",
      "           Conv2d-35              [-1, 6, 1, 1]             150\n",
      "             SiLU-36              [-1, 6, 1, 1]               0\n",
      "           Conv2d-37             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-38             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-39         [-1, 24, 228, 228]               0\n",
      "           Conv2d-40         [-1, 24, 228, 228]             576\n",
      "      BatchNorm2d-41         [-1, 24, 228, 228]              48\n",
      "InvertedResidualBlock-42         [-1, 24, 228, 228]               0\n",
      "           Conv2d-43         [-1, 24, 228, 228]             216\n",
      "      BatchNorm2d-44         [-1, 24, 228, 228]              48\n",
      "             SiLU-45         [-1, 24, 228, 228]               0\n",
      "         CNNBlock-46         [-1, 24, 228, 228]               0\n",
      "AdaptiveAvgPool2d-47             [-1, 24, 1, 1]               0\n",
      "           Conv2d-48              [-1, 6, 1, 1]             150\n",
      "             SiLU-49              [-1, 6, 1, 1]               0\n",
      "           Conv2d-50             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-51             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-52         [-1, 24, 228, 228]               0\n",
      "           Conv2d-53         [-1, 24, 228, 228]             576\n",
      "      BatchNorm2d-54         [-1, 24, 228, 228]              48\n",
      "InvertedResidualBlock-55         [-1, 24, 228, 228]               0\n",
      "           Conv2d-56        [-1, 144, 228, 228]          31,104\n",
      "      BatchNorm2d-57        [-1, 144, 228, 228]             288\n",
      "             SiLU-58        [-1, 144, 228, 228]               0\n",
      "         CNNBlock-59        [-1, 144, 228, 228]               0\n",
      "           Conv2d-60        [-1, 144, 114, 114]           1,296\n",
      "      BatchNorm2d-61        [-1, 144, 114, 114]             288\n",
      "             SiLU-62        [-1, 144, 114, 114]               0\n",
      "         CNNBlock-63        [-1, 144, 114, 114]               0\n",
      "AdaptiveAvgPool2d-64            [-1, 144, 1, 1]               0\n",
      "           Conv2d-65              [-1, 6, 1, 1]             870\n",
      "             SiLU-66              [-1, 6, 1, 1]               0\n",
      "           Conv2d-67            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-68            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-69        [-1, 144, 114, 114]               0\n",
      "           Conv2d-70         [-1, 36, 114, 114]           5,184\n",
      "      BatchNorm2d-71         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-72         [-1, 36, 114, 114]               0\n",
      "           Conv2d-73        [-1, 216, 114, 114]          69,984\n",
      "      BatchNorm2d-74        [-1, 216, 114, 114]             432\n",
      "             SiLU-75        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-76        [-1, 216, 114, 114]               0\n",
      "           Conv2d-77        [-1, 216, 114, 114]           1,944\n",
      "      BatchNorm2d-78        [-1, 216, 114, 114]             432\n",
      "             SiLU-79        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-80        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-81            [-1, 216, 1, 1]               0\n",
      "           Conv2d-82              [-1, 9, 1, 1]           1,953\n",
      "             SiLU-83              [-1, 9, 1, 1]               0\n",
      "           Conv2d-84            [-1, 216, 1, 1]           2,160\n",
      "          Sigmoid-85            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-86        [-1, 216, 114, 114]               0\n",
      "           Conv2d-87         [-1, 36, 114, 114]           7,776\n",
      "      BatchNorm2d-88         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-89         [-1, 36, 114, 114]               0\n",
      "           Conv2d-90        [-1, 216, 114, 114]          69,984\n",
      "      BatchNorm2d-91        [-1, 216, 114, 114]             432\n",
      "             SiLU-92        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-93        [-1, 216, 114, 114]               0\n",
      "           Conv2d-94        [-1, 216, 114, 114]           1,944\n",
      "      BatchNorm2d-95        [-1, 216, 114, 114]             432\n",
      "             SiLU-96        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-97        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-98            [-1, 216, 1, 1]               0\n",
      "           Conv2d-99              [-1, 9, 1, 1]           1,953\n",
      "            SiLU-100              [-1, 9, 1, 1]               0\n",
      "          Conv2d-101            [-1, 216, 1, 1]           2,160\n",
      "         Sigmoid-102            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-103        [-1, 216, 114, 114]               0\n",
      "          Conv2d-104         [-1, 36, 114, 114]           7,776\n",
      "     BatchNorm2d-105         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-106         [-1, 36, 114, 114]               0\n",
      "          Conv2d-107        [-1, 216, 114, 114]          69,984\n",
      "     BatchNorm2d-108        [-1, 216, 114, 114]             432\n",
      "            SiLU-109        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-110        [-1, 216, 114, 114]               0\n",
      "          Conv2d-111        [-1, 216, 114, 114]           1,944\n",
      "     BatchNorm2d-112        [-1, 216, 114, 114]             432\n",
      "            SiLU-113        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-114        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 216, 1, 1]               0\n",
      "          Conv2d-116              [-1, 9, 1, 1]           1,953\n",
      "            SiLU-117              [-1, 9, 1, 1]               0\n",
      "          Conv2d-118            [-1, 216, 1, 1]           2,160\n",
      "         Sigmoid-119            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-120        [-1, 216, 114, 114]               0\n",
      "          Conv2d-121         [-1, 36, 114, 114]           7,776\n",
      "     BatchNorm2d-122         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-123         [-1, 36, 114, 114]               0\n",
      "          Conv2d-124        [-1, 216, 114, 114]          69,984\n",
      "     BatchNorm2d-125        [-1, 216, 114, 114]             432\n",
      "            SiLU-126        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-127        [-1, 216, 114, 114]               0\n",
      "          Conv2d-128        [-1, 216, 114, 114]           1,944\n",
      "     BatchNorm2d-129        [-1, 216, 114, 114]             432\n",
      "            SiLU-130        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-131        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-132            [-1, 216, 1, 1]               0\n",
      "          Conv2d-133              [-1, 9, 1, 1]           1,953\n",
      "            SiLU-134              [-1, 9, 1, 1]               0\n",
      "          Conv2d-135            [-1, 216, 1, 1]           2,160\n",
      "         Sigmoid-136            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-137        [-1, 216, 114, 114]               0\n",
      "          Conv2d-138         [-1, 36, 114, 114]           7,776\n",
      "     BatchNorm2d-139         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-140         [-1, 36, 114, 114]               0\n",
      "          Conv2d-141        [-1, 216, 114, 114]          69,984\n",
      "     BatchNorm2d-142        [-1, 216, 114, 114]             432\n",
      "            SiLU-143        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-144        [-1, 216, 114, 114]               0\n",
      "          Conv2d-145          [-1, 216, 57, 57]           5,400\n",
      "     BatchNorm2d-146          [-1, 216, 57, 57]             432\n",
      "            SiLU-147          [-1, 216, 57, 57]               0\n",
      "        CNNBlock-148          [-1, 216, 57, 57]               0\n",
      "AdaptiveAvgPool2d-149            [-1, 216, 1, 1]               0\n",
      "          Conv2d-150              [-1, 9, 1, 1]           1,953\n",
      "            SiLU-151              [-1, 9, 1, 1]               0\n",
      "          Conv2d-152            [-1, 216, 1, 1]           2,160\n",
      "         Sigmoid-153            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-154          [-1, 216, 57, 57]               0\n",
      "          Conv2d-155           [-1, 60, 57, 57]          12,960\n",
      "     BatchNorm2d-156           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-157           [-1, 60, 57, 57]               0\n",
      "          Conv2d-158          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-159          [-1, 360, 57, 57]             720\n",
      "            SiLU-160          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-161          [-1, 360, 57, 57]               0\n",
      "          Conv2d-162          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-163          [-1, 360, 57, 57]             720\n",
      "            SiLU-164          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-165          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-166            [-1, 360, 1, 1]               0\n",
      "          Conv2d-167             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-168             [-1, 15, 1, 1]               0\n",
      "          Conv2d-169            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-170            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-171          [-1, 360, 57, 57]               0\n",
      "          Conv2d-172           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-173           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-174           [-1, 60, 57, 57]               0\n",
      "          Conv2d-175          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-176          [-1, 360, 57, 57]             720\n",
      "            SiLU-177          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-178          [-1, 360, 57, 57]               0\n",
      "          Conv2d-179          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-180          [-1, 360, 57, 57]             720\n",
      "            SiLU-181          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-182          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-183            [-1, 360, 1, 1]               0\n",
      "          Conv2d-184             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-185             [-1, 15, 1, 1]               0\n",
      "          Conv2d-186            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-187            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-188          [-1, 360, 57, 57]               0\n",
      "          Conv2d-189           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-190           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-191           [-1, 60, 57, 57]               0\n",
      "          Conv2d-192          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-193          [-1, 360, 57, 57]             720\n",
      "            SiLU-194          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-195          [-1, 360, 57, 57]               0\n",
      "          Conv2d-196          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-197          [-1, 360, 57, 57]             720\n",
      "            SiLU-198          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-199          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-200            [-1, 360, 1, 1]               0\n",
      "          Conv2d-201             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-202             [-1, 15, 1, 1]               0\n",
      "          Conv2d-203            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-204            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-205          [-1, 360, 57, 57]               0\n",
      "          Conv2d-206           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-207           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-208           [-1, 60, 57, 57]               0\n",
      "          Conv2d-209          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-210          [-1, 360, 57, 57]             720\n",
      "            SiLU-211          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-212          [-1, 360, 57, 57]               0\n",
      "          Conv2d-213          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-214          [-1, 360, 57, 57]             720\n",
      "            SiLU-215          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-216          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-217            [-1, 360, 1, 1]               0\n",
      "          Conv2d-218             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-219             [-1, 15, 1, 1]               0\n",
      "          Conv2d-220            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-221            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-222          [-1, 360, 57, 57]               0\n",
      "          Conv2d-223           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-224           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-225           [-1, 60, 57, 57]               0\n",
      "          Conv2d-226          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-227          [-1, 360, 57, 57]             720\n",
      "            SiLU-228          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-229          [-1, 360, 57, 57]               0\n",
      "          Conv2d-230          [-1, 360, 29, 29]           3,240\n",
      "     BatchNorm2d-231          [-1, 360, 29, 29]             720\n",
      "            SiLU-232          [-1, 360, 29, 29]               0\n",
      "        CNNBlock-233          [-1, 360, 29, 29]               0\n",
      "AdaptiveAvgPool2d-234            [-1, 360, 1, 1]               0\n",
      "          Conv2d-235             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-236             [-1, 15, 1, 1]               0\n",
      "          Conv2d-237            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-238            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-239          [-1, 360, 29, 29]               0\n",
      "          Conv2d-240          [-1, 120, 29, 29]          43,200\n",
      "     BatchNorm2d-241          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-242          [-1, 120, 29, 29]               0\n",
      "          Conv2d-243          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-244          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-245          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-246          [-1, 720, 29, 29]               0\n",
      "          Conv2d-247          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-248          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-249          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-250          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-251            [-1, 720, 1, 1]               0\n",
      "          Conv2d-252             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-253             [-1, 30, 1, 1]               0\n",
      "          Conv2d-254            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-255            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-256          [-1, 720, 29, 29]               0\n",
      "          Conv2d-257          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-258          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-259          [-1, 120, 29, 29]               0\n",
      "          Conv2d-260          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-261          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-262          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-263          [-1, 720, 29, 29]               0\n",
      "          Conv2d-264          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-265          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-266          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-267          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-268            [-1, 720, 1, 1]               0\n",
      "          Conv2d-269             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-270             [-1, 30, 1, 1]               0\n",
      "          Conv2d-271            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-272            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-273          [-1, 720, 29, 29]               0\n",
      "          Conv2d-274          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-275          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-276          [-1, 120, 29, 29]               0\n",
      "          Conv2d-277          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-278          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-279          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-280          [-1, 720, 29, 29]               0\n",
      "          Conv2d-281          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-282          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-283          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-284          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-285            [-1, 720, 1, 1]               0\n",
      "          Conv2d-286             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-287             [-1, 30, 1, 1]               0\n",
      "          Conv2d-288            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-289            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-290          [-1, 720, 29, 29]               0\n",
      "          Conv2d-291          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-292          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-293          [-1, 120, 29, 29]               0\n",
      "          Conv2d-294          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-295          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-296          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-297          [-1, 720, 29, 29]               0\n",
      "          Conv2d-298          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-299          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-300          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-301          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-302            [-1, 720, 1, 1]               0\n",
      "          Conv2d-303             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-304             [-1, 30, 1, 1]               0\n",
      "          Conv2d-305            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-306            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-307          [-1, 720, 29, 29]               0\n",
      "          Conv2d-308          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-309          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-310          [-1, 120, 29, 29]               0\n",
      "          Conv2d-311          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-312          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-313          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-314          [-1, 720, 29, 29]               0\n",
      "          Conv2d-315          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-316          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-317          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-318          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-319            [-1, 720, 1, 1]               0\n",
      "          Conv2d-320             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-321             [-1, 30, 1, 1]               0\n",
      "          Conv2d-322            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-323            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-324          [-1, 720, 29, 29]               0\n",
      "          Conv2d-325          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-326          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-327          [-1, 120, 29, 29]               0\n",
      "          Conv2d-328          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-329          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-330          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-331          [-1, 720, 29, 29]               0\n",
      "          Conv2d-332          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-333          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-334          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-335          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-336            [-1, 720, 1, 1]               0\n",
      "          Conv2d-337             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-338             [-1, 30, 1, 1]               0\n",
      "          Conv2d-339            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-340            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-341          [-1, 720, 29, 29]               0\n",
      "          Conv2d-342          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-343          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-344          [-1, 120, 29, 29]               0\n",
      "          Conv2d-345          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-346          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-347          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-348          [-1, 720, 29, 29]               0\n",
      "          Conv2d-349          [-1, 720, 29, 29]          18,000\n",
      "     BatchNorm2d-350          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-351          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-352          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-353            [-1, 720, 1, 1]               0\n",
      "          Conv2d-354             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-355             [-1, 30, 1, 1]               0\n",
      "          Conv2d-356            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-357            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-358          [-1, 720, 29, 29]               0\n",
      "          Conv2d-359          [-1, 164, 29, 29]         118,080\n",
      "     BatchNorm2d-360          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-361          [-1, 164, 29, 29]               0\n",
      "          Conv2d-362          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-363          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-364          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-365          [-1, 984, 29, 29]               0\n",
      "          Conv2d-366          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-367          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-368          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-369          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-370            [-1, 984, 1, 1]               0\n",
      "          Conv2d-371             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-372             [-1, 41, 1, 1]               0\n",
      "          Conv2d-373            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-374            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-375          [-1, 984, 29, 29]               0\n",
      "          Conv2d-376          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-377          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-378          [-1, 164, 29, 29]               0\n",
      "          Conv2d-379          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-380          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-381          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-382          [-1, 984, 29, 29]               0\n",
      "          Conv2d-383          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-384          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-385          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-386          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-387            [-1, 984, 1, 1]               0\n",
      "          Conv2d-388             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-389             [-1, 41, 1, 1]               0\n",
      "          Conv2d-390            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-391            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-392          [-1, 984, 29, 29]               0\n",
      "          Conv2d-393          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-394          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-395          [-1, 164, 29, 29]               0\n",
      "          Conv2d-396          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-397          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-398          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-399          [-1, 984, 29, 29]               0\n",
      "          Conv2d-400          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-401          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-402          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-403          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-404            [-1, 984, 1, 1]               0\n",
      "          Conv2d-405             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-406             [-1, 41, 1, 1]               0\n",
      "          Conv2d-407            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-408            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-409          [-1, 984, 29, 29]               0\n",
      "          Conv2d-410          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-411          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-412          [-1, 164, 29, 29]               0\n",
      "          Conv2d-413          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-414          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-415          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-416          [-1, 984, 29, 29]               0\n",
      "          Conv2d-417          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-418          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-419          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-420          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-421            [-1, 984, 1, 1]               0\n",
      "          Conv2d-422             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-423             [-1, 41, 1, 1]               0\n",
      "          Conv2d-424            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-425            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-426          [-1, 984, 29, 29]               0\n",
      "          Conv2d-427          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-428          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-429          [-1, 164, 29, 29]               0\n",
      "          Conv2d-430          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-431          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-432          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-433          [-1, 984, 29, 29]               0\n",
      "          Conv2d-434          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-435          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-436          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-437          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-438            [-1, 984, 1, 1]               0\n",
      "          Conv2d-439             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-440             [-1, 41, 1, 1]               0\n",
      "          Conv2d-441            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-442            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-443          [-1, 984, 29, 29]               0\n",
      "          Conv2d-444          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-445          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-446          [-1, 164, 29, 29]               0\n",
      "          Conv2d-447          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-448          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-449          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-450          [-1, 984, 29, 29]               0\n",
      "          Conv2d-451          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-452          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-453          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-454          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-455            [-1, 984, 1, 1]               0\n",
      "          Conv2d-456             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-457             [-1, 41, 1, 1]               0\n",
      "          Conv2d-458            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-459            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-460          [-1, 984, 29, 29]               0\n",
      "          Conv2d-461          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-462          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-463          [-1, 164, 29, 29]               0\n",
      "          Conv2d-464          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-465          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-466          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-467          [-1, 984, 29, 29]               0\n",
      "          Conv2d-468          [-1, 984, 15, 15]          24,600\n",
      "     BatchNorm2d-469          [-1, 984, 15, 15]           1,968\n",
      "            SiLU-470          [-1, 984, 15, 15]               0\n",
      "        CNNBlock-471          [-1, 984, 15, 15]               0\n",
      "AdaptiveAvgPool2d-472            [-1, 984, 1, 1]               0\n",
      "          Conv2d-473             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-474             [-1, 41, 1, 1]               0\n",
      "          Conv2d-475            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-476            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-477          [-1, 984, 15, 15]               0\n",
      "          Conv2d-478          [-1, 284, 15, 15]         279,456\n",
      "     BatchNorm2d-479          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-480          [-1, 284, 15, 15]               0\n",
      "          Conv2d-481         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-482         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-483         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-484         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-485         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-486         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-487         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-488         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-489           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-490             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-491             [-1, 71, 1, 1]               0\n",
      "          Conv2d-492           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-493           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-494         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-495          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-496          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-497          [-1, 284, 15, 15]               0\n",
      "          Conv2d-498         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-499         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-500         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-501         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-502         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-503         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-504         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-505         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-506           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-507             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-508             [-1, 71, 1, 1]               0\n",
      "          Conv2d-509           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-510           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-511         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-512          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-513          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-514          [-1, 284, 15, 15]               0\n",
      "          Conv2d-515         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-516         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-517         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-518         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-519         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-520         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-521         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-522         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-523           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-524             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-525             [-1, 71, 1, 1]               0\n",
      "          Conv2d-526           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-527           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-528         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-529          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-530          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-531          [-1, 284, 15, 15]               0\n",
      "          Conv2d-532         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-533         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-534         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-535         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-536         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-537         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-538         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-539         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-540           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-541             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-542             [-1, 71, 1, 1]               0\n",
      "          Conv2d-543           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-544           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-545         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-546          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-547          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-548          [-1, 284, 15, 15]               0\n",
      "          Conv2d-549         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-550         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-551         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-552         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-553         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-554         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-555         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-556         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-557           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-558             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-559             [-1, 71, 1, 1]               0\n",
      "          Conv2d-560           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-561           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-562         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-563          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-564          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-565          [-1, 284, 15, 15]               0\n",
      "          Conv2d-566         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-567         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-568         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-569         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-570         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-571         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-572         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-573         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-574           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-575             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-576             [-1, 71, 1, 1]               0\n",
      "          Conv2d-577           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-578           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-579         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-580          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-581          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-582          [-1, 284, 15, 15]               0\n",
      "          Conv2d-583         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-584         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-585         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-586         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-587         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-588         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-589         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-590         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-591           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-592             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-593             [-1, 71, 1, 1]               0\n",
      "          Conv2d-594           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-595           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-596         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-597          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-598          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-599          [-1, 284, 15, 15]               0\n",
      "          Conv2d-600         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-601         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-602         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-603         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-604         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-605         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-606         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-607         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-608           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-609             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-610             [-1, 71, 1, 1]               0\n",
      "          Conv2d-611           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-612           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-613         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-614          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-615          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-616          [-1, 284, 15, 15]               0\n",
      "          Conv2d-617         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-618         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-619         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-620         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-621         [-1, 1704, 15, 15]          15,336\n",
      "     BatchNorm2d-622         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-623         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-624         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-625           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-626             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-627             [-1, 71, 1, 1]               0\n",
      "          Conv2d-628           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-629           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-630         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-631          [-1, 468, 15, 15]         797,472\n",
      "     BatchNorm2d-632          [-1, 468, 15, 15]             936\n",
      "InvertedResidualBlock-633          [-1, 468, 15, 15]               0\n",
      "          Conv2d-634         [-1, 2808, 15, 15]      11,827,296\n",
      "     BatchNorm2d-635         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-636         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-637         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-638         [-1, 2808, 15, 15]          25,272\n",
      "     BatchNorm2d-639         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-640         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-641         [-1, 2808, 15, 15]               0\n",
      "AdaptiveAvgPool2d-642           [-1, 2808, 1, 1]               0\n",
      "          Conv2d-643            [-1, 117, 1, 1]         328,653\n",
      "            SiLU-644            [-1, 117, 1, 1]               0\n",
      "          Conv2d-645           [-1, 2808, 1, 1]         331,344\n",
      "         Sigmoid-646           [-1, 2808, 1, 1]               0\n",
      "SqueezeExcitation-647         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-648          [-1, 468, 15, 15]       1,314,144\n",
      "     BatchNorm2d-649          [-1, 468, 15, 15]             936\n",
      "InvertedResidualBlock-650          [-1, 468, 15, 15]               0\n",
      "          Conv2d-651         [-1, 2808, 15, 15]      11,827,296\n",
      "     BatchNorm2d-652         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-653         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-654         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-655         [-1, 2808, 15, 15]          25,272\n",
      "     BatchNorm2d-656         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-657         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-658         [-1, 2808, 15, 15]               0\n",
      "AdaptiveAvgPool2d-659           [-1, 2808, 1, 1]               0\n",
      "          Conv2d-660            [-1, 117, 1, 1]         328,653\n",
      "            SiLU-661            [-1, 117, 1, 1]               0\n",
      "          Conv2d-662           [-1, 2808, 1, 1]         331,344\n",
      "         Sigmoid-663           [-1, 2808, 1, 1]               0\n",
      "SqueezeExcitation-664         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-665          [-1, 468, 15, 15]       1,314,144\n",
      "     BatchNorm2d-666          [-1, 468, 15, 15]             936\n",
      "InvertedResidualBlock-667          [-1, 468, 15, 15]               0\n",
      "          Conv2d-668         [-1, 1875, 15, 15]         877,500\n",
      "     BatchNorm2d-669         [-1, 1875, 15, 15]           3,750\n",
      "            SiLU-670         [-1, 1875, 15, 15]               0\n",
      "        CNNBlock-671         [-1, 1875, 15, 15]               0\n",
      "AdaptiveAvgPool2d-672           [-1, 1875, 1, 1]               0\n",
      "         Dropout-673                 [-1, 1875]               0\n",
      "          Linear-674                  [-1, 101]         189,476\n",
      "================================================================\n",
      "Total params: 99,446,653\n",
      "Trainable params: 99,446,653\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.38\n",
      "Forward/backward pass size (MB): 3087.42\n",
      "Params size (MB): 379.36\n",
      "Estimated Total Size (MB): 3469.16\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model shape ready\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet(\n",
    "    version=version,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "from thop import profile\n",
    "import colorama\n",
    "from colorama import Fore,Style\n",
    "#vital params\n",
    "def getGFLOPS(test_model):\n",
    "    model=test_model.to(device)\n",
    "    input = torch.randn(1, 3, res, res).to(device)\n",
    "    macs, params = profile(model, inputs=(input, ))\n",
    "\n",
    "    print(Fore.BLUE + \"The model requires: {:.4f} GFLOPS\".format(macs/1000_000_000))\n",
    "    print(Style.RESET_ALL)\n",
    "    return macs\n",
    "getGFLOPS(model)\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(batch_size,3,res,res)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(summary(model, input_size=(3, res, res)))\n",
    "print(\"model shape ready\")\n",
    "\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs/EfficientNetb5_stn120220108092945\n",
      "grandstore_string:\n",
      "grandstore/caltech101_EfficientNetb5_stn120220108092945.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string:\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c876d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7db3ad776da4ad8bc5cae96620fa891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d26a4933d3488fb6e325f2fb1bc131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 24.00 GiB total capacity; 21.70 GiB already allocated; 2.94 MiB free; 21.90 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/3215578226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mtop_accuracy_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOP_ACCURACY_TRACK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0maccwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                    )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/1740864069.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(max_epoch, top_accuracy_track, grandstore, minepoch, epochwindow, accwindow)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mvalid_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/2512676557.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/2512676557.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_residual\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/2512676557.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSqueezeExcitation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1896\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1897\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1898\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 24.00 GiB total capacity; 21.70 GiB already allocated; 2.94 MiB free; 21.90 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 12\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=350,\n",
    "                    minepoch=200,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                 \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b45315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Run 350 epoch(s)\n",
      "Accuracy MIN: 8.166666666666666 / MAX: 73.25\n",
      "\n",
      "Top 12 performing epochs:\n",
      "#1 epoch 345\t||train_acc 73.25%\t||test 72.08%\n",
      "#2 epoch 335\t||train_acc 73.25%\t||test 70.50%\n",
      "#3 epoch 341\t||train_acc 73.17%\t||test 71.17%\n",
      "#4 epoch 330\t||train_acc 73.17%\t||test 70.00%\n",
      "#5 epoch 283\t||train_acc 73.08%\t||test 70.50%\n",
      "#6 epoch 315\t||train_acc 73.00%\t||test 72.67%\n",
      "#7 epoch 284\t||train_acc 72.92%\t||test 71.58%\n",
      "#8 epoch 279\t||train_acc 72.92%\t||test 71.50%\n",
      "#9 epoch 331\t||train_acc 72.92%\t||test 71.42%\n",
      "#10 epoch 214\t||train_acc 72.92%\t||test 71.08%\n",
      "#11 epoch 328\t||train_acc 72.83%\t||test 70.17%\n",
      "#12 epoch 172\t||train_acc 72.75%\t||test 71.83%\n",
      "\n",
      "Last epoch:\n",
      "epoch 350\t||train_acc 71.75%\t||test 70.58%\n",
      "\n",
      "The model has parameters: 59266444\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {:.2f}%\\t||test {:.2f}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {:.2f}%\\t||test {:.2f}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Grandstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21296/3243412051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrandstore_string\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGrandstore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"writings done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Grandstore' is not defined"
     ]
    }
   ],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "\n",
    "print(\"writings done!\")\n",
    "print(\"Files at: \"+grandstore_string)\n",
    "\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a341d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
