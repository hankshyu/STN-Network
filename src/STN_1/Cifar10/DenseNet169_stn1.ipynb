{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DenseNet169_stn1 with 10 classes running on: cifar10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "#vital params\n",
    "\n",
    " \n",
    "model_name=\"DenseNet169_stn1\"\n",
    "\n",
    "dataset_name=\"cifar10\"\n",
    "\n",
    "#hyperparameters\n",
    "batch_size=20\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "image_size=(224,224)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38958e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset size: Train: 40000, Valid: 10000, Test: 10000\n",
      "torch.Size([3, 224, 224])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd38adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "#from .utils import load_state_dict_from_url\n",
    "\n",
    "\n",
    "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
    "\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)  \n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features] \n",
    "        for name, layer in self.named_children():   \n",
    "            new_features = layer(*features) \n",
    "            features.append(new_features)  \n",
    "        return torch.cat(features, 1)   \n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=num_classes, memory_efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,  \n",
    "                num_input_features=num_features,    \n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,    \n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)  \n",
    "            num_features = num_features + num_layers * growth_rate \n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)  \n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2   \n",
    "\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            #nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.Conv2d(3,8,kernel_size=3),\n",
    "            nn.Conv2d(8,8,kernel_size=3),\n",
    "            nn.Conv2d(8,8,kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            \n",
    "            #nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.Conv2d(8,10, kernel_size = 3),\n",
    "            nn.Conv2d(10,10,kernel_size = 3),\n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(27040, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        \n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 52 * 52)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.stn(x)\n",
    "        features = self.features(x) \n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))   \n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out) \n",
    "        return out\n",
    "\n",
    "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
    "              **kwargs):\n",
    "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
    "    if pretrained:\n",
    "        _load_state_dict(model, model_urls[arch], progress)\n",
    "    return model\n",
    "\n",
    "\n",
    "def densenet121(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-121 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet161(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-161 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet161', 32, (6, 12, 32, 32), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet169(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-169 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet169', 32, (6, 12, 48, 32), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet201(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-201 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet201', 32, (6, 12, 64, 48), 64, pretrained, progress,\n",
    "                     **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:4044: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3982: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "model shape ready\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 222, 222]             224\n",
      "            Conv2d-2          [-1, 8, 220, 220]             584\n",
      "            Conv2d-3          [-1, 8, 218, 218]             584\n",
      "         MaxPool2d-4          [-1, 8, 109, 109]               0\n",
      "              ReLU-5          [-1, 8, 109, 109]               0\n",
      "            Conv2d-6         [-1, 10, 107, 107]             730\n",
      "            Conv2d-7         [-1, 10, 105, 105]             910\n",
      "         MaxPool2d-8           [-1, 10, 52, 52]               0\n",
      "              ReLU-9           [-1, 10, 52, 52]               0\n",
      "           Linear-10                   [-1, 32]         865,312\n",
      "             ReLU-11                   [-1, 32]               0\n",
      "           Linear-12                    [-1, 6]             198\n",
      "           Conv2d-13         [-1, 64, 112, 112]           9,408\n",
      "      BatchNorm2d-14         [-1, 64, 112, 112]             128\n",
      "             ReLU-15         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-16           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "             ReLU-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "             ReLU-21          [-1, 128, 56, 56]               0\n",
      "           Conv2d-22           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 96, 56, 56]             192\n",
      "             ReLU-24           [-1, 96, 56, 56]               0\n",
      "           Conv2d-25          [-1, 128, 56, 56]          12,288\n",
      "      BatchNorm2d-26          [-1, 128, 56, 56]             256\n",
      "             ReLU-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
      "             ReLU-30          [-1, 128, 56, 56]               0\n",
      "           Conv2d-31          [-1, 128, 56, 56]          16,384\n",
      "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
      "             ReLU-33          [-1, 128, 56, 56]               0\n",
      "           Conv2d-34           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-35          [-1, 160, 56, 56]             320\n",
      "             ReLU-36          [-1, 160, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          20,480\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-41          [-1, 192, 56, 56]             384\n",
      "             ReLU-42          [-1, 192, 56, 56]               0\n",
      "           Conv2d-43          [-1, 128, 56, 56]          24,576\n",
      "      BatchNorm2d-44          [-1, 128, 56, 56]             256\n",
      "             ReLU-45          [-1, 128, 56, 56]               0\n",
      "           Conv2d-46           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-47          [-1, 224, 56, 56]             448\n",
      "             ReLU-48          [-1, 224, 56, 56]               0\n",
      "           Conv2d-49          [-1, 128, 56, 56]          28,672\n",
      "      BatchNorm2d-50          [-1, 128, 56, 56]             256\n",
      "             ReLU-51          [-1, 128, 56, 56]               0\n",
      "           Conv2d-52           [-1, 32, 56, 56]          36,864\n",
      "      _DenseBlock-53          [-1, 256, 56, 56]               0\n",
      "      BatchNorm2d-54          [-1, 256, 56, 56]             512\n",
      "             ReLU-55          [-1, 256, 56, 56]               0\n",
      "           Conv2d-56          [-1, 128, 56, 56]          32,768\n",
      "        AvgPool2d-57          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "             ReLU-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-64          [-1, 160, 28, 28]             320\n",
      "             ReLU-65          [-1, 160, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]          20,480\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-70          [-1, 192, 28, 28]             384\n",
      "             ReLU-71          [-1, 192, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]          24,576\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-76          [-1, 224, 28, 28]             448\n",
      "             ReLU-77          [-1, 224, 28, 28]               0\n",
      "           Conv2d-78          [-1, 128, 28, 28]          28,672\n",
      "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
      "             ReLU-80          [-1, 128, 28, 28]               0\n",
      "           Conv2d-81           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-82          [-1, 256, 28, 28]             512\n",
      "             ReLU-83          [-1, 256, 28, 28]               0\n",
      "           Conv2d-84          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-85          [-1, 128, 28, 28]             256\n",
      "             ReLU-86          [-1, 128, 28, 28]               0\n",
      "           Conv2d-87           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-88          [-1, 288, 28, 28]             576\n",
      "             ReLU-89          [-1, 288, 28, 28]               0\n",
      "           Conv2d-90          [-1, 128, 28, 28]          36,864\n",
      "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
      "             ReLU-92          [-1, 128, 28, 28]               0\n",
      "           Conv2d-93           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-94          [-1, 320, 28, 28]             640\n",
      "             ReLU-95          [-1, 320, 28, 28]               0\n",
      "           Conv2d-96          [-1, 128, 28, 28]          40,960\n",
      "      BatchNorm2d-97          [-1, 128, 28, 28]             256\n",
      "             ReLU-98          [-1, 128, 28, 28]               0\n",
      "           Conv2d-99           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-100          [-1, 352, 28, 28]             704\n",
      "            ReLU-101          [-1, 352, 28, 28]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]          45,056\n",
      "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
      "            ReLU-104          [-1, 128, 28, 28]               0\n",
      "          Conv2d-105           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-106          [-1, 384, 28, 28]             768\n",
      "            ReLU-107          [-1, 384, 28, 28]               0\n",
      "          Conv2d-108          [-1, 128, 28, 28]          49,152\n",
      "     BatchNorm2d-109          [-1, 128, 28, 28]             256\n",
      "            ReLU-110          [-1, 128, 28, 28]               0\n",
      "          Conv2d-111           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-112          [-1, 416, 28, 28]             832\n",
      "            ReLU-113          [-1, 416, 28, 28]               0\n",
      "          Conv2d-114          [-1, 128, 28, 28]          53,248\n",
      "     BatchNorm2d-115          [-1, 128, 28, 28]             256\n",
      "            ReLU-116          [-1, 128, 28, 28]               0\n",
      "          Conv2d-117           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-118          [-1, 448, 28, 28]             896\n",
      "            ReLU-119          [-1, 448, 28, 28]               0\n",
      "          Conv2d-120          [-1, 128, 28, 28]          57,344\n",
      "     BatchNorm2d-121          [-1, 128, 28, 28]             256\n",
      "            ReLU-122          [-1, 128, 28, 28]               0\n",
      "          Conv2d-123           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-124          [-1, 480, 28, 28]             960\n",
      "            ReLU-125          [-1, 480, 28, 28]               0\n",
      "          Conv2d-126          [-1, 128, 28, 28]          61,440\n",
      "     BatchNorm2d-127          [-1, 128, 28, 28]             256\n",
      "            ReLU-128          [-1, 128, 28, 28]               0\n",
      "          Conv2d-129           [-1, 32, 28, 28]          36,864\n",
      "     _DenseBlock-130          [-1, 512, 28, 28]               0\n",
      "     BatchNorm2d-131          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-132          [-1, 512, 28, 28]               0\n",
      "          Conv2d-133          [-1, 256, 28, 28]         131,072\n",
      "       AvgPool2d-134          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-138          [-1, 128, 14, 14]             256\n",
      "            ReLU-139          [-1, 128, 14, 14]               0\n",
      "          Conv2d-140           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-141          [-1, 288, 14, 14]             576\n",
      "            ReLU-142          [-1, 288, 14, 14]               0\n",
      "          Conv2d-143          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-144          [-1, 128, 14, 14]             256\n",
      "            ReLU-145          [-1, 128, 14, 14]               0\n",
      "          Conv2d-146           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-147          [-1, 320, 14, 14]             640\n",
      "            ReLU-148          [-1, 320, 14, 14]               0\n",
      "          Conv2d-149          [-1, 128, 14, 14]          40,960\n",
      "     BatchNorm2d-150          [-1, 128, 14, 14]             256\n",
      "            ReLU-151          [-1, 128, 14, 14]               0\n",
      "          Conv2d-152           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-153          [-1, 352, 14, 14]             704\n",
      "            ReLU-154          [-1, 352, 14, 14]               0\n",
      "          Conv2d-155          [-1, 128, 14, 14]          45,056\n",
      "     BatchNorm2d-156          [-1, 128, 14, 14]             256\n",
      "            ReLU-157          [-1, 128, 14, 14]               0\n",
      "          Conv2d-158           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-159          [-1, 384, 14, 14]             768\n",
      "            ReLU-160          [-1, 384, 14, 14]               0\n",
      "          Conv2d-161          [-1, 128, 14, 14]          49,152\n",
      "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
      "            ReLU-163          [-1, 128, 14, 14]               0\n",
      "          Conv2d-164           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-165          [-1, 416, 14, 14]             832\n",
      "            ReLU-166          [-1, 416, 14, 14]               0\n",
      "          Conv2d-167          [-1, 128, 14, 14]          53,248\n",
      "     BatchNorm2d-168          [-1, 128, 14, 14]             256\n",
      "            ReLU-169          [-1, 128, 14, 14]               0\n",
      "          Conv2d-170           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-171          [-1, 448, 14, 14]             896\n",
      "            ReLU-172          [-1, 448, 14, 14]               0\n",
      "          Conv2d-173          [-1, 128, 14, 14]          57,344\n",
      "     BatchNorm2d-174          [-1, 128, 14, 14]             256\n",
      "            ReLU-175          [-1, 128, 14, 14]               0\n",
      "          Conv2d-176           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-177          [-1, 480, 14, 14]             960\n",
      "            ReLU-178          [-1, 480, 14, 14]               0\n",
      "          Conv2d-179          [-1, 128, 14, 14]          61,440\n",
      "     BatchNorm2d-180          [-1, 128, 14, 14]             256\n",
      "            ReLU-181          [-1, 128, 14, 14]               0\n",
      "          Conv2d-182           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-183          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-184          [-1, 512, 14, 14]               0\n",
      "          Conv2d-185          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-186          [-1, 128, 14, 14]             256\n",
      "            ReLU-187          [-1, 128, 14, 14]               0\n",
      "          Conv2d-188           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-189          [-1, 544, 14, 14]           1,088\n",
      "            ReLU-190          [-1, 544, 14, 14]               0\n",
      "          Conv2d-191          [-1, 128, 14, 14]          69,632\n",
      "     BatchNorm2d-192          [-1, 128, 14, 14]             256\n",
      "            ReLU-193          [-1, 128, 14, 14]               0\n",
      "          Conv2d-194           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-195          [-1, 576, 14, 14]           1,152\n",
      "            ReLU-196          [-1, 576, 14, 14]               0\n",
      "          Conv2d-197          [-1, 128, 14, 14]          73,728\n",
      "     BatchNorm2d-198          [-1, 128, 14, 14]             256\n",
      "            ReLU-199          [-1, 128, 14, 14]               0\n",
      "          Conv2d-200           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-201          [-1, 608, 14, 14]           1,216\n",
      "            ReLU-202          [-1, 608, 14, 14]               0\n",
      "          Conv2d-203          [-1, 128, 14, 14]          77,824\n",
      "     BatchNorm2d-204          [-1, 128, 14, 14]             256\n",
      "            ReLU-205          [-1, 128, 14, 14]               0\n",
      "          Conv2d-206           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-207          [-1, 640, 14, 14]           1,280\n",
      "            ReLU-208          [-1, 640, 14, 14]               0\n",
      "          Conv2d-209          [-1, 128, 14, 14]          81,920\n",
      "     BatchNorm2d-210          [-1, 128, 14, 14]             256\n",
      "            ReLU-211          [-1, 128, 14, 14]               0\n",
      "          Conv2d-212           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-213          [-1, 672, 14, 14]           1,344\n",
      "            ReLU-214          [-1, 672, 14, 14]               0\n",
      "          Conv2d-215          [-1, 128, 14, 14]          86,016\n",
      "     BatchNorm2d-216          [-1, 128, 14, 14]             256\n",
      "            ReLU-217          [-1, 128, 14, 14]               0\n",
      "          Conv2d-218           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-219          [-1, 704, 14, 14]           1,408\n",
      "            ReLU-220          [-1, 704, 14, 14]               0\n",
      "          Conv2d-221          [-1, 128, 14, 14]          90,112\n",
      "     BatchNorm2d-222          [-1, 128, 14, 14]             256\n",
      "            ReLU-223          [-1, 128, 14, 14]               0\n",
      "          Conv2d-224           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-225          [-1, 736, 14, 14]           1,472\n",
      "            ReLU-226          [-1, 736, 14, 14]               0\n",
      "          Conv2d-227          [-1, 128, 14, 14]          94,208\n",
      "     BatchNorm2d-228          [-1, 128, 14, 14]             256\n",
      "            ReLU-229          [-1, 128, 14, 14]               0\n",
      "          Conv2d-230           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-231          [-1, 768, 14, 14]           1,536\n",
      "            ReLU-232          [-1, 768, 14, 14]               0\n",
      "          Conv2d-233          [-1, 128, 14, 14]          98,304\n",
      "     BatchNorm2d-234          [-1, 128, 14, 14]             256\n",
      "            ReLU-235          [-1, 128, 14, 14]               0\n",
      "          Conv2d-236           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-237          [-1, 800, 14, 14]           1,600\n",
      "            ReLU-238          [-1, 800, 14, 14]               0\n",
      "          Conv2d-239          [-1, 128, 14, 14]         102,400\n",
      "     BatchNorm2d-240          [-1, 128, 14, 14]             256\n",
      "            ReLU-241          [-1, 128, 14, 14]               0\n",
      "          Conv2d-242           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-243          [-1, 832, 14, 14]           1,664\n",
      "            ReLU-244          [-1, 832, 14, 14]               0\n",
      "          Conv2d-245          [-1, 128, 14, 14]         106,496\n",
      "     BatchNorm2d-246          [-1, 128, 14, 14]             256\n",
      "            ReLU-247          [-1, 128, 14, 14]               0\n",
      "          Conv2d-248           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-249          [-1, 864, 14, 14]           1,728\n",
      "            ReLU-250          [-1, 864, 14, 14]               0\n",
      "          Conv2d-251          [-1, 128, 14, 14]         110,592\n",
      "     BatchNorm2d-252          [-1, 128, 14, 14]             256\n",
      "            ReLU-253          [-1, 128, 14, 14]               0\n",
      "          Conv2d-254           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-255          [-1, 896, 14, 14]           1,792\n",
      "            ReLU-256          [-1, 896, 14, 14]               0\n",
      "          Conv2d-257          [-1, 128, 14, 14]         114,688\n",
      "     BatchNorm2d-258          [-1, 128, 14, 14]             256\n",
      "            ReLU-259          [-1, 128, 14, 14]               0\n",
      "          Conv2d-260           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-261          [-1, 928, 14, 14]           1,856\n",
      "            ReLU-262          [-1, 928, 14, 14]               0\n",
      "          Conv2d-263          [-1, 128, 14, 14]         118,784\n",
      "     BatchNorm2d-264          [-1, 128, 14, 14]             256\n",
      "            ReLU-265          [-1, 128, 14, 14]               0\n",
      "          Conv2d-266           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-267          [-1, 960, 14, 14]           1,920\n",
      "            ReLU-268          [-1, 960, 14, 14]               0\n",
      "          Conv2d-269          [-1, 128, 14, 14]         122,880\n",
      "     BatchNorm2d-270          [-1, 128, 14, 14]             256\n",
      "            ReLU-271          [-1, 128, 14, 14]               0\n",
      "          Conv2d-272           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-273          [-1, 992, 14, 14]           1,984\n",
      "            ReLU-274          [-1, 992, 14, 14]               0\n",
      "          Conv2d-275          [-1, 128, 14, 14]         126,976\n",
      "     BatchNorm2d-276          [-1, 128, 14, 14]             256\n",
      "            ReLU-277          [-1, 128, 14, 14]               0\n",
      "          Conv2d-278           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-279         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 128, 14, 14]         131,072\n",
      "     BatchNorm2d-282          [-1, 128, 14, 14]             256\n",
      "            ReLU-283          [-1, 128, 14, 14]               0\n",
      "          Conv2d-284           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-285         [-1, 1056, 14, 14]           2,112\n",
      "            ReLU-286         [-1, 1056, 14, 14]               0\n",
      "          Conv2d-287          [-1, 128, 14, 14]         135,168\n",
      "     BatchNorm2d-288          [-1, 128, 14, 14]             256\n",
      "            ReLU-289          [-1, 128, 14, 14]               0\n",
      "          Conv2d-290           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-291         [-1, 1088, 14, 14]           2,176\n",
      "            ReLU-292         [-1, 1088, 14, 14]               0\n",
      "          Conv2d-293          [-1, 128, 14, 14]         139,264\n",
      "     BatchNorm2d-294          [-1, 128, 14, 14]             256\n",
      "            ReLU-295          [-1, 128, 14, 14]               0\n",
      "          Conv2d-296           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-297         [-1, 1120, 14, 14]           2,240\n",
      "            ReLU-298         [-1, 1120, 14, 14]               0\n",
      "          Conv2d-299          [-1, 128, 14, 14]         143,360\n",
      "     BatchNorm2d-300          [-1, 128, 14, 14]             256\n",
      "            ReLU-301          [-1, 128, 14, 14]               0\n",
      "          Conv2d-302           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-303         [-1, 1152, 14, 14]           2,304\n",
      "            ReLU-304         [-1, 1152, 14, 14]               0\n",
      "          Conv2d-305          [-1, 128, 14, 14]         147,456\n",
      "     BatchNorm2d-306          [-1, 128, 14, 14]             256\n",
      "            ReLU-307          [-1, 128, 14, 14]               0\n",
      "          Conv2d-308           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-309         [-1, 1184, 14, 14]           2,368\n",
      "            ReLU-310         [-1, 1184, 14, 14]               0\n",
      "          Conv2d-311          [-1, 128, 14, 14]         151,552\n",
      "     BatchNorm2d-312          [-1, 128, 14, 14]             256\n",
      "            ReLU-313          [-1, 128, 14, 14]               0\n",
      "          Conv2d-314           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-315         [-1, 1216, 14, 14]           2,432\n",
      "            ReLU-316         [-1, 1216, 14, 14]               0\n",
      "          Conv2d-317          [-1, 128, 14, 14]         155,648\n",
      "     BatchNorm2d-318          [-1, 128, 14, 14]             256\n",
      "            ReLU-319          [-1, 128, 14, 14]               0\n",
      "          Conv2d-320           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-321         [-1, 1248, 14, 14]           2,496\n",
      "            ReLU-322         [-1, 1248, 14, 14]               0\n",
      "          Conv2d-323          [-1, 128, 14, 14]         159,744\n",
      "     BatchNorm2d-324          [-1, 128, 14, 14]             256\n",
      "            ReLU-325          [-1, 128, 14, 14]               0\n",
      "          Conv2d-326           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-327         [-1, 1280, 14, 14]           2,560\n",
      "            ReLU-328         [-1, 1280, 14, 14]               0\n",
      "          Conv2d-329          [-1, 128, 14, 14]         163,840\n",
      "     BatchNorm2d-330          [-1, 128, 14, 14]             256\n",
      "            ReLU-331          [-1, 128, 14, 14]               0\n",
      "          Conv2d-332           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-333         [-1, 1312, 14, 14]           2,624\n",
      "            ReLU-334         [-1, 1312, 14, 14]               0\n",
      "          Conv2d-335          [-1, 128, 14, 14]         167,936\n",
      "     BatchNorm2d-336          [-1, 128, 14, 14]             256\n",
      "            ReLU-337          [-1, 128, 14, 14]               0\n",
      "          Conv2d-338           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-339         [-1, 1344, 14, 14]           2,688\n",
      "            ReLU-340         [-1, 1344, 14, 14]               0\n",
      "          Conv2d-341          [-1, 128, 14, 14]         172,032\n",
      "     BatchNorm2d-342          [-1, 128, 14, 14]             256\n",
      "            ReLU-343          [-1, 128, 14, 14]               0\n",
      "          Conv2d-344           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-345         [-1, 1376, 14, 14]           2,752\n",
      "            ReLU-346         [-1, 1376, 14, 14]               0\n",
      "          Conv2d-347          [-1, 128, 14, 14]         176,128\n",
      "     BatchNorm2d-348          [-1, 128, 14, 14]             256\n",
      "            ReLU-349          [-1, 128, 14, 14]               0\n",
      "          Conv2d-350           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-351         [-1, 1408, 14, 14]           2,816\n",
      "            ReLU-352         [-1, 1408, 14, 14]               0\n",
      "          Conv2d-353          [-1, 128, 14, 14]         180,224\n",
      "     BatchNorm2d-354          [-1, 128, 14, 14]             256\n",
      "            ReLU-355          [-1, 128, 14, 14]               0\n",
      "          Conv2d-356           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-357         [-1, 1440, 14, 14]           2,880\n",
      "            ReLU-358         [-1, 1440, 14, 14]               0\n",
      "          Conv2d-359          [-1, 128, 14, 14]         184,320\n",
      "     BatchNorm2d-360          [-1, 128, 14, 14]             256\n",
      "            ReLU-361          [-1, 128, 14, 14]               0\n",
      "          Conv2d-362           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-363         [-1, 1472, 14, 14]           2,944\n",
      "            ReLU-364         [-1, 1472, 14, 14]               0\n",
      "          Conv2d-365          [-1, 128, 14, 14]         188,416\n",
      "     BatchNorm2d-366          [-1, 128, 14, 14]             256\n",
      "            ReLU-367          [-1, 128, 14, 14]               0\n",
      "          Conv2d-368           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-369         [-1, 1504, 14, 14]           3,008\n",
      "            ReLU-370         [-1, 1504, 14, 14]               0\n",
      "          Conv2d-371          [-1, 128, 14, 14]         192,512\n",
      "     BatchNorm2d-372          [-1, 128, 14, 14]             256\n",
      "            ReLU-373          [-1, 128, 14, 14]               0\n",
      "          Conv2d-374           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-375         [-1, 1536, 14, 14]           3,072\n",
      "            ReLU-376         [-1, 1536, 14, 14]               0\n",
      "          Conv2d-377          [-1, 128, 14, 14]         196,608\n",
      "     BatchNorm2d-378          [-1, 128, 14, 14]             256\n",
      "            ReLU-379          [-1, 128, 14, 14]               0\n",
      "          Conv2d-380           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-381         [-1, 1568, 14, 14]           3,136\n",
      "            ReLU-382         [-1, 1568, 14, 14]               0\n",
      "          Conv2d-383          [-1, 128, 14, 14]         200,704\n",
      "     BatchNorm2d-384          [-1, 128, 14, 14]             256\n",
      "            ReLU-385          [-1, 128, 14, 14]               0\n",
      "          Conv2d-386           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-387         [-1, 1600, 14, 14]           3,200\n",
      "            ReLU-388         [-1, 1600, 14, 14]               0\n",
      "          Conv2d-389          [-1, 128, 14, 14]         204,800\n",
      "     BatchNorm2d-390          [-1, 128, 14, 14]             256\n",
      "            ReLU-391          [-1, 128, 14, 14]               0\n",
      "          Conv2d-392           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-393         [-1, 1632, 14, 14]           3,264\n",
      "            ReLU-394         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-395          [-1, 128, 14, 14]         208,896\n",
      "     BatchNorm2d-396          [-1, 128, 14, 14]             256\n",
      "            ReLU-397          [-1, 128, 14, 14]               0\n",
      "          Conv2d-398           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-399         [-1, 1664, 14, 14]           3,328\n",
      "            ReLU-400         [-1, 1664, 14, 14]               0\n",
      "          Conv2d-401          [-1, 128, 14, 14]         212,992\n",
      "     BatchNorm2d-402          [-1, 128, 14, 14]             256\n",
      "            ReLU-403          [-1, 128, 14, 14]               0\n",
      "          Conv2d-404           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-405         [-1, 1696, 14, 14]           3,392\n",
      "            ReLU-406         [-1, 1696, 14, 14]               0\n",
      "          Conv2d-407          [-1, 128, 14, 14]         217,088\n",
      "     BatchNorm2d-408          [-1, 128, 14, 14]             256\n",
      "            ReLU-409          [-1, 128, 14, 14]               0\n",
      "          Conv2d-410           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-411         [-1, 1728, 14, 14]           3,456\n",
      "            ReLU-412         [-1, 1728, 14, 14]               0\n",
      "          Conv2d-413          [-1, 128, 14, 14]         221,184\n",
      "     BatchNorm2d-414          [-1, 128, 14, 14]             256\n",
      "            ReLU-415          [-1, 128, 14, 14]               0\n",
      "          Conv2d-416           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-417         [-1, 1760, 14, 14]           3,520\n",
      "            ReLU-418         [-1, 1760, 14, 14]               0\n",
      "          Conv2d-419          [-1, 128, 14, 14]         225,280\n",
      "     BatchNorm2d-420          [-1, 128, 14, 14]             256\n",
      "            ReLU-421          [-1, 128, 14, 14]               0\n",
      "          Conv2d-422           [-1, 32, 14, 14]          36,864\n",
      "     _DenseBlock-423         [-1, 1792, 14, 14]               0\n",
      "     BatchNorm2d-424         [-1, 1792, 14, 14]           3,584\n",
      "            ReLU-425         [-1, 1792, 14, 14]               0\n",
      "          Conv2d-426          [-1, 896, 14, 14]       1,605,632\n",
      "       AvgPool2d-427            [-1, 896, 7, 7]               0\n",
      "     BatchNorm2d-428            [-1, 896, 7, 7]           1,792\n",
      "            ReLU-429            [-1, 896, 7, 7]               0\n",
      "          Conv2d-430            [-1, 128, 7, 7]         114,688\n",
      "     BatchNorm2d-431            [-1, 128, 7, 7]             256\n",
      "            ReLU-432            [-1, 128, 7, 7]               0\n",
      "          Conv2d-433             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-434            [-1, 928, 7, 7]           1,856\n",
      "            ReLU-435            [-1, 928, 7, 7]               0\n",
      "          Conv2d-436            [-1, 128, 7, 7]         118,784\n",
      "     BatchNorm2d-437            [-1, 128, 7, 7]             256\n",
      "            ReLU-438            [-1, 128, 7, 7]               0\n",
      "          Conv2d-439             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-440            [-1, 960, 7, 7]           1,920\n",
      "            ReLU-441            [-1, 960, 7, 7]               0\n",
      "          Conv2d-442            [-1, 128, 7, 7]         122,880\n",
      "     BatchNorm2d-443            [-1, 128, 7, 7]             256\n",
      "            ReLU-444            [-1, 128, 7, 7]               0\n",
      "          Conv2d-445             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-446            [-1, 992, 7, 7]           1,984\n",
      "            ReLU-447            [-1, 992, 7, 7]               0\n",
      "          Conv2d-448            [-1, 128, 7, 7]         126,976\n",
      "     BatchNorm2d-449            [-1, 128, 7, 7]             256\n",
      "            ReLU-450            [-1, 128, 7, 7]               0\n",
      "          Conv2d-451             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-452           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-453           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-454            [-1, 128, 7, 7]         131,072\n",
      "     BatchNorm2d-455            [-1, 128, 7, 7]             256\n",
      "            ReLU-456            [-1, 128, 7, 7]               0\n",
      "          Conv2d-457             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-458           [-1, 1056, 7, 7]           2,112\n",
      "            ReLU-459           [-1, 1056, 7, 7]               0\n",
      "          Conv2d-460            [-1, 128, 7, 7]         135,168\n",
      "     BatchNorm2d-461            [-1, 128, 7, 7]             256\n",
      "            ReLU-462            [-1, 128, 7, 7]               0\n",
      "          Conv2d-463             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-464           [-1, 1088, 7, 7]           2,176\n",
      "            ReLU-465           [-1, 1088, 7, 7]               0\n",
      "          Conv2d-466            [-1, 128, 7, 7]         139,264\n",
      "     BatchNorm2d-467            [-1, 128, 7, 7]             256\n",
      "            ReLU-468            [-1, 128, 7, 7]               0\n",
      "          Conv2d-469             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-470           [-1, 1120, 7, 7]           2,240\n",
      "            ReLU-471           [-1, 1120, 7, 7]               0\n",
      "          Conv2d-472            [-1, 128, 7, 7]         143,360\n",
      "     BatchNorm2d-473            [-1, 128, 7, 7]             256\n",
      "            ReLU-474            [-1, 128, 7, 7]               0\n",
      "          Conv2d-475             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-476           [-1, 1152, 7, 7]           2,304\n",
      "            ReLU-477           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-478            [-1, 128, 7, 7]         147,456\n",
      "     BatchNorm2d-479            [-1, 128, 7, 7]             256\n",
      "            ReLU-480            [-1, 128, 7, 7]               0\n",
      "          Conv2d-481             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-482           [-1, 1184, 7, 7]           2,368\n",
      "            ReLU-483           [-1, 1184, 7, 7]               0\n",
      "          Conv2d-484            [-1, 128, 7, 7]         151,552\n",
      "     BatchNorm2d-485            [-1, 128, 7, 7]             256\n",
      "            ReLU-486            [-1, 128, 7, 7]               0\n",
      "          Conv2d-487             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-488           [-1, 1216, 7, 7]           2,432\n",
      "            ReLU-489           [-1, 1216, 7, 7]               0\n",
      "          Conv2d-490            [-1, 128, 7, 7]         155,648\n",
      "     BatchNorm2d-491            [-1, 128, 7, 7]             256\n",
      "            ReLU-492            [-1, 128, 7, 7]               0\n",
      "          Conv2d-493             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-494           [-1, 1248, 7, 7]           2,496\n",
      "            ReLU-495           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-496            [-1, 128, 7, 7]         159,744\n",
      "     BatchNorm2d-497            [-1, 128, 7, 7]             256\n",
      "            ReLU-498            [-1, 128, 7, 7]               0\n",
      "          Conv2d-499             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-500           [-1, 1280, 7, 7]           2,560\n",
      "            ReLU-501           [-1, 1280, 7, 7]               0\n",
      "          Conv2d-502            [-1, 128, 7, 7]         163,840\n",
      "     BatchNorm2d-503            [-1, 128, 7, 7]             256\n",
      "            ReLU-504            [-1, 128, 7, 7]               0\n",
      "          Conv2d-505             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-506           [-1, 1312, 7, 7]           2,624\n",
      "            ReLU-507           [-1, 1312, 7, 7]               0\n",
      "          Conv2d-508            [-1, 128, 7, 7]         167,936\n",
      "     BatchNorm2d-509            [-1, 128, 7, 7]             256\n",
      "            ReLU-510            [-1, 128, 7, 7]               0\n",
      "          Conv2d-511             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-512           [-1, 1344, 7, 7]           2,688\n",
      "            ReLU-513           [-1, 1344, 7, 7]               0\n",
      "          Conv2d-514            [-1, 128, 7, 7]         172,032\n",
      "     BatchNorm2d-515            [-1, 128, 7, 7]             256\n",
      "            ReLU-516            [-1, 128, 7, 7]               0\n",
      "          Conv2d-517             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-518           [-1, 1376, 7, 7]           2,752\n",
      "            ReLU-519           [-1, 1376, 7, 7]               0\n",
      "          Conv2d-520            [-1, 128, 7, 7]         176,128\n",
      "     BatchNorm2d-521            [-1, 128, 7, 7]             256\n",
      "            ReLU-522            [-1, 128, 7, 7]               0\n",
      "          Conv2d-523             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-524           [-1, 1408, 7, 7]           2,816\n",
      "            ReLU-525           [-1, 1408, 7, 7]               0\n",
      "          Conv2d-526            [-1, 128, 7, 7]         180,224\n",
      "     BatchNorm2d-527            [-1, 128, 7, 7]             256\n",
      "            ReLU-528            [-1, 128, 7, 7]               0\n",
      "          Conv2d-529             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-530           [-1, 1440, 7, 7]           2,880\n",
      "            ReLU-531           [-1, 1440, 7, 7]               0\n",
      "          Conv2d-532            [-1, 128, 7, 7]         184,320\n",
      "     BatchNorm2d-533            [-1, 128, 7, 7]             256\n",
      "            ReLU-534            [-1, 128, 7, 7]               0\n",
      "          Conv2d-535             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-536           [-1, 1472, 7, 7]           2,944\n",
      "            ReLU-537           [-1, 1472, 7, 7]               0\n",
      "          Conv2d-538            [-1, 128, 7, 7]         188,416\n",
      "     BatchNorm2d-539            [-1, 128, 7, 7]             256\n",
      "            ReLU-540            [-1, 128, 7, 7]               0\n",
      "          Conv2d-541             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-542           [-1, 1504, 7, 7]           3,008\n",
      "            ReLU-543           [-1, 1504, 7, 7]               0\n",
      "          Conv2d-544            [-1, 128, 7, 7]         192,512\n",
      "     BatchNorm2d-545            [-1, 128, 7, 7]             256\n",
      "            ReLU-546            [-1, 128, 7, 7]               0\n",
      "          Conv2d-547             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-548           [-1, 1536, 7, 7]           3,072\n",
      "            ReLU-549           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-550            [-1, 128, 7, 7]         196,608\n",
      "     BatchNorm2d-551            [-1, 128, 7, 7]             256\n",
      "            ReLU-552            [-1, 128, 7, 7]               0\n",
      "          Conv2d-553             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-554           [-1, 1568, 7, 7]           3,136\n",
      "            ReLU-555           [-1, 1568, 7, 7]               0\n",
      "          Conv2d-556            [-1, 128, 7, 7]         200,704\n",
      "     BatchNorm2d-557            [-1, 128, 7, 7]             256\n",
      "            ReLU-558            [-1, 128, 7, 7]               0\n",
      "          Conv2d-559             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-560           [-1, 1600, 7, 7]           3,200\n",
      "            ReLU-561           [-1, 1600, 7, 7]               0\n",
      "          Conv2d-562            [-1, 128, 7, 7]         204,800\n",
      "     BatchNorm2d-563            [-1, 128, 7, 7]             256\n",
      "            ReLU-564            [-1, 128, 7, 7]               0\n",
      "          Conv2d-565             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-566           [-1, 1632, 7, 7]           3,264\n",
      "            ReLU-567           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-568            [-1, 128, 7, 7]         208,896\n",
      "     BatchNorm2d-569            [-1, 128, 7, 7]             256\n",
      "            ReLU-570            [-1, 128, 7, 7]               0\n",
      "          Conv2d-571             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-572           [-1, 1664, 7, 7]           3,328\n",
      "            ReLU-573           [-1, 1664, 7, 7]               0\n",
      "          Conv2d-574            [-1, 128, 7, 7]         212,992\n",
      "     BatchNorm2d-575            [-1, 128, 7, 7]             256\n",
      "            ReLU-576            [-1, 128, 7, 7]               0\n",
      "          Conv2d-577             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-578           [-1, 1696, 7, 7]           3,392\n",
      "            ReLU-579           [-1, 1696, 7, 7]               0\n",
      "          Conv2d-580            [-1, 128, 7, 7]         217,088\n",
      "     BatchNorm2d-581            [-1, 128, 7, 7]             256\n",
      "            ReLU-582            [-1, 128, 7, 7]               0\n",
      "          Conv2d-583             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-584           [-1, 1728, 7, 7]           3,456\n",
      "            ReLU-585           [-1, 1728, 7, 7]               0\n",
      "          Conv2d-586            [-1, 128, 7, 7]         221,184\n",
      "     BatchNorm2d-587            [-1, 128, 7, 7]             256\n",
      "            ReLU-588            [-1, 128, 7, 7]               0\n",
      "          Conv2d-589             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-590           [-1, 1760, 7, 7]           3,520\n",
      "            ReLU-591           [-1, 1760, 7, 7]               0\n",
      "          Conv2d-592            [-1, 128, 7, 7]         225,280\n",
      "     BatchNorm2d-593            [-1, 128, 7, 7]             256\n",
      "            ReLU-594            [-1, 128, 7, 7]               0\n",
      "          Conv2d-595             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-596           [-1, 1792, 7, 7]           3,584\n",
      "            ReLU-597           [-1, 1792, 7, 7]               0\n",
      "          Conv2d-598            [-1, 128, 7, 7]         229,376\n",
      "     BatchNorm2d-599            [-1, 128, 7, 7]             256\n",
      "            ReLU-600            [-1, 128, 7, 7]               0\n",
      "          Conv2d-601             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-602           [-1, 1824, 7, 7]           3,648\n",
      "            ReLU-603           [-1, 1824, 7, 7]               0\n",
      "          Conv2d-604            [-1, 128, 7, 7]         233,472\n",
      "     BatchNorm2d-605            [-1, 128, 7, 7]             256\n",
      "            ReLU-606            [-1, 128, 7, 7]               0\n",
      "          Conv2d-607             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-608           [-1, 1856, 7, 7]           3,712\n",
      "            ReLU-609           [-1, 1856, 7, 7]               0\n",
      "          Conv2d-610            [-1, 128, 7, 7]         237,568\n",
      "     BatchNorm2d-611            [-1, 128, 7, 7]             256\n",
      "            ReLU-612            [-1, 128, 7, 7]               0\n",
      "          Conv2d-613             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-614           [-1, 1888, 7, 7]           3,776\n",
      "            ReLU-615           [-1, 1888, 7, 7]               0\n",
      "          Conv2d-616            [-1, 128, 7, 7]         241,664\n",
      "     BatchNorm2d-617            [-1, 128, 7, 7]             256\n",
      "            ReLU-618            [-1, 128, 7, 7]               0\n",
      "          Conv2d-619             [-1, 32, 7, 7]          36,864\n",
      "     _DenseBlock-620           [-1, 1920, 7, 7]               0\n",
      "     BatchNorm2d-621           [-1, 1920, 7, 7]           3,840\n",
      "          Linear-622                   [-1, 10]          19,210\n",
      "================================================================\n",
      "Total params: 18,980,680\n",
      "Trainable params: 18,980,680\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 464.21\n",
      "Params size (MB): 72.41\n",
      "Estimated Total Size (MB): 537.19\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model = densenet169().to(device)\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(batch_size,3,224,224)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(\"model shape ready\")\n",
    "print(summary(model, input_size=(3, 224, 224)))\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592a8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs//DenseNet169_stn120211201020027\n",
      "grandstore_string\n",
      "grandstore/cifar10_DenseNet169_stn120211201020027.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+\"/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3316dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1619e7613abc473f8d0278d72629f8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcff673968da4715b9cf02a1c07bce75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396edeb2509240e187b8ea3b96fba0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.710\n",
      "Training Loss: 91703639.6188 \tValidation Loss: 14683338.6218\n",
      "\n",
      "Running epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cdb0b872194dd4a88a4af8e7d5e5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8936/1910285087.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mtop_accuracy_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOP_ACCURACY_TRACK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0maccwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                    )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8936/1740864069.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(max_epoch, top_accuracy_track, grandstore, minepoch, epochwindow, accwindow)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mtotaltrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 10\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=330,\n",
    "                    minepoch=150,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                  \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4013c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Run 120 epoch(s)\n",
      "Accuracy MIN: 55.94 / MAX: 90.13\n",
      "\n",
      "Top 5 performing epochs:\n",
      "#1 epoch 119\t||train_acc 90.13%\t||test 89.86%\n",
      "#2 epoch 116\t||train_acc 90.12%\t||test 89.57%\n",
      "#3 epoch 90\t||train_acc 89.69%\t||test 89.5%\n",
      "#4 epoch 112\t||train_acc 89.67%\t||test 89.59%\n",
      "#5 epoch 104\t||train_acc 89.65%\t||test 89.36%\n",
      "\n",
      "Last epoch:\n",
      "epoch 120\t||train_acc 88.64%\t||test 88.55%\n",
      "\n",
      "The model has parameters: 19001332\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {}%\\t||test {}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {}%\\t||test {}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writings done!\n",
      "Files at: grandstore/cifar10_DenseNet169_sp20211103115722.pkl\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "\n",
    "print(\"writings done!\")\n",
    "print(\"Files at: \"+grandstore_string)\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ddef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
