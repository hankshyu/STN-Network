{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auther: Tzu-Han Hsu\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) 2022, Anywhere Door Lab (ADL) and Tzu-Han Hsu\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "#    list of conditions and the following disclaimer.\n",
    "\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "#    this list of conditions and the following disclaimer in the documentation\n",
    "#    and/or other materials provided with the distribution.\n",
    "\n",
    "# 3. Neither the name of the copyright holder nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived from\n",
    "#    this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientNetb3 with 101 classes running on: caltech101\n",
      "Dataset size: Train: 6277, Valid: 1200, Test: 1200\n",
      "{'Faces': 0, 'Faces_easy': 1, 'Leopards': 2, 'Motorbikes': 3, 'accordion': 4, 'airplanes': 5, 'anchor': 6, 'ant': 7, 'barrel': 8, 'bass': 9, 'beaver': 10, 'binocular': 11, 'bonsai': 12, 'brain': 13, 'brontosaurus': 14, 'buddha': 15, 'butterfly': 16, 'camera': 17, 'cannon': 18, 'car_side': 19, 'ceiling_fan': 20, 'cellphone': 21, 'chair': 22, 'chandelier': 23, 'cougar_body': 24, 'cougar_face': 25, 'crab': 26, 'crayfish': 27, 'crocodile': 28, 'crocodile_head': 29, 'cup': 30, 'dalmatian': 31, 'dollar_bill': 32, 'dolphin': 33, 'dragonfly': 34, 'electric_guitar': 35, 'elephant': 36, 'emu': 37, 'euphonium': 38, 'ewer': 39, 'ferry': 40, 'flamingo': 41, 'flamingo_head': 42, 'garfield': 43, 'gerenuk': 44, 'gramophone': 45, 'grand_piano': 46, 'hawksbill': 47, 'headphone': 48, 'hedgehog': 49, 'helicopter': 50, 'ibis': 51, 'inline_skate': 52, 'joshua_tree': 53, 'kangaroo': 54, 'ketch': 55, 'lamp': 56, 'laptop': 57, 'llama': 58, 'lobster': 59, 'lotus': 60, 'mandolin': 61, 'mayfly': 62, 'menorah': 63, 'metronome': 64, 'minaret': 65, 'nautilus': 66, 'octopus': 67, 'okapi': 68, 'pagoda': 69, 'panda': 70, 'pigeon': 71, 'pizza': 72, 'platypus': 73, 'pyramid': 74, 'revolver': 75, 'rhino': 76, 'rooster': 77, 'saxophone': 78, 'schooner': 79, 'scissors': 80, 'scorpion': 81, 'sea_horse': 82, 'snoopy': 83, 'soccer_ball': 84, 'stapler': 85, 'starfish': 86, 'stegosaurus': 87, 'stop_sign': 88, 'strawberry': 89, 'sunflower': 90, 'tick': 91, 'trilobite': 92, 'umbrella': 93, 'watch': 94, 'water_lilly': 95, 'wheelchair': 96, 'wild_cat': 97, 'windsor_chair': 98, 'wrench': 99, 'yin_yang': 100}\n",
      "torch.Size([3, 224, 224])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#vital params\n",
    "\n",
    "\n",
    "\n",
    "dataset_name=\"caltech101\"\n",
    " \n",
    "model_name=\"EfficientNetb3\"\n",
    "version = \"b3\"\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "phi, res, drop_rate = phi_values[version]\n",
    "#hyperparameters\n",
    "batch_size=10\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "image_size=(224,224)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "elif dataset_name == \"caltech101\":\n",
    "    num_classes=101\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n",
    "\n",
    "\n",
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "elif dataset_name == \"caltech101\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/caltech101\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [7477, 1200])\n",
    "    train_set, test_set = data.random_split(train_set,[6277, 1200])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "    \n",
    "    \n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "print(head_train_set.class_to_idx)\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6cfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU() # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4, # squeeze excitation\n",
    "            survival_prob=0.8, # for stochastic depth\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,\n",
    "            ),\n",
    "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            return self.stochastic_depth(self.conv(x)) + inputs\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = math.ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "        )\n",
    "\n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate\n",
    "\n",
    "    def create_features(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4*math.ceil(int(channels*width_factor) / 4)\n",
    "            layers_repeats = math.ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride = stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size//2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.features(x))\n",
    "        return self.classifier(x.view(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.SiLU'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.CNNBlock'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Sigmoid'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.SqueezeExcitation'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.InvertedResidualBlock'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.EfficientNet'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[34mThe model requires: 8.2597 GFLOPS\n",
      "\u001b[0m\n",
      "torch.Size([10, 3, 300, 300])\n",
      "torch.Size([10, 101])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 38, 150, 150]           1,026\n",
      "       BatchNorm2d-2         [-1, 38, 150, 150]              76\n",
      "              SiLU-3         [-1, 38, 150, 150]               0\n",
      "          CNNBlock-4         [-1, 38, 150, 150]               0\n",
      "            Conv2d-5         [-1, 38, 150, 150]             342\n",
      "       BatchNorm2d-6         [-1, 38, 150, 150]              76\n",
      "              SiLU-7         [-1, 38, 150, 150]               0\n",
      "          CNNBlock-8         [-1, 38, 150, 150]               0\n",
      " AdaptiveAvgPool2d-9             [-1, 38, 1, 1]               0\n",
      "           Conv2d-10              [-1, 9, 1, 1]             351\n",
      "             SiLU-11              [-1, 9, 1, 1]               0\n",
      "           Conv2d-12             [-1, 38, 1, 1]             380\n",
      "          Sigmoid-13             [-1, 38, 1, 1]               0\n",
      "SqueezeExcitation-14         [-1, 38, 150, 150]               0\n",
      "           Conv2d-15         [-1, 20, 150, 150]             760\n",
      "      BatchNorm2d-16         [-1, 20, 150, 150]              40\n",
      "InvertedResidualBlock-17         [-1, 20, 150, 150]               0\n",
      "           Conv2d-18         [-1, 20, 150, 150]             180\n",
      "      BatchNorm2d-19         [-1, 20, 150, 150]              40\n",
      "             SiLU-20         [-1, 20, 150, 150]               0\n",
      "         CNNBlock-21         [-1, 20, 150, 150]               0\n",
      "AdaptiveAvgPool2d-22             [-1, 20, 1, 1]               0\n",
      "           Conv2d-23              [-1, 5, 1, 1]             105\n",
      "             SiLU-24              [-1, 5, 1, 1]               0\n",
      "           Conv2d-25             [-1, 20, 1, 1]             120\n",
      "          Sigmoid-26             [-1, 20, 1, 1]               0\n",
      "SqueezeExcitation-27         [-1, 20, 150, 150]               0\n",
      "           Conv2d-28         [-1, 20, 150, 150]             400\n",
      "      BatchNorm2d-29         [-1, 20, 150, 150]              40\n",
      "InvertedResidualBlock-30         [-1, 20, 150, 150]               0\n",
      "           Conv2d-31        [-1, 120, 150, 150]          21,600\n",
      "      BatchNorm2d-32        [-1, 120, 150, 150]             240\n",
      "             SiLU-33        [-1, 120, 150, 150]               0\n",
      "         CNNBlock-34        [-1, 120, 150, 150]               0\n",
      "           Conv2d-35          [-1, 120, 75, 75]           1,080\n",
      "      BatchNorm2d-36          [-1, 120, 75, 75]             240\n",
      "             SiLU-37          [-1, 120, 75, 75]               0\n",
      "         CNNBlock-38          [-1, 120, 75, 75]               0\n",
      "AdaptiveAvgPool2d-39            [-1, 120, 1, 1]               0\n",
      "           Conv2d-40              [-1, 5, 1, 1]             605\n",
      "             SiLU-41              [-1, 5, 1, 1]               0\n",
      "           Conv2d-42            [-1, 120, 1, 1]             720\n",
      "          Sigmoid-43            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-44          [-1, 120, 75, 75]               0\n",
      "           Conv2d-45           [-1, 32, 75, 75]           3,840\n",
      "      BatchNorm2d-46           [-1, 32, 75, 75]              64\n",
      "InvertedResidualBlock-47           [-1, 32, 75, 75]               0\n",
      "           Conv2d-48          [-1, 192, 75, 75]          55,296\n",
      "      BatchNorm2d-49          [-1, 192, 75, 75]             384\n",
      "             SiLU-50          [-1, 192, 75, 75]               0\n",
      "         CNNBlock-51          [-1, 192, 75, 75]               0\n",
      "           Conv2d-52          [-1, 192, 75, 75]           1,728\n",
      "      BatchNorm2d-53          [-1, 192, 75, 75]             384\n",
      "             SiLU-54          [-1, 192, 75, 75]               0\n",
      "         CNNBlock-55          [-1, 192, 75, 75]               0\n",
      "AdaptiveAvgPool2d-56            [-1, 192, 1, 1]               0\n",
      "           Conv2d-57              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-58              [-1, 8, 1, 1]               0\n",
      "           Conv2d-59            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-60            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-61          [-1, 192, 75, 75]               0\n",
      "           Conv2d-62           [-1, 32, 75, 75]           6,144\n",
      "      BatchNorm2d-63           [-1, 32, 75, 75]              64\n",
      "InvertedResidualBlock-64           [-1, 32, 75, 75]               0\n",
      "           Conv2d-65          [-1, 192, 75, 75]          55,296\n",
      "      BatchNorm2d-66          [-1, 192, 75, 75]             384\n",
      "             SiLU-67          [-1, 192, 75, 75]               0\n",
      "         CNNBlock-68          [-1, 192, 75, 75]               0\n",
      "           Conv2d-69          [-1, 192, 75, 75]           1,728\n",
      "      BatchNorm2d-70          [-1, 192, 75, 75]             384\n",
      "             SiLU-71          [-1, 192, 75, 75]               0\n",
      "         CNNBlock-72          [-1, 192, 75, 75]               0\n",
      "AdaptiveAvgPool2d-73            [-1, 192, 1, 1]               0\n",
      "           Conv2d-74              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-75              [-1, 8, 1, 1]               0\n",
      "           Conv2d-76            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-77            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-78          [-1, 192, 75, 75]               0\n",
      "           Conv2d-79           [-1, 32, 75, 75]           6,144\n",
      "      BatchNorm2d-80           [-1, 32, 75, 75]              64\n",
      "InvertedResidualBlock-81           [-1, 32, 75, 75]               0\n",
      "           Conv2d-82          [-1, 192, 75, 75]          55,296\n",
      "      BatchNorm2d-83          [-1, 192, 75, 75]             384\n",
      "             SiLU-84          [-1, 192, 75, 75]               0\n",
      "         CNNBlock-85          [-1, 192, 75, 75]               0\n",
      "           Conv2d-86          [-1, 192, 38, 38]           4,800\n",
      "      BatchNorm2d-87          [-1, 192, 38, 38]             384\n",
      "             SiLU-88          [-1, 192, 38, 38]               0\n",
      "         CNNBlock-89          [-1, 192, 38, 38]               0\n",
      "AdaptiveAvgPool2d-90            [-1, 192, 1, 1]               0\n",
      "           Conv2d-91              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-92              [-1, 8, 1, 1]               0\n",
      "           Conv2d-93            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-94            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-95          [-1, 192, 38, 38]               0\n",
      "           Conv2d-96           [-1, 48, 38, 38]           9,216\n",
      "      BatchNorm2d-97           [-1, 48, 38, 38]              96\n",
      "InvertedResidualBlock-98           [-1, 48, 38, 38]               0\n",
      "           Conv2d-99          [-1, 288, 38, 38]         124,416\n",
      "     BatchNorm2d-100          [-1, 288, 38, 38]             576\n",
      "            SiLU-101          [-1, 288, 38, 38]               0\n",
      "        CNNBlock-102          [-1, 288, 38, 38]               0\n",
      "          Conv2d-103          [-1, 288, 38, 38]           7,200\n",
      "     BatchNorm2d-104          [-1, 288, 38, 38]             576\n",
      "            SiLU-105          [-1, 288, 38, 38]               0\n",
      "        CNNBlock-106          [-1, 288, 38, 38]               0\n",
      "AdaptiveAvgPool2d-107            [-1, 288, 1, 1]               0\n",
      "          Conv2d-108             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-109             [-1, 12, 1, 1]               0\n",
      "          Conv2d-110            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-111            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-112          [-1, 288, 38, 38]               0\n",
      "          Conv2d-113           [-1, 48, 38, 38]          13,824\n",
      "     BatchNorm2d-114           [-1, 48, 38, 38]              96\n",
      "InvertedResidualBlock-115           [-1, 48, 38, 38]               0\n",
      "          Conv2d-116          [-1, 288, 38, 38]         124,416\n",
      "     BatchNorm2d-117          [-1, 288, 38, 38]             576\n",
      "            SiLU-118          [-1, 288, 38, 38]               0\n",
      "        CNNBlock-119          [-1, 288, 38, 38]               0\n",
      "          Conv2d-120          [-1, 288, 38, 38]           7,200\n",
      "     BatchNorm2d-121          [-1, 288, 38, 38]             576\n",
      "            SiLU-122          [-1, 288, 38, 38]               0\n",
      "        CNNBlock-123          [-1, 288, 38, 38]               0\n",
      "AdaptiveAvgPool2d-124            [-1, 288, 1, 1]               0\n",
      "          Conv2d-125             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-126             [-1, 12, 1, 1]               0\n",
      "          Conv2d-127            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-128            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-129          [-1, 288, 38, 38]               0\n",
      "          Conv2d-130           [-1, 48, 38, 38]          13,824\n",
      "     BatchNorm2d-131           [-1, 48, 38, 38]              96\n",
      "InvertedResidualBlock-132           [-1, 48, 38, 38]               0\n",
      "          Conv2d-133          [-1, 288, 38, 38]         124,416\n",
      "     BatchNorm2d-134          [-1, 288, 38, 38]             576\n",
      "            SiLU-135          [-1, 288, 38, 38]               0\n",
      "        CNNBlock-136          [-1, 288, 38, 38]               0\n",
      "          Conv2d-137          [-1, 288, 19, 19]           2,592\n",
      "     BatchNorm2d-138          [-1, 288, 19, 19]             576\n",
      "            SiLU-139          [-1, 288, 19, 19]               0\n",
      "        CNNBlock-140          [-1, 288, 19, 19]               0\n",
      "AdaptiveAvgPool2d-141            [-1, 288, 1, 1]               0\n",
      "          Conv2d-142             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-143             [-1, 12, 1, 1]               0\n",
      "          Conv2d-144            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-145            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-146          [-1, 288, 19, 19]               0\n",
      "          Conv2d-147           [-1, 96, 19, 19]          27,648\n",
      "     BatchNorm2d-148           [-1, 96, 19, 19]             192\n",
      "InvertedResidualBlock-149           [-1, 96, 19, 19]               0\n",
      "          Conv2d-150          [-1, 576, 19, 19]         497,664\n",
      "     BatchNorm2d-151          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-152          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-153          [-1, 576, 19, 19]               0\n",
      "          Conv2d-154          [-1, 576, 19, 19]           5,184\n",
      "     BatchNorm2d-155          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-156          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-157          [-1, 576, 19, 19]               0\n",
      "AdaptiveAvgPool2d-158            [-1, 576, 1, 1]               0\n",
      "          Conv2d-159             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-160             [-1, 24, 1, 1]               0\n",
      "          Conv2d-161            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-162            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-163          [-1, 576, 19, 19]               0\n",
      "          Conv2d-164           [-1, 96, 19, 19]          55,296\n",
      "     BatchNorm2d-165           [-1, 96, 19, 19]             192\n",
      "InvertedResidualBlock-166           [-1, 96, 19, 19]               0\n",
      "          Conv2d-167          [-1, 576, 19, 19]         497,664\n",
      "     BatchNorm2d-168          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-169          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-170          [-1, 576, 19, 19]               0\n",
      "          Conv2d-171          [-1, 576, 19, 19]           5,184\n",
      "     BatchNorm2d-172          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-173          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-174          [-1, 576, 19, 19]               0\n",
      "AdaptiveAvgPool2d-175            [-1, 576, 1, 1]               0\n",
      "          Conv2d-176             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-177             [-1, 24, 1, 1]               0\n",
      "          Conv2d-178            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-179            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-180          [-1, 576, 19, 19]               0\n",
      "          Conv2d-181           [-1, 96, 19, 19]          55,296\n",
      "     BatchNorm2d-182           [-1, 96, 19, 19]             192\n",
      "InvertedResidualBlock-183           [-1, 96, 19, 19]               0\n",
      "          Conv2d-184          [-1, 576, 19, 19]         497,664\n",
      "     BatchNorm2d-185          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-186          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-187          [-1, 576, 19, 19]               0\n",
      "          Conv2d-188          [-1, 576, 19, 19]           5,184\n",
      "     BatchNorm2d-189          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-190          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-191          [-1, 576, 19, 19]               0\n",
      "AdaptiveAvgPool2d-192            [-1, 576, 1, 1]               0\n",
      "          Conv2d-193             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-194             [-1, 24, 1, 1]               0\n",
      "          Conv2d-195            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-196            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-197          [-1, 576, 19, 19]               0\n",
      "          Conv2d-198           [-1, 96, 19, 19]          55,296\n",
      "     BatchNorm2d-199           [-1, 96, 19, 19]             192\n",
      "InvertedResidualBlock-200           [-1, 96, 19, 19]               0\n",
      "          Conv2d-201          [-1, 576, 19, 19]         497,664\n",
      "     BatchNorm2d-202          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-203          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-204          [-1, 576, 19, 19]               0\n",
      "          Conv2d-205          [-1, 576, 19, 19]           5,184\n",
      "     BatchNorm2d-206          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-207          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-208          [-1, 576, 19, 19]               0\n",
      "AdaptiveAvgPool2d-209            [-1, 576, 1, 1]               0\n",
      "          Conv2d-210             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-211             [-1, 24, 1, 1]               0\n",
      "          Conv2d-212            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-213            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-214          [-1, 576, 19, 19]               0\n",
      "          Conv2d-215           [-1, 96, 19, 19]          55,296\n",
      "     BatchNorm2d-216           [-1, 96, 19, 19]             192\n",
      "InvertedResidualBlock-217           [-1, 96, 19, 19]               0\n",
      "          Conv2d-218          [-1, 576, 19, 19]         497,664\n",
      "     BatchNorm2d-219          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-220          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-221          [-1, 576, 19, 19]               0\n",
      "          Conv2d-222          [-1, 576, 19, 19]          14,400\n",
      "     BatchNorm2d-223          [-1, 576, 19, 19]           1,152\n",
      "            SiLU-224          [-1, 576, 19, 19]               0\n",
      "        CNNBlock-225          [-1, 576, 19, 19]               0\n",
      "AdaptiveAvgPool2d-226            [-1, 576, 1, 1]               0\n",
      "          Conv2d-227             [-1, 24, 1, 1]          13,848\n",
      "            SiLU-228             [-1, 24, 1, 1]               0\n",
      "          Conv2d-229            [-1, 576, 1, 1]          14,400\n",
      "         Sigmoid-230            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-231          [-1, 576, 19, 19]               0\n",
      "          Conv2d-232          [-1, 136, 19, 19]          78,336\n",
      "     BatchNorm2d-233          [-1, 136, 19, 19]             272\n",
      "InvertedResidualBlock-234          [-1, 136, 19, 19]               0\n",
      "          Conv2d-235          [-1, 816, 19, 19]         998,784\n",
      "     BatchNorm2d-236          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-237          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-238          [-1, 816, 19, 19]               0\n",
      "          Conv2d-239          [-1, 816, 19, 19]          20,400\n",
      "     BatchNorm2d-240          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-241          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-242          [-1, 816, 19, 19]               0\n",
      "AdaptiveAvgPool2d-243            [-1, 816, 1, 1]               0\n",
      "          Conv2d-244             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-245             [-1, 34, 1, 1]               0\n",
      "          Conv2d-246            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-247            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-248          [-1, 816, 19, 19]               0\n",
      "          Conv2d-249          [-1, 136, 19, 19]         110,976\n",
      "     BatchNorm2d-250          [-1, 136, 19, 19]             272\n",
      "InvertedResidualBlock-251          [-1, 136, 19, 19]               0\n",
      "          Conv2d-252          [-1, 816, 19, 19]         998,784\n",
      "     BatchNorm2d-253          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-254          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-255          [-1, 816, 19, 19]               0\n",
      "          Conv2d-256          [-1, 816, 19, 19]          20,400\n",
      "     BatchNorm2d-257          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-258          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-259          [-1, 816, 19, 19]               0\n",
      "AdaptiveAvgPool2d-260            [-1, 816, 1, 1]               0\n",
      "          Conv2d-261             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-262             [-1, 34, 1, 1]               0\n",
      "          Conv2d-263            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-264            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-265          [-1, 816, 19, 19]               0\n",
      "          Conv2d-266          [-1, 136, 19, 19]         110,976\n",
      "     BatchNorm2d-267          [-1, 136, 19, 19]             272\n",
      "InvertedResidualBlock-268          [-1, 136, 19, 19]               0\n",
      "          Conv2d-269          [-1, 816, 19, 19]         998,784\n",
      "     BatchNorm2d-270          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-271          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-272          [-1, 816, 19, 19]               0\n",
      "          Conv2d-273          [-1, 816, 19, 19]          20,400\n",
      "     BatchNorm2d-274          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-275          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-276          [-1, 816, 19, 19]               0\n",
      "AdaptiveAvgPool2d-277            [-1, 816, 1, 1]               0\n",
      "          Conv2d-278             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-279             [-1, 34, 1, 1]               0\n",
      "          Conv2d-280            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-281            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-282          [-1, 816, 19, 19]               0\n",
      "          Conv2d-283          [-1, 136, 19, 19]         110,976\n",
      "     BatchNorm2d-284          [-1, 136, 19, 19]             272\n",
      "InvertedResidualBlock-285          [-1, 136, 19, 19]               0\n",
      "          Conv2d-286          [-1, 816, 19, 19]         998,784\n",
      "     BatchNorm2d-287          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-288          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-289          [-1, 816, 19, 19]               0\n",
      "          Conv2d-290          [-1, 816, 19, 19]          20,400\n",
      "     BatchNorm2d-291          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-292          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-293          [-1, 816, 19, 19]               0\n",
      "AdaptiveAvgPool2d-294            [-1, 816, 1, 1]               0\n",
      "          Conv2d-295             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-296             [-1, 34, 1, 1]               0\n",
      "          Conv2d-297            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-298            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-299          [-1, 816, 19, 19]               0\n",
      "          Conv2d-300          [-1, 136, 19, 19]         110,976\n",
      "     BatchNorm2d-301          [-1, 136, 19, 19]             272\n",
      "InvertedResidualBlock-302          [-1, 136, 19, 19]               0\n",
      "          Conv2d-303          [-1, 816, 19, 19]         998,784\n",
      "     BatchNorm2d-304          [-1, 816, 19, 19]           1,632\n",
      "            SiLU-305          [-1, 816, 19, 19]               0\n",
      "        CNNBlock-306          [-1, 816, 19, 19]               0\n",
      "          Conv2d-307          [-1, 816, 10, 10]          20,400\n",
      "     BatchNorm2d-308          [-1, 816, 10, 10]           1,632\n",
      "            SiLU-309          [-1, 816, 10, 10]               0\n",
      "        CNNBlock-310          [-1, 816, 10, 10]               0\n",
      "AdaptiveAvgPool2d-311            [-1, 816, 1, 1]               0\n",
      "          Conv2d-312             [-1, 34, 1, 1]          27,778\n",
      "            SiLU-313             [-1, 34, 1, 1]               0\n",
      "          Conv2d-314            [-1, 816, 1, 1]          28,560\n",
      "         Sigmoid-315            [-1, 816, 1, 1]               0\n",
      "SqueezeExcitation-316          [-1, 816, 10, 10]               0\n",
      "          Conv2d-317          [-1, 232, 10, 10]         189,312\n",
      "     BatchNorm2d-318          [-1, 232, 10, 10]             464\n",
      "InvertedResidualBlock-319          [-1, 232, 10, 10]               0\n",
      "          Conv2d-320         [-1, 1392, 10, 10]       2,906,496\n",
      "     BatchNorm2d-321         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-322         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-323         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-324         [-1, 1392, 10, 10]          34,800\n",
      "     BatchNorm2d-325         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-326         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-327         [-1, 1392, 10, 10]               0\n",
      "AdaptiveAvgPool2d-328           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-329             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-330             [-1, 58, 1, 1]               0\n",
      "          Conv2d-331           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-332           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-333         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-334          [-1, 232, 10, 10]         322,944\n",
      "     BatchNorm2d-335          [-1, 232, 10, 10]             464\n",
      "InvertedResidualBlock-336          [-1, 232, 10, 10]               0\n",
      "          Conv2d-337         [-1, 1392, 10, 10]       2,906,496\n",
      "     BatchNorm2d-338         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-339         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-340         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-341         [-1, 1392, 10, 10]          34,800\n",
      "     BatchNorm2d-342         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-343         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-344         [-1, 1392, 10, 10]               0\n",
      "AdaptiveAvgPool2d-345           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-346             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-347             [-1, 58, 1, 1]               0\n",
      "          Conv2d-348           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-349           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-350         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-351          [-1, 232, 10, 10]         322,944\n",
      "     BatchNorm2d-352          [-1, 232, 10, 10]             464\n",
      "InvertedResidualBlock-353          [-1, 232, 10, 10]               0\n",
      "          Conv2d-354         [-1, 1392, 10, 10]       2,906,496\n",
      "     BatchNorm2d-355         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-356         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-357         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-358         [-1, 1392, 10, 10]          34,800\n",
      "     BatchNorm2d-359         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-360         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-361         [-1, 1392, 10, 10]               0\n",
      "AdaptiveAvgPool2d-362           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-363             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-364             [-1, 58, 1, 1]               0\n",
      "          Conv2d-365           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-366           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-367         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-368          [-1, 232, 10, 10]         322,944\n",
      "     BatchNorm2d-369          [-1, 232, 10, 10]             464\n",
      "InvertedResidualBlock-370          [-1, 232, 10, 10]               0\n",
      "          Conv2d-371         [-1, 1392, 10, 10]       2,906,496\n",
      "     BatchNorm2d-372         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-373         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-374         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-375         [-1, 1392, 10, 10]          34,800\n",
      "     BatchNorm2d-376         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-377         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-378         [-1, 1392, 10, 10]               0\n",
      "AdaptiveAvgPool2d-379           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-380             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-381             [-1, 58, 1, 1]               0\n",
      "          Conv2d-382           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-383           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-384         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-385          [-1, 232, 10, 10]         322,944\n",
      "     BatchNorm2d-386          [-1, 232, 10, 10]             464\n",
      "InvertedResidualBlock-387          [-1, 232, 10, 10]               0\n",
      "          Conv2d-388         [-1, 1392, 10, 10]       2,906,496\n",
      "     BatchNorm2d-389         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-390         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-391         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-392         [-1, 1392, 10, 10]          34,800\n",
      "     BatchNorm2d-393         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-394         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-395         [-1, 1392, 10, 10]               0\n",
      "AdaptiveAvgPool2d-396           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-397             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-398             [-1, 58, 1, 1]               0\n",
      "          Conv2d-399           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-400           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-401         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-402          [-1, 232, 10, 10]         322,944\n",
      "     BatchNorm2d-403          [-1, 232, 10, 10]             464\n",
      "InvertedResidualBlock-404          [-1, 232, 10, 10]               0\n",
      "          Conv2d-405         [-1, 1392, 10, 10]       2,906,496\n",
      "     BatchNorm2d-406         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-407         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-408         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-409         [-1, 1392, 10, 10]          12,528\n",
      "     BatchNorm2d-410         [-1, 1392, 10, 10]           2,784\n",
      "            SiLU-411         [-1, 1392, 10, 10]               0\n",
      "        CNNBlock-412         [-1, 1392, 10, 10]               0\n",
      "AdaptiveAvgPool2d-413           [-1, 1392, 1, 1]               0\n",
      "          Conv2d-414             [-1, 58, 1, 1]          80,794\n",
      "            SiLU-415             [-1, 58, 1, 1]               0\n",
      "          Conv2d-416           [-1, 1392, 1, 1]          82,128\n",
      "         Sigmoid-417           [-1, 1392, 1, 1]               0\n",
      "SqueezeExcitation-418         [-1, 1392, 10, 10]               0\n",
      "          Conv2d-419          [-1, 388, 10, 10]         540,096\n",
      "     BatchNorm2d-420          [-1, 388, 10, 10]             776\n",
      "InvertedResidualBlock-421          [-1, 388, 10, 10]               0\n",
      "          Conv2d-422         [-1, 2328, 10, 10]       8,129,376\n",
      "     BatchNorm2d-423         [-1, 2328, 10, 10]           4,656\n",
      "            SiLU-424         [-1, 2328, 10, 10]               0\n",
      "        CNNBlock-425         [-1, 2328, 10, 10]               0\n",
      "          Conv2d-426         [-1, 2328, 10, 10]          20,952\n",
      "     BatchNorm2d-427         [-1, 2328, 10, 10]           4,656\n",
      "            SiLU-428         [-1, 2328, 10, 10]               0\n",
      "        CNNBlock-429         [-1, 2328, 10, 10]               0\n",
      "AdaptiveAvgPool2d-430           [-1, 2328, 1, 1]               0\n",
      "          Conv2d-431             [-1, 97, 1, 1]         225,913\n",
      "            SiLU-432             [-1, 97, 1, 1]               0\n",
      "          Conv2d-433           [-1, 2328, 1, 1]         228,144\n",
      "         Sigmoid-434           [-1, 2328, 1, 1]               0\n",
      "SqueezeExcitation-435         [-1, 2328, 10, 10]               0\n",
      "          Conv2d-436          [-1, 388, 10, 10]         903,264\n",
      "     BatchNorm2d-437          [-1, 388, 10, 10]             776\n",
      "InvertedResidualBlock-438          [-1, 388, 10, 10]               0\n",
      "          Conv2d-439         [-1, 1549, 10, 10]         601,012\n",
      "     BatchNorm2d-440         [-1, 1549, 10, 10]           3,098\n",
      "            SiLU-441         [-1, 1549, 10, 10]               0\n",
      "        CNNBlock-442         [-1, 1549, 10, 10]               0\n",
      "AdaptiveAvgPool2d-443           [-1, 1549, 1, 1]               0\n",
      "         Dropout-444                 [-1, 1549]               0\n",
      "          Linear-445                  [-1, 101]         156,550\n",
      "================================================================\n",
      "Total params: 40,789,556\n",
      "Trainable params: 40,789,556\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 744.84\n",
      "Params size (MB): 155.60\n",
      "Estimated Total Size (MB): 901.47\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model shape ready\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet(\n",
    "    version=version,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "from thop import profile\n",
    "import colorama\n",
    "from colorama import Fore,Style\n",
    "def getGFLOPS(test_model):\n",
    "    model=test_model.to(device)\n",
    "    input = torch.randn(1, 3, res, res).to(device)\n",
    "    macs, params = profile(model, inputs=(input, ))\n",
    "\n",
    "    print(Fore.BLUE + \"The model requires: {:.4f} GFLOPS\".format(macs/1000_000_000))\n",
    "    print(Style.RESET_ALL)\n",
    "    return macs\n",
    "getGFLOPS(model)\n",
    "#pretesting model for shape\n",
    "x=torch.randn(batch_size,3,res,res)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(summary(model, input_size=(3, res, res)))\n",
    "print(\"model shape ready\")\n",
    "\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee819dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs/EfficientNetb320211222201932\n",
      "grandstore_string:\n",
      "grandstore/caltech101_EfficientNetb320211222201932.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string:\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c876d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3871e11e43914a249dd3e0137f04dfc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17972/50271932.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mtop_accuracy_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOP_ACCURACY_TRACK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0maccwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                    )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17972/1740864069.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(max_epoch, top_accuracy_track, grandstore, minepoch, epochwindow, accwindow)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mtotaltrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 5\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=200,\n",
    "                    minepoch=120,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                  \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b45315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Run 120 epoch(s)\n",
      "Accuracy MIN: 49.11 / MAX: 87.11\n",
      "\n",
      "Top 5 performing epochs:\n",
      "#1 epoch 81\t||train_acc 87.11%\t||test 86.31%\n",
      "#2 epoch 117\t||train_acc 87.08%\t||test 86.79%\n",
      "#3 epoch 115\t||train_acc 86.85%\t||test 86.68%\n",
      "#4 epoch 113\t||train_acc 86.78%\t||test 86.58%\n",
      "#5 epoch 92\t||train_acc 86.74%\t||test 86.49%\n",
      "\n",
      "Last epoch:\n",
      "epoch 120\t||train_acc 85.87%\t||test 86.4%\n",
      "\n",
      "The model has parameters: 40648506\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {}%\\t||test {}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {}%\\t||test {}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writings done!\n",
      "Files at: grandstore/cifar10_EfficientNetb320211021014801.pkl\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "\n",
    "print(\"writings done!\")\n",
    "print(\"Files at: \"+grandstore_string)\n",
    "\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a341d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
