{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ResNet152_sp with 10 classes running on: cifar10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "\n",
    "#vital params\n",
    "\n",
    " \n",
    "model_name=\"ResNet152_sp\"\n",
    "\n",
    "dataset_name=\"cifar10\"\n",
    "\n",
    "#hyperparameters\n",
    "batch_size=20\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "image_size=(224,224)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38958e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset size: Train: 40000, Valid: 10000, Test: 10000\n",
      "torch.Size([3, 224, 224])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "    print(head_train_set.class_to_idx)\n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bccfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:4044: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3982: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
    "    ):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(\n",
    "            block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(27040, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        \n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "\n",
    "        xs = self.localization(x)\n",
    "\n",
    "        xs = xs.view(-1, 10 * 52 * 52)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        self.stn(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "def ResNet50(img_channel=3, num_classes=num_classes):\n",
    "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n",
    "\n",
    "\n",
    "def ResNet101(img_channel=3, num_classes=num_classes):\n",
    "    return ResNet(block, [3, 4, 23, 3], img_channel, num_classes)\n",
    "\n",
    "\n",
    "def ResNet152(img_channel=3, num_classes=num_classes):\n",
    "    return ResNet(block, [3, 8, 36, 3], img_channel, num_classes)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet101(img_channel=3, num_classes=num_classes)\n",
    "    y = net(torch.randn(4, 3, 224, 224)).to(device)\n",
    "    print(y.size())\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64, 10])\n",
      "model shape ready\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 218, 218]           1,184\n",
      "         MaxPool2d-2          [-1, 8, 109, 109]               0\n",
      "              ReLU-3          [-1, 8, 109, 109]               0\n",
      "            Conv2d-4         [-1, 10, 105, 105]           2,010\n",
      "         MaxPool2d-5           [-1, 10, 52, 52]               0\n",
      "              ReLU-6           [-1, 10, 52, 52]               0\n",
      "            Linear-7                   [-1, 32]         865,312\n",
      "              ReLU-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 6]             198\n",
      "           Conv2d-10         [-1, 64, 112, 112]           9,408\n",
      "      BatchNorm2d-11         [-1, 64, 112, 112]             128\n",
      "             ReLU-12         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]           4,096\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "             ReLU-16           [-1, 64, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-21          [-1, 256, 56, 56]             512\n",
      "           Conv2d-22          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
      "             ReLU-24          [-1, 256, 56, 56]               0\n",
      "            block-25          [-1, 256, 56, 56]               0\n",
      "           Conv2d-26           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-27           [-1, 64, 56, 56]             128\n",
      "             ReLU-28           [-1, 64, 56, 56]               0\n",
      "           Conv2d-29           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "           Conv2d-32          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-33          [-1, 256, 56, 56]             512\n",
      "             ReLU-34          [-1, 256, 56, 56]               0\n",
      "            block-35          [-1, 256, 56, 56]               0\n",
      "           Conv2d-36           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-37           [-1, 64, 56, 56]             128\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "           Conv2d-39           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-40           [-1, 64, 56, 56]             128\n",
      "             ReLU-41           [-1, 64, 56, 56]               0\n",
      "           Conv2d-42          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-43          [-1, 256, 56, 56]             512\n",
      "             ReLU-44          [-1, 256, 56, 56]               0\n",
      "            block-45          [-1, 256, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-47          [-1, 128, 56, 56]             256\n",
      "             ReLU-48          [-1, 128, 56, 56]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-53          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-54          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-55          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-56          [-1, 512, 28, 28]               0\n",
      "            block-57          [-1, 512, 28, 28]               0\n",
      "           Conv2d-58          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-59          [-1, 128, 28, 28]             256\n",
      "             ReLU-60          [-1, 128, 28, 28]               0\n",
      "           Conv2d-61          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-62          [-1, 128, 28, 28]             256\n",
      "             ReLU-63          [-1, 128, 28, 28]               0\n",
      "           Conv2d-64          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-65          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-66          [-1, 512, 28, 28]               0\n",
      "            block-67          [-1, 512, 28, 28]               0\n",
      "           Conv2d-68          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-69          [-1, 128, 28, 28]             256\n",
      "             ReLU-70          [-1, 128, 28, 28]               0\n",
      "           Conv2d-71          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-72          [-1, 128, 28, 28]             256\n",
      "             ReLU-73          [-1, 128, 28, 28]               0\n",
      "           Conv2d-74          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-75          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-76          [-1, 512, 28, 28]               0\n",
      "            block-77          [-1, 512, 28, 28]               0\n",
      "           Conv2d-78          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
      "             ReLU-80          [-1, 128, 28, 28]               0\n",
      "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
      "             ReLU-83          [-1, 128, 28, 28]               0\n",
      "           Conv2d-84          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-85          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-86          [-1, 512, 28, 28]               0\n",
      "            block-87          [-1, 512, 28, 28]               0\n",
      "           Conv2d-88          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-89          [-1, 128, 28, 28]             256\n",
      "             ReLU-90          [-1, 128, 28, 28]               0\n",
      "           Conv2d-91          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-92          [-1, 128, 28, 28]             256\n",
      "             ReLU-93          [-1, 128, 28, 28]               0\n",
      "           Conv2d-94          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-95          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-96          [-1, 512, 28, 28]               0\n",
      "            block-97          [-1, 512, 28, 28]               0\n",
      "           Conv2d-98          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-99          [-1, 128, 28, 28]             256\n",
      "            ReLU-100          [-1, 128, 28, 28]               0\n",
      "          Conv2d-101          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-102          [-1, 128, 28, 28]             256\n",
      "            ReLU-103          [-1, 128, 28, 28]               0\n",
      "          Conv2d-104          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-105          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-106          [-1, 512, 28, 28]               0\n",
      "           block-107          [-1, 512, 28, 28]               0\n",
      "          Conv2d-108          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-109          [-1, 128, 28, 28]             256\n",
      "            ReLU-110          [-1, 128, 28, 28]               0\n",
      "          Conv2d-111          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-112          [-1, 128, 28, 28]             256\n",
      "            ReLU-113          [-1, 128, 28, 28]               0\n",
      "          Conv2d-114          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-115          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-116          [-1, 512, 28, 28]               0\n",
      "           block-117          [-1, 512, 28, 28]               0\n",
      "          Conv2d-118          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-119          [-1, 128, 28, 28]             256\n",
      "            ReLU-120          [-1, 128, 28, 28]               0\n",
      "          Conv2d-121          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-122          [-1, 128, 28, 28]             256\n",
      "            ReLU-123          [-1, 128, 28, 28]               0\n",
      "          Conv2d-124          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-125          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-126          [-1, 512, 28, 28]               0\n",
      "           block-127          [-1, 512, 28, 28]               0\n",
      "          Conv2d-128          [-1, 256, 28, 28]         131,072\n",
      "     BatchNorm2d-129          [-1, 256, 28, 28]             512\n",
      "            ReLU-130          [-1, 256, 28, 28]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-135         [-1, 1024, 14, 14]           2,048\n",
      "          Conv2d-136         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-137         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-138         [-1, 1024, 14, 14]               0\n",
      "           block-139         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-140          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-141          [-1, 256, 14, 14]             512\n",
      "            ReLU-142          [-1, 256, 14, 14]               0\n",
      "          Conv2d-143          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-144          [-1, 256, 14, 14]             512\n",
      "            ReLU-145          [-1, 256, 14, 14]               0\n",
      "          Conv2d-146         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-147         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-148         [-1, 1024, 14, 14]               0\n",
      "           block-149         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-150          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
      "            ReLU-152          [-1, 256, 14, 14]               0\n",
      "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
      "            ReLU-155          [-1, 256, 14, 14]               0\n",
      "          Conv2d-156         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-157         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-158         [-1, 1024, 14, 14]               0\n",
      "           block-159         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-160          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-161          [-1, 256, 14, 14]             512\n",
      "            ReLU-162          [-1, 256, 14, 14]               0\n",
      "          Conv2d-163          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-164          [-1, 256, 14, 14]             512\n",
      "            ReLU-165          [-1, 256, 14, 14]               0\n",
      "          Conv2d-166         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-167         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-168         [-1, 1024, 14, 14]               0\n",
      "           block-169         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-170          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-171          [-1, 256, 14, 14]             512\n",
      "            ReLU-172          [-1, 256, 14, 14]               0\n",
      "          Conv2d-173          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-174          [-1, 256, 14, 14]             512\n",
      "            ReLU-175          [-1, 256, 14, 14]               0\n",
      "          Conv2d-176         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-177         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-178         [-1, 1024, 14, 14]               0\n",
      "           block-179         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-180          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-181          [-1, 256, 14, 14]             512\n",
      "            ReLU-182          [-1, 256, 14, 14]               0\n",
      "          Conv2d-183          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-184          [-1, 256, 14, 14]             512\n",
      "            ReLU-185          [-1, 256, 14, 14]               0\n",
      "          Conv2d-186         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-187         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-188         [-1, 1024, 14, 14]               0\n",
      "           block-189         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-190          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
      "            ReLU-192          [-1, 256, 14, 14]               0\n",
      "          Conv2d-193          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-194          [-1, 256, 14, 14]             512\n",
      "            ReLU-195          [-1, 256, 14, 14]               0\n",
      "          Conv2d-196         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-197         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-198         [-1, 1024, 14, 14]               0\n",
      "           block-199         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-200          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-201          [-1, 256, 14, 14]             512\n",
      "            ReLU-202          [-1, 256, 14, 14]               0\n",
      "          Conv2d-203          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-204          [-1, 256, 14, 14]             512\n",
      "            ReLU-205          [-1, 256, 14, 14]               0\n",
      "          Conv2d-206         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-207         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-208         [-1, 1024, 14, 14]               0\n",
      "           block-209         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-210          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-211          [-1, 256, 14, 14]             512\n",
      "            ReLU-212          [-1, 256, 14, 14]               0\n",
      "          Conv2d-213          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-214          [-1, 256, 14, 14]             512\n",
      "            ReLU-215          [-1, 256, 14, 14]               0\n",
      "          Conv2d-216         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-217         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-218         [-1, 1024, 14, 14]               0\n",
      "           block-219         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-220          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-221          [-1, 256, 14, 14]             512\n",
      "            ReLU-222          [-1, 256, 14, 14]               0\n",
      "          Conv2d-223          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-224          [-1, 256, 14, 14]             512\n",
      "            ReLU-225          [-1, 256, 14, 14]               0\n",
      "          Conv2d-226         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-227         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-228         [-1, 1024, 14, 14]               0\n",
      "           block-229         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-230          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-231          [-1, 256, 14, 14]             512\n",
      "            ReLU-232          [-1, 256, 14, 14]               0\n",
      "          Conv2d-233          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-234          [-1, 256, 14, 14]             512\n",
      "            ReLU-235          [-1, 256, 14, 14]               0\n",
      "          Conv2d-236         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-237         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-238         [-1, 1024, 14, 14]               0\n",
      "           block-239         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-240          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-241          [-1, 256, 14, 14]             512\n",
      "            ReLU-242          [-1, 256, 14, 14]               0\n",
      "          Conv2d-243          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-244          [-1, 256, 14, 14]             512\n",
      "            ReLU-245          [-1, 256, 14, 14]               0\n",
      "          Conv2d-246         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-247         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-248         [-1, 1024, 14, 14]               0\n",
      "           block-249         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-250          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-251          [-1, 256, 14, 14]             512\n",
      "            ReLU-252          [-1, 256, 14, 14]               0\n",
      "          Conv2d-253          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-254          [-1, 256, 14, 14]             512\n",
      "            ReLU-255          [-1, 256, 14, 14]               0\n",
      "          Conv2d-256         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-257         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-258         [-1, 1024, 14, 14]               0\n",
      "           block-259         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-260          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-261          [-1, 256, 14, 14]             512\n",
      "            ReLU-262          [-1, 256, 14, 14]               0\n",
      "          Conv2d-263          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-264          [-1, 256, 14, 14]             512\n",
      "            ReLU-265          [-1, 256, 14, 14]               0\n",
      "          Conv2d-266         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-267         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-268         [-1, 1024, 14, 14]               0\n",
      "           block-269         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-270          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-271          [-1, 256, 14, 14]             512\n",
      "            ReLU-272          [-1, 256, 14, 14]               0\n",
      "          Conv2d-273          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-274          [-1, 256, 14, 14]             512\n",
      "            ReLU-275          [-1, 256, 14, 14]               0\n",
      "          Conv2d-276         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-277         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-278         [-1, 1024, 14, 14]               0\n",
      "           block-279         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-280          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-281          [-1, 256, 14, 14]             512\n",
      "            ReLU-282          [-1, 256, 14, 14]               0\n",
      "          Conv2d-283          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-284          [-1, 256, 14, 14]             512\n",
      "            ReLU-285          [-1, 256, 14, 14]               0\n",
      "          Conv2d-286         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-287         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-288         [-1, 1024, 14, 14]               0\n",
      "           block-289         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-290          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-291          [-1, 256, 14, 14]             512\n",
      "            ReLU-292          [-1, 256, 14, 14]               0\n",
      "          Conv2d-293          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-294          [-1, 256, 14, 14]             512\n",
      "            ReLU-295          [-1, 256, 14, 14]               0\n",
      "          Conv2d-296         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-297         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-298         [-1, 1024, 14, 14]               0\n",
      "           block-299         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-300          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-301          [-1, 256, 14, 14]             512\n",
      "            ReLU-302          [-1, 256, 14, 14]               0\n",
      "          Conv2d-303          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-304          [-1, 256, 14, 14]             512\n",
      "            ReLU-305          [-1, 256, 14, 14]               0\n",
      "          Conv2d-306         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-307         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-308         [-1, 1024, 14, 14]               0\n",
      "           block-309         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-310          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-311          [-1, 256, 14, 14]             512\n",
      "            ReLU-312          [-1, 256, 14, 14]               0\n",
      "          Conv2d-313          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-314          [-1, 256, 14, 14]             512\n",
      "            ReLU-315          [-1, 256, 14, 14]               0\n",
      "          Conv2d-316         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-317         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-318         [-1, 1024, 14, 14]               0\n",
      "           block-319         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-320          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-321          [-1, 256, 14, 14]             512\n",
      "            ReLU-322          [-1, 256, 14, 14]               0\n",
      "          Conv2d-323          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-324          [-1, 256, 14, 14]             512\n",
      "            ReLU-325          [-1, 256, 14, 14]               0\n",
      "          Conv2d-326         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-327         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-328         [-1, 1024, 14, 14]               0\n",
      "           block-329         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-330          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-331          [-1, 256, 14, 14]             512\n",
      "            ReLU-332          [-1, 256, 14, 14]               0\n",
      "          Conv2d-333          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-334          [-1, 256, 14, 14]             512\n",
      "            ReLU-335          [-1, 256, 14, 14]               0\n",
      "          Conv2d-336         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-337         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-338         [-1, 1024, 14, 14]               0\n",
      "           block-339         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-340          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-341          [-1, 256, 14, 14]             512\n",
      "            ReLU-342          [-1, 256, 14, 14]               0\n",
      "          Conv2d-343          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-344          [-1, 256, 14, 14]             512\n",
      "            ReLU-345          [-1, 256, 14, 14]               0\n",
      "          Conv2d-346         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-347         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-348         [-1, 1024, 14, 14]               0\n",
      "           block-349         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-350          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-351          [-1, 256, 14, 14]             512\n",
      "            ReLU-352          [-1, 256, 14, 14]               0\n",
      "          Conv2d-353          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-354          [-1, 256, 14, 14]             512\n",
      "            ReLU-355          [-1, 256, 14, 14]               0\n",
      "          Conv2d-356         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-357         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-358         [-1, 1024, 14, 14]               0\n",
      "           block-359         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-360          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-361          [-1, 256, 14, 14]             512\n",
      "            ReLU-362          [-1, 256, 14, 14]               0\n",
      "          Conv2d-363          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-364          [-1, 256, 14, 14]             512\n",
      "            ReLU-365          [-1, 256, 14, 14]               0\n",
      "          Conv2d-366         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-367         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-368         [-1, 1024, 14, 14]               0\n",
      "           block-369         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-370          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-371          [-1, 256, 14, 14]             512\n",
      "            ReLU-372          [-1, 256, 14, 14]               0\n",
      "          Conv2d-373          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-374          [-1, 256, 14, 14]             512\n",
      "            ReLU-375          [-1, 256, 14, 14]               0\n",
      "          Conv2d-376         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-377         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-378         [-1, 1024, 14, 14]               0\n",
      "           block-379         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-380          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-381          [-1, 256, 14, 14]             512\n",
      "            ReLU-382          [-1, 256, 14, 14]               0\n",
      "          Conv2d-383          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-384          [-1, 256, 14, 14]             512\n",
      "            ReLU-385          [-1, 256, 14, 14]               0\n",
      "          Conv2d-386         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-387         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-388         [-1, 1024, 14, 14]               0\n",
      "           block-389         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-390          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-391          [-1, 256, 14, 14]             512\n",
      "            ReLU-392          [-1, 256, 14, 14]               0\n",
      "          Conv2d-393          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-394          [-1, 256, 14, 14]             512\n",
      "            ReLU-395          [-1, 256, 14, 14]               0\n",
      "          Conv2d-396         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-397         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-398         [-1, 1024, 14, 14]               0\n",
      "           block-399         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-400          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-401          [-1, 256, 14, 14]             512\n",
      "            ReLU-402          [-1, 256, 14, 14]               0\n",
      "          Conv2d-403          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-404          [-1, 256, 14, 14]             512\n",
      "            ReLU-405          [-1, 256, 14, 14]               0\n",
      "          Conv2d-406         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-407         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-408         [-1, 1024, 14, 14]               0\n",
      "           block-409         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-410          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-411          [-1, 256, 14, 14]             512\n",
      "            ReLU-412          [-1, 256, 14, 14]               0\n",
      "          Conv2d-413          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-414          [-1, 256, 14, 14]             512\n",
      "            ReLU-415          [-1, 256, 14, 14]               0\n",
      "          Conv2d-416         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-417         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-418         [-1, 1024, 14, 14]               0\n",
      "           block-419         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-420          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-421          [-1, 256, 14, 14]             512\n",
      "            ReLU-422          [-1, 256, 14, 14]               0\n",
      "          Conv2d-423          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-424          [-1, 256, 14, 14]             512\n",
      "            ReLU-425          [-1, 256, 14, 14]               0\n",
      "          Conv2d-426         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-427         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-428         [-1, 1024, 14, 14]               0\n",
      "           block-429         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-430          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-431          [-1, 256, 14, 14]             512\n",
      "            ReLU-432          [-1, 256, 14, 14]               0\n",
      "          Conv2d-433          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-434          [-1, 256, 14, 14]             512\n",
      "            ReLU-435          [-1, 256, 14, 14]               0\n",
      "          Conv2d-436         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-437         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-438         [-1, 1024, 14, 14]               0\n",
      "           block-439         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-440          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-441          [-1, 256, 14, 14]             512\n",
      "            ReLU-442          [-1, 256, 14, 14]               0\n",
      "          Conv2d-443          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-444          [-1, 256, 14, 14]             512\n",
      "            ReLU-445          [-1, 256, 14, 14]               0\n",
      "          Conv2d-446         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-447         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-448         [-1, 1024, 14, 14]               0\n",
      "           block-449         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-450          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-451          [-1, 256, 14, 14]             512\n",
      "            ReLU-452          [-1, 256, 14, 14]               0\n",
      "          Conv2d-453          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-454          [-1, 256, 14, 14]             512\n",
      "            ReLU-455          [-1, 256, 14, 14]               0\n",
      "          Conv2d-456         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-457         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-458         [-1, 1024, 14, 14]               0\n",
      "           block-459         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-460          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-461          [-1, 256, 14, 14]             512\n",
      "            ReLU-462          [-1, 256, 14, 14]               0\n",
      "          Conv2d-463          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-464          [-1, 256, 14, 14]             512\n",
      "            ReLU-465          [-1, 256, 14, 14]               0\n",
      "          Conv2d-466         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-467         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-468         [-1, 1024, 14, 14]               0\n",
      "           block-469         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-470          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-471          [-1, 256, 14, 14]             512\n",
      "            ReLU-472          [-1, 256, 14, 14]               0\n",
      "          Conv2d-473          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-474          [-1, 256, 14, 14]             512\n",
      "            ReLU-475          [-1, 256, 14, 14]               0\n",
      "          Conv2d-476         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-477         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-478         [-1, 1024, 14, 14]               0\n",
      "           block-479         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-480          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-481          [-1, 256, 14, 14]             512\n",
      "            ReLU-482          [-1, 256, 14, 14]               0\n",
      "          Conv2d-483          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-484          [-1, 256, 14, 14]             512\n",
      "            ReLU-485          [-1, 256, 14, 14]               0\n",
      "          Conv2d-486         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-487         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-488         [-1, 1024, 14, 14]               0\n",
      "           block-489         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-490          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-491          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-492          [-1, 512, 14, 14]               0\n",
      "          Conv2d-493            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-495            [-1, 512, 7, 7]               0\n",
      "          Conv2d-496           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-497           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-498           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-499           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-500           [-1, 2048, 7, 7]               0\n",
      "           block-501           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-502            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-503            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-504            [-1, 512, 7, 7]               0\n",
      "          Conv2d-505            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-506            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-507            [-1, 512, 7, 7]               0\n",
      "          Conv2d-508           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-509           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-510           [-1, 2048, 7, 7]               0\n",
      "           block-511           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-512            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-513            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-514            [-1, 512, 7, 7]               0\n",
      "          Conv2d-515            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-516            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-517            [-1, 512, 7, 7]               0\n",
      "          Conv2d-518           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-519           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-520           [-1, 2048, 7, 7]               0\n",
      "           block-521           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-522           [-1, 2048, 1, 1]               0\n",
      "          Linear-523                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 59,033,002\n",
      "Trainable params: 59,033,002\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 612.19\n",
      "Params size (MB): 225.19\n",
      "Estimated Total Size (MB): 837.95\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model=ResNet152(img_channel=3, num_classes=num_classes).to(device)\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(64,3,224,224)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(\"model shape ready\")\n",
    "print(summary(model, input_size=(3, 224, 224)))\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592a8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs//ResNet152_sp20211030010023\n",
      "grandstore_string\n",
      "grandstore/cifar10_ResNet152_sp20211030010023.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+\"/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3316dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60cbb31a1144d69b5e136d02295ad63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7724/3031762457.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mtop_accuracy_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOP_ACCURACY_TRACK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0maccwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                    )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7724/1740864069.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(max_epoch, top_accuracy_track, grandstore, minepoch, epochwindow, accwindow)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#this is the training part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 5\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=120,\n",
    "                    minepoch=30,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                 \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4013c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model of: ResNet34_sp running on: cifar10\n",
      "\n",
      "Total Run 40 epoch(s)\n",
      "Accuracy MIN: 9.74 / MAX: 10.3\n",
      "\n",
      "Top 5 performing epochs:\n",
      "#1 epoch 30\t||train_acc 10.3%\t||test 10.0%\n",
      "#2 epoch 22\t||train_acc 10.3%\t||test 10.0%\n",
      "#3 epoch 5\t||train_acc 10.24%\t||test 10.0%\n",
      "#4 epoch 1\t||train_acc 10.24%\t||test 10.0%\n",
      "#5 epoch 11\t||train_acc 10.19%\t||test 10.0%\n",
      "\n",
      "Last epoch:\n",
      "epoch 40\t||train_acc 10.03%\t||test 10.0%\n",
      "\n",
      "The model has parameters: 22158506\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model of: \"+model_name +\n",
    "      \" running on: \"+dataset_name+\"\\n\")\n",
    "\n",
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {}%\\t||test {}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {}%\\t||test {}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ddef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
