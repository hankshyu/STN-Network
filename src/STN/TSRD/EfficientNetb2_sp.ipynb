{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auther: Tzu-Han Hsu\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) 2022, Anywhere Door Lab (ADL) and Tzu-Han Hsu\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "#    list of conditions and the following disclaimer.\n",
    "\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "#    this list of conditions and the following disclaimer in the documentation\n",
    "#    and/or other materials provided with the distribution.\n",
    "\n",
    "# 3. Neither the name of the copyright holder nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived from\n",
    "#    this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientNetb2_sp with 58 classes running on: tsrd\n",
      "The input image size is:(260, 260)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "from math import ceil\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#vital params\n",
    "\n",
    "\n",
    "\n",
    "dataset_name=\"tsrd\"\n",
    " \n",
    "model_name=\"EfficientNetb2_sp\"\n",
    "version = \"b2\"\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "phi, res, drop_rate = phi_values[version]\n",
    "#hyperparameters\n",
    "batch_size=20\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "image_size=(res,res)\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n",
    "print(\"The input image size is:{}\".format(image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38958e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '10': 2, '11': 3, '12': 4, '13': 5, '14': 6, '15': 7, '16': 8, '17': 9, '18': 10, '19': 11, '2': 12, '20': 13, '21': 14, '22': 15, '23': 16, '24': 17, '25': 18, '26': 19, '27': 20, '28': 21, '29': 22, '3': 23, '30': 24, '31': 25, '32': 26, '33': 27, '34': 28, '35': 29, '36': 30, '37': 31, '38': 32, '39': 33, '4': 34, '40': 35, '41': 36, '42': 37, '43': 38, '44': 39, '45': 40, '46': 41, '47': 42, '48': 43, '49': 44, '5': 45, '50': 46, '51': 47, '52': 48, '53': 49, '54': 50, '55': 51, '56': 52, '57': 53, '6': 54, '7': 55, '8': 56, '9': 57}\n",
      "Dataset size: Train: 4000, Valid: 998, Test: 1000\n",
      "torch.Size([3, 260, 260])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "    print(head_train_set.class_to_idx)\n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82de9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 260, 260])\n",
      "torch.Size([4, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:4044: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3982: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU() # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4, # squeeze excitation\n",
    "            survival_prob=0.8, # for stochastic depth\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,\n",
    "            ),\n",
    "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            return self.stochastic_depth(self.conv(x)) + inputs\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = ceil(1280 * width_factor)\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(37210, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 61 * 61)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        \n",
    "        \n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate\n",
    "\n",
    "    def create_features(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4*ceil(int(channels*width_factor) / 4)\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride = stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size//2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.stn(x)\n",
    "        x = self.pool(self.features(x))\n",
    "        \n",
    "        return self.classifier(x.view(x.shape[0], -1))\n",
    "\n",
    "\n",
    "def test():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    version = \"b2\"\n",
    "    phi, res, drop_rate = phi_values[version]\n",
    "    num_examples = 4\n",
    "    x = torch.randn((num_examples, 3, res, res)).to(device)\n",
    "    model = EfficientNet(\n",
    "        version=version,\n",
    "        num_classes=num_classes,\n",
    "    ).to(device)\n",
    "    print(x.shape)\n",
    "    print(model(x).shape) # (num_examples, num_classes)\n",
    "\n",
    "test()\n",
    "\n",
    "#print(\"Efficient Net model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 260, 260])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 24.00 GiB total capacity; 10.90 GiB already allocated; 5.50 MiB free; 11.13 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15248/3196626563.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model shape ready\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15248/4008467020.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15248/4008467020.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_residual\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15248/4008467020.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSqueezeExcitation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 440\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 24.00 GiB total capacity; 10.90 GiB already allocated; 5.50 MiB free; 11.13 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model = EfficientNet(\n",
    "    version=version,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(64,3,res,res)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(summary(model, input_size=(3, res, res)))\n",
    "print(\"model shape ready\")\n",
    "\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee819dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs/EfficientNetb2_sp20211030145707\n",
      "grandstore_string:\n",
      "grandstore/tsrd_EfficientNetb2_sp20211030145707.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string:\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c876d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a9a23778fe47dcb609a5df2675c64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c09ffb9e5446fa8641798edc5fd4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 24.00 GiB total capacity; 6.58 GiB already allocated; 10.10 MiB free; 6.65 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1048/1253722838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mtop_accuracy_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOP_ACCURACY_TRACK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0maccwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                    )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1048/1740864069.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(max_epoch, top_accuracy_track, grandstore, minepoch, epochwindow, accwindow)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mvalid_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1048/4008467020.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1048/4008467020.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_residual\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1048/4008467020.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSqueezeExcitation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1896\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1897\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1898\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 24.00 GiB total capacity; 6.58 GiB already allocated; 10.10 MiB free; 6.65 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 5\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=120,\n",
    "                    minepoch=30,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                  \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5b45315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Run 120 epoch(s)\n",
      "Accuracy MIN: 49.24 / MAX: 86.16\n",
      "\n",
      "Top 5 performing epochs:\n",
      "#1 epoch 97\t||train_acc 86.16%\t||test 85.72%\n",
      "#2 epoch 119\t||train_acc 86.1%\t||test 85.75%\n",
      "#3 epoch 80\t||train_acc 86.01%\t||test 86.03%\n",
      "#4 epoch 117\t||train_acc 85.92%\t||test 85.68%\n",
      "#5 epoch 87\t||train_acc 85.9%\t||test 86.03%\n",
      "\n",
      "Last epoch:\n",
      "epoch 120\t||train_acc 85.43%\t||test 85.72%\n",
      "\n",
      "The model has parameters: 30681502\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {}%\\t||test {}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {}%\\t||test {}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writings done!\n",
      "Files at: grandstore/cifar10_EfficientNetb2_sp20211025201545.pkl\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "\n",
    "print(\"writings done!\")\n",
    "print(\"Files at: \"+grandstore_string)\n",
    "\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a341d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
