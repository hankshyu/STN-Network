{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientNetb5 with 101 classes running on: caltech101\n",
      "Dataset size: Train: 6277, Valid: 1200, Test: 1200\n",
      "{'Faces': 0, 'Faces_easy': 1, 'Leopards': 2, 'Motorbikes': 3, 'accordion': 4, 'airplanes': 5, 'anchor': 6, 'ant': 7, 'barrel': 8, 'bass': 9, 'beaver': 10, 'binocular': 11, 'bonsai': 12, 'brain': 13, 'brontosaurus': 14, 'buddha': 15, 'butterfly': 16, 'camera': 17, 'cannon': 18, 'car_side': 19, 'ceiling_fan': 20, 'cellphone': 21, 'chair': 22, 'chandelier': 23, 'cougar_body': 24, 'cougar_face': 25, 'crab': 26, 'crayfish': 27, 'crocodile': 28, 'crocodile_head': 29, 'cup': 30, 'dalmatian': 31, 'dollar_bill': 32, 'dolphin': 33, 'dragonfly': 34, 'electric_guitar': 35, 'elephant': 36, 'emu': 37, 'euphonium': 38, 'ewer': 39, 'ferry': 40, 'flamingo': 41, 'flamingo_head': 42, 'garfield': 43, 'gerenuk': 44, 'gramophone': 45, 'grand_piano': 46, 'hawksbill': 47, 'headphone': 48, 'hedgehog': 49, 'helicopter': 50, 'ibis': 51, 'inline_skate': 52, 'joshua_tree': 53, 'kangaroo': 54, 'ketch': 55, 'lamp': 56, 'laptop': 57, 'llama': 58, 'lobster': 59, 'lotus': 60, 'mandolin': 61, 'mayfly': 62, 'menorah': 63, 'metronome': 64, 'minaret': 65, 'nautilus': 66, 'octopus': 67, 'okapi': 68, 'pagoda': 69, 'panda': 70, 'pigeon': 71, 'pizza': 72, 'platypus': 73, 'pyramid': 74, 'revolver': 75, 'rhino': 76, 'rooster': 77, 'saxophone': 78, 'schooner': 79, 'scissors': 80, 'scorpion': 81, 'sea_horse': 82, 'snoopy': 83, 'soccer_ball': 84, 'stapler': 85, 'starfish': 86, 'stegosaurus': 87, 'stop_sign': 88, 'strawberry': 89, 'sunflower': 90, 'tick': 91, 'trilobite': 92, 'umbrella': 93, 'watch': 94, 'water_lilly': 95, 'wheelchair': 96, 'wild_cat': 97, 'windsor_chair': 98, 'wrench': 99, 'yin_yang': 100}\n",
      "torch.Size([3, 224, 224])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#vital params\n",
    "\n",
    "dataset_name=\"caltech101\"\n",
    " \n",
    "model_name=\"EfficientNetb5\"\n",
    "version = \"b5\"\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "phi, res, drop_rate = phi_values[version]\n",
    "#hyperparameters\n",
    "batch_size=3\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "image_size=(224,224)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "elif dataset_name == \"caltech101\":\n",
    "    num_classes=101\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n",
    "\n",
    "\n",
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "elif dataset_name == \"caltech101\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/caltech101\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [7477, 1200])\n",
    "    train_set, test_set = data.random_split(train_set,[6277, 1200])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "    \n",
    "    \n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "print(head_train_set.class_to_idx)\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6cfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU() # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4, # squeeze excitation\n",
    "            survival_prob=0.8, # for stochastic depth\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,\n",
    "            ),\n",
    "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            return self.stochastic_depth(self.conv(x)) + inputs\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = math.ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "        )\n",
    "\n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate\n",
    "\n",
    "    def create_features(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4*math.ceil(int(channels*width_factor) / 4)\n",
    "            layers_repeats = math.ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride = stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size//2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.features(x))\n",
    "        return self.classifier(x.view(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 456, 456])\n",
      "torch.Size([3, 101])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 46, 228, 228]           1,242\n",
      "       BatchNorm2d-2         [-1, 46, 228, 228]              92\n",
      "              SiLU-3         [-1, 46, 228, 228]               0\n",
      "          CNNBlock-4         [-1, 46, 228, 228]               0\n",
      "            Conv2d-5         [-1, 46, 228, 228]             414\n",
      "       BatchNorm2d-6         [-1, 46, 228, 228]              92\n",
      "              SiLU-7         [-1, 46, 228, 228]               0\n",
      "          CNNBlock-8         [-1, 46, 228, 228]               0\n",
      " AdaptiveAvgPool2d-9             [-1, 46, 1, 1]               0\n",
      "           Conv2d-10             [-1, 11, 1, 1]             517\n",
      "             SiLU-11             [-1, 11, 1, 1]               0\n",
      "           Conv2d-12             [-1, 46, 1, 1]             552\n",
      "          Sigmoid-13             [-1, 46, 1, 1]               0\n",
      "SqueezeExcitation-14         [-1, 46, 228, 228]               0\n",
      "           Conv2d-15         [-1, 24, 228, 228]           1,104\n",
      "      BatchNorm2d-16         [-1, 24, 228, 228]              48\n",
      "InvertedResidualBlock-17         [-1, 24, 228, 228]               0\n",
      "           Conv2d-18         [-1, 24, 228, 228]             216\n",
      "      BatchNorm2d-19         [-1, 24, 228, 228]              48\n",
      "             SiLU-20         [-1, 24, 228, 228]               0\n",
      "         CNNBlock-21         [-1, 24, 228, 228]               0\n",
      "AdaptiveAvgPool2d-22             [-1, 24, 1, 1]               0\n",
      "           Conv2d-23              [-1, 6, 1, 1]             150\n",
      "             SiLU-24              [-1, 6, 1, 1]               0\n",
      "           Conv2d-25             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-26             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-27         [-1, 24, 228, 228]               0\n",
      "           Conv2d-28         [-1, 24, 228, 228]             576\n",
      "      BatchNorm2d-29         [-1, 24, 228, 228]              48\n",
      "InvertedResidualBlock-30         [-1, 24, 228, 228]               0\n",
      "           Conv2d-31         [-1, 24, 228, 228]             216\n",
      "      BatchNorm2d-32         [-1, 24, 228, 228]              48\n",
      "             SiLU-33         [-1, 24, 228, 228]               0\n",
      "         CNNBlock-34         [-1, 24, 228, 228]               0\n",
      "AdaptiveAvgPool2d-35             [-1, 24, 1, 1]               0\n",
      "           Conv2d-36              [-1, 6, 1, 1]             150\n",
      "             SiLU-37              [-1, 6, 1, 1]               0\n",
      "           Conv2d-38             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-39             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-40         [-1, 24, 228, 228]               0\n",
      "           Conv2d-41         [-1, 24, 228, 228]             576\n",
      "      BatchNorm2d-42         [-1, 24, 228, 228]              48\n",
      "InvertedResidualBlock-43         [-1, 24, 228, 228]               0\n",
      "           Conv2d-44        [-1, 144, 228, 228]          31,104\n",
      "      BatchNorm2d-45        [-1, 144, 228, 228]             288\n",
      "             SiLU-46        [-1, 144, 228, 228]               0\n",
      "         CNNBlock-47        [-1, 144, 228, 228]               0\n",
      "           Conv2d-48        [-1, 144, 114, 114]           1,296\n",
      "      BatchNorm2d-49        [-1, 144, 114, 114]             288\n",
      "             SiLU-50        [-1, 144, 114, 114]               0\n",
      "         CNNBlock-51        [-1, 144, 114, 114]               0\n",
      "AdaptiveAvgPool2d-52            [-1, 144, 1, 1]               0\n",
      "           Conv2d-53              [-1, 6, 1, 1]             870\n",
      "             SiLU-54              [-1, 6, 1, 1]               0\n",
      "           Conv2d-55            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-56            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-57        [-1, 144, 114, 114]               0\n",
      "           Conv2d-58         [-1, 36, 114, 114]           5,184\n",
      "      BatchNorm2d-59         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-60         [-1, 36, 114, 114]               0\n",
      "           Conv2d-61        [-1, 216, 114, 114]          69,984\n",
      "      BatchNorm2d-62        [-1, 216, 114, 114]             432\n",
      "             SiLU-63        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-64        [-1, 216, 114, 114]               0\n",
      "           Conv2d-65        [-1, 216, 114, 114]           1,944\n",
      "      BatchNorm2d-66        [-1, 216, 114, 114]             432\n",
      "             SiLU-67        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-68        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-69            [-1, 216, 1, 1]               0\n",
      "           Conv2d-70              [-1, 9, 1, 1]           1,953\n",
      "             SiLU-71              [-1, 9, 1, 1]               0\n",
      "           Conv2d-72            [-1, 216, 1, 1]           2,160\n",
      "          Sigmoid-73            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-74        [-1, 216, 114, 114]               0\n",
      "           Conv2d-75         [-1, 36, 114, 114]           7,776\n",
      "      BatchNorm2d-76         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-77         [-1, 36, 114, 114]               0\n",
      "           Conv2d-78        [-1, 216, 114, 114]          69,984\n",
      "      BatchNorm2d-79        [-1, 216, 114, 114]             432\n",
      "             SiLU-80        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-81        [-1, 216, 114, 114]               0\n",
      "           Conv2d-82        [-1, 216, 114, 114]           1,944\n",
      "      BatchNorm2d-83        [-1, 216, 114, 114]             432\n",
      "             SiLU-84        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-85        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-86            [-1, 216, 1, 1]               0\n",
      "           Conv2d-87              [-1, 9, 1, 1]           1,953\n",
      "             SiLU-88              [-1, 9, 1, 1]               0\n",
      "           Conv2d-89            [-1, 216, 1, 1]           2,160\n",
      "          Sigmoid-90            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-91        [-1, 216, 114, 114]               0\n",
      "           Conv2d-92         [-1, 36, 114, 114]           7,776\n",
      "      BatchNorm2d-93         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-94         [-1, 36, 114, 114]               0\n",
      "           Conv2d-95        [-1, 216, 114, 114]          69,984\n",
      "      BatchNorm2d-96        [-1, 216, 114, 114]             432\n",
      "             SiLU-97        [-1, 216, 114, 114]               0\n",
      "         CNNBlock-98        [-1, 216, 114, 114]               0\n",
      "           Conv2d-99        [-1, 216, 114, 114]           1,944\n",
      "     BatchNorm2d-100        [-1, 216, 114, 114]             432\n",
      "            SiLU-101        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-102        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-103            [-1, 216, 1, 1]               0\n",
      "          Conv2d-104              [-1, 9, 1, 1]           1,953\n",
      "            SiLU-105              [-1, 9, 1, 1]               0\n",
      "          Conv2d-106            [-1, 216, 1, 1]           2,160\n",
      "         Sigmoid-107            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-108        [-1, 216, 114, 114]               0\n",
      "          Conv2d-109         [-1, 36, 114, 114]           7,776\n",
      "     BatchNorm2d-110         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-111         [-1, 36, 114, 114]               0\n",
      "          Conv2d-112        [-1, 216, 114, 114]          69,984\n",
      "     BatchNorm2d-113        [-1, 216, 114, 114]             432\n",
      "            SiLU-114        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-115        [-1, 216, 114, 114]               0\n",
      "          Conv2d-116        [-1, 216, 114, 114]           1,944\n",
      "     BatchNorm2d-117        [-1, 216, 114, 114]             432\n",
      "            SiLU-118        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-119        [-1, 216, 114, 114]               0\n",
      "AdaptiveAvgPool2d-120            [-1, 216, 1, 1]               0\n",
      "          Conv2d-121              [-1, 9, 1, 1]           1,953\n",
      "            SiLU-122              [-1, 9, 1, 1]               0\n",
      "          Conv2d-123            [-1, 216, 1, 1]           2,160\n",
      "         Sigmoid-124            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-125        [-1, 216, 114, 114]               0\n",
      "          Conv2d-126         [-1, 36, 114, 114]           7,776\n",
      "     BatchNorm2d-127         [-1, 36, 114, 114]              72\n",
      "InvertedResidualBlock-128         [-1, 36, 114, 114]               0\n",
      "          Conv2d-129        [-1, 216, 114, 114]          69,984\n",
      "     BatchNorm2d-130        [-1, 216, 114, 114]             432\n",
      "            SiLU-131        [-1, 216, 114, 114]               0\n",
      "        CNNBlock-132        [-1, 216, 114, 114]               0\n",
      "          Conv2d-133          [-1, 216, 57, 57]           5,400\n",
      "     BatchNorm2d-134          [-1, 216, 57, 57]             432\n",
      "            SiLU-135          [-1, 216, 57, 57]               0\n",
      "        CNNBlock-136          [-1, 216, 57, 57]               0\n",
      "AdaptiveAvgPool2d-137            [-1, 216, 1, 1]               0\n",
      "          Conv2d-138              [-1, 9, 1, 1]           1,953\n",
      "            SiLU-139              [-1, 9, 1, 1]               0\n",
      "          Conv2d-140            [-1, 216, 1, 1]           2,160\n",
      "         Sigmoid-141            [-1, 216, 1, 1]               0\n",
      "SqueezeExcitation-142          [-1, 216, 57, 57]               0\n",
      "          Conv2d-143           [-1, 60, 57, 57]          12,960\n",
      "     BatchNorm2d-144           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-145           [-1, 60, 57, 57]               0\n",
      "          Conv2d-146          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-147          [-1, 360, 57, 57]             720\n",
      "            SiLU-148          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-149          [-1, 360, 57, 57]               0\n",
      "          Conv2d-150          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-151          [-1, 360, 57, 57]             720\n",
      "            SiLU-152          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-153          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-154            [-1, 360, 1, 1]               0\n",
      "          Conv2d-155             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-156             [-1, 15, 1, 1]               0\n",
      "          Conv2d-157            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-158            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-159          [-1, 360, 57, 57]               0\n",
      "          Conv2d-160           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-161           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-162           [-1, 60, 57, 57]               0\n",
      "          Conv2d-163          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-164          [-1, 360, 57, 57]             720\n",
      "            SiLU-165          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-166          [-1, 360, 57, 57]               0\n",
      "          Conv2d-167          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-168          [-1, 360, 57, 57]             720\n",
      "            SiLU-169          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-170          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-171            [-1, 360, 1, 1]               0\n",
      "          Conv2d-172             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-173             [-1, 15, 1, 1]               0\n",
      "          Conv2d-174            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-175            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-176          [-1, 360, 57, 57]               0\n",
      "          Conv2d-177           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-178           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-179           [-1, 60, 57, 57]               0\n",
      "          Conv2d-180          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-181          [-1, 360, 57, 57]             720\n",
      "            SiLU-182          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-183          [-1, 360, 57, 57]               0\n",
      "          Conv2d-184          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-185          [-1, 360, 57, 57]             720\n",
      "            SiLU-186          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-187          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-188            [-1, 360, 1, 1]               0\n",
      "          Conv2d-189             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-190             [-1, 15, 1, 1]               0\n",
      "          Conv2d-191            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-192            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-193          [-1, 360, 57, 57]               0\n",
      "          Conv2d-194           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-195           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-196           [-1, 60, 57, 57]               0\n",
      "          Conv2d-197          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-198          [-1, 360, 57, 57]             720\n",
      "            SiLU-199          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-200          [-1, 360, 57, 57]               0\n",
      "          Conv2d-201          [-1, 360, 57, 57]           9,000\n",
      "     BatchNorm2d-202          [-1, 360, 57, 57]             720\n",
      "            SiLU-203          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-204          [-1, 360, 57, 57]               0\n",
      "AdaptiveAvgPool2d-205            [-1, 360, 1, 1]               0\n",
      "          Conv2d-206             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-207             [-1, 15, 1, 1]               0\n",
      "          Conv2d-208            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-209            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-210          [-1, 360, 57, 57]               0\n",
      "          Conv2d-211           [-1, 60, 57, 57]          21,600\n",
      "     BatchNorm2d-212           [-1, 60, 57, 57]             120\n",
      "InvertedResidualBlock-213           [-1, 60, 57, 57]               0\n",
      "          Conv2d-214          [-1, 360, 57, 57]         194,400\n",
      "     BatchNorm2d-215          [-1, 360, 57, 57]             720\n",
      "            SiLU-216          [-1, 360, 57, 57]               0\n",
      "        CNNBlock-217          [-1, 360, 57, 57]               0\n",
      "          Conv2d-218          [-1, 360, 29, 29]           3,240\n",
      "     BatchNorm2d-219          [-1, 360, 29, 29]             720\n",
      "            SiLU-220          [-1, 360, 29, 29]               0\n",
      "        CNNBlock-221          [-1, 360, 29, 29]               0\n",
      "AdaptiveAvgPool2d-222            [-1, 360, 1, 1]               0\n",
      "          Conv2d-223             [-1, 15, 1, 1]           5,415\n",
      "            SiLU-224             [-1, 15, 1, 1]               0\n",
      "          Conv2d-225            [-1, 360, 1, 1]           5,760\n",
      "         Sigmoid-226            [-1, 360, 1, 1]               0\n",
      "SqueezeExcitation-227          [-1, 360, 29, 29]               0\n",
      "          Conv2d-228          [-1, 120, 29, 29]          43,200\n",
      "     BatchNorm2d-229          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-230          [-1, 120, 29, 29]               0\n",
      "          Conv2d-231          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-232          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-233          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-234          [-1, 720, 29, 29]               0\n",
      "          Conv2d-235          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-236          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-237          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-238          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-239            [-1, 720, 1, 1]               0\n",
      "          Conv2d-240             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-241             [-1, 30, 1, 1]               0\n",
      "          Conv2d-242            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-243            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-244          [-1, 720, 29, 29]               0\n",
      "          Conv2d-245          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-246          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-247          [-1, 120, 29, 29]               0\n",
      "          Conv2d-248          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-249          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-250          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-251          [-1, 720, 29, 29]               0\n",
      "          Conv2d-252          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-253          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-254          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-255          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-256            [-1, 720, 1, 1]               0\n",
      "          Conv2d-257             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-258             [-1, 30, 1, 1]               0\n",
      "          Conv2d-259            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-260            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-261          [-1, 720, 29, 29]               0\n",
      "          Conv2d-262          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-263          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-264          [-1, 120, 29, 29]               0\n",
      "          Conv2d-265          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-266          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-267          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-268          [-1, 720, 29, 29]               0\n",
      "          Conv2d-269          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-270          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-271          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-272          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-273            [-1, 720, 1, 1]               0\n",
      "          Conv2d-274             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-275             [-1, 30, 1, 1]               0\n",
      "          Conv2d-276            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-277            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-278          [-1, 720, 29, 29]               0\n",
      "          Conv2d-279          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-280          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-281          [-1, 120, 29, 29]               0\n",
      "          Conv2d-282          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-283          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-284          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-285          [-1, 720, 29, 29]               0\n",
      "          Conv2d-286          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-287          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-288          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-289          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-290            [-1, 720, 1, 1]               0\n",
      "          Conv2d-291             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-292             [-1, 30, 1, 1]               0\n",
      "          Conv2d-293            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-294            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-295          [-1, 720, 29, 29]               0\n",
      "          Conv2d-296          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-297          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-298          [-1, 120, 29, 29]               0\n",
      "          Conv2d-299          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-300          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-301          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-302          [-1, 720, 29, 29]               0\n",
      "          Conv2d-303          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-304          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-305          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-306          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-307            [-1, 720, 1, 1]               0\n",
      "          Conv2d-308             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-309             [-1, 30, 1, 1]               0\n",
      "          Conv2d-310            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-311            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-312          [-1, 720, 29, 29]               0\n",
      "          Conv2d-313          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-314          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-315          [-1, 120, 29, 29]               0\n",
      "          Conv2d-316          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-317          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-318          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-319          [-1, 720, 29, 29]               0\n",
      "          Conv2d-320          [-1, 720, 29, 29]           6,480\n",
      "     BatchNorm2d-321          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-322          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-323          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-324            [-1, 720, 1, 1]               0\n",
      "          Conv2d-325             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-326             [-1, 30, 1, 1]               0\n",
      "          Conv2d-327            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-328            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-329          [-1, 720, 29, 29]               0\n",
      "          Conv2d-330          [-1, 120, 29, 29]          86,400\n",
      "     BatchNorm2d-331          [-1, 120, 29, 29]             240\n",
      "InvertedResidualBlock-332          [-1, 120, 29, 29]               0\n",
      "          Conv2d-333          [-1, 720, 29, 29]         777,600\n",
      "     BatchNorm2d-334          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-335          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-336          [-1, 720, 29, 29]               0\n",
      "          Conv2d-337          [-1, 720, 29, 29]          18,000\n",
      "     BatchNorm2d-338          [-1, 720, 29, 29]           1,440\n",
      "            SiLU-339          [-1, 720, 29, 29]               0\n",
      "        CNNBlock-340          [-1, 720, 29, 29]               0\n",
      "AdaptiveAvgPool2d-341            [-1, 720, 1, 1]               0\n",
      "          Conv2d-342             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-343             [-1, 30, 1, 1]               0\n",
      "          Conv2d-344            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-345            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-346          [-1, 720, 29, 29]               0\n",
      "          Conv2d-347          [-1, 164, 29, 29]         118,080\n",
      "     BatchNorm2d-348          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-349          [-1, 164, 29, 29]               0\n",
      "          Conv2d-350          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-351          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-352          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-353          [-1, 984, 29, 29]               0\n",
      "          Conv2d-354          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-355          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-356          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-357          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-358            [-1, 984, 1, 1]               0\n",
      "          Conv2d-359             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-360             [-1, 41, 1, 1]               0\n",
      "          Conv2d-361            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-362            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-363          [-1, 984, 29, 29]               0\n",
      "          Conv2d-364          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-365          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-366          [-1, 164, 29, 29]               0\n",
      "          Conv2d-367          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-368          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-369          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-370          [-1, 984, 29, 29]               0\n",
      "          Conv2d-371          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-372          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-373          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-374          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-375            [-1, 984, 1, 1]               0\n",
      "          Conv2d-376             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-377             [-1, 41, 1, 1]               0\n",
      "          Conv2d-378            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-379            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-380          [-1, 984, 29, 29]               0\n",
      "          Conv2d-381          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-382          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-383          [-1, 164, 29, 29]               0\n",
      "          Conv2d-384          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-385          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-386          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-387          [-1, 984, 29, 29]               0\n",
      "          Conv2d-388          [-1, 984, 29, 29]          24,600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-389          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-390          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-391          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-392            [-1, 984, 1, 1]               0\n",
      "          Conv2d-393             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-394             [-1, 41, 1, 1]               0\n",
      "          Conv2d-395            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-396            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-397          [-1, 984, 29, 29]               0\n",
      "          Conv2d-398          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-399          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-400          [-1, 164, 29, 29]               0\n",
      "          Conv2d-401          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-402          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-403          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-404          [-1, 984, 29, 29]               0\n",
      "          Conv2d-405          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-406          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-407          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-408          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-409            [-1, 984, 1, 1]               0\n",
      "          Conv2d-410             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-411             [-1, 41, 1, 1]               0\n",
      "          Conv2d-412            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-413            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-414          [-1, 984, 29, 29]               0\n",
      "          Conv2d-415          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-416          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-417          [-1, 164, 29, 29]               0\n",
      "          Conv2d-418          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-419          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-420          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-421          [-1, 984, 29, 29]               0\n",
      "          Conv2d-422          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-423          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-424          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-425          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-426            [-1, 984, 1, 1]               0\n",
      "          Conv2d-427             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-428             [-1, 41, 1, 1]               0\n",
      "          Conv2d-429            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-430            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-431          [-1, 984, 29, 29]               0\n",
      "          Conv2d-432          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-433          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-434          [-1, 164, 29, 29]               0\n",
      "          Conv2d-435          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-436          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-437          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-438          [-1, 984, 29, 29]               0\n",
      "          Conv2d-439          [-1, 984, 29, 29]          24,600\n",
      "     BatchNorm2d-440          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-441          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-442          [-1, 984, 29, 29]               0\n",
      "AdaptiveAvgPool2d-443            [-1, 984, 1, 1]               0\n",
      "          Conv2d-444             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-445             [-1, 41, 1, 1]               0\n",
      "          Conv2d-446            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-447            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-448          [-1, 984, 29, 29]               0\n",
      "          Conv2d-449          [-1, 164, 29, 29]         161,376\n",
      "     BatchNorm2d-450          [-1, 164, 29, 29]             328\n",
      "InvertedResidualBlock-451          [-1, 164, 29, 29]               0\n",
      "          Conv2d-452          [-1, 984, 29, 29]       1,452,384\n",
      "     BatchNorm2d-453          [-1, 984, 29, 29]           1,968\n",
      "            SiLU-454          [-1, 984, 29, 29]               0\n",
      "        CNNBlock-455          [-1, 984, 29, 29]               0\n",
      "          Conv2d-456          [-1, 984, 15, 15]          24,600\n",
      "     BatchNorm2d-457          [-1, 984, 15, 15]           1,968\n",
      "            SiLU-458          [-1, 984, 15, 15]               0\n",
      "        CNNBlock-459          [-1, 984, 15, 15]               0\n",
      "AdaptiveAvgPool2d-460            [-1, 984, 1, 1]               0\n",
      "          Conv2d-461             [-1, 41, 1, 1]          40,385\n",
      "            SiLU-462             [-1, 41, 1, 1]               0\n",
      "          Conv2d-463            [-1, 984, 1, 1]          41,328\n",
      "         Sigmoid-464            [-1, 984, 1, 1]               0\n",
      "SqueezeExcitation-465          [-1, 984, 15, 15]               0\n",
      "          Conv2d-466          [-1, 284, 15, 15]         279,456\n",
      "     BatchNorm2d-467          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-468          [-1, 284, 15, 15]               0\n",
      "          Conv2d-469         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-470         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-471         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-472         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-473         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-474         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-475         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-476         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-477           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-478             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-479             [-1, 71, 1, 1]               0\n",
      "          Conv2d-480           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-481           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-482         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-483          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-484          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-485          [-1, 284, 15, 15]               0\n",
      "          Conv2d-486         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-487         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-488         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-489         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-490         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-491         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-492         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-493         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-494           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-495             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-496             [-1, 71, 1, 1]               0\n",
      "          Conv2d-497           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-498           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-499         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-500          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-501          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-502          [-1, 284, 15, 15]               0\n",
      "          Conv2d-503         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-504         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-505         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-506         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-507         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-508         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-509         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-510         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-511           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-512             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-513             [-1, 71, 1, 1]               0\n",
      "          Conv2d-514           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-515           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-516         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-517          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-518          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-519          [-1, 284, 15, 15]               0\n",
      "          Conv2d-520         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-521         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-522         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-523         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-524         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-525         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-526         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-527         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-528           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-529             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-530             [-1, 71, 1, 1]               0\n",
      "          Conv2d-531           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-532           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-533         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-534          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-535          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-536          [-1, 284, 15, 15]               0\n",
      "          Conv2d-537         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-538         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-539         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-540         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-541         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-542         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-543         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-544         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-545           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-546             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-547             [-1, 71, 1, 1]               0\n",
      "          Conv2d-548           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-549           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-550         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-551          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-552          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-553          [-1, 284, 15, 15]               0\n",
      "          Conv2d-554         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-555         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-556         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-557         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-558         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-559         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-560         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-561         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-562           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-563             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-564             [-1, 71, 1, 1]               0\n",
      "          Conv2d-565           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-566           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-567         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-568          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-569          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-570          [-1, 284, 15, 15]               0\n",
      "          Conv2d-571         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-572         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-573         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-574         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-575         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-576         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-577         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-578         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-579           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-580             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-581             [-1, 71, 1, 1]               0\n",
      "          Conv2d-582           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-583           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-584         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-585          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-586          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-587          [-1, 284, 15, 15]               0\n",
      "          Conv2d-588         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-589         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-590         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-591         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-592         [-1, 1704, 15, 15]          42,600\n",
      "     BatchNorm2d-593         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-594         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-595         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-596           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-597             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-598             [-1, 71, 1, 1]               0\n",
      "          Conv2d-599           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-600           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-601         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-602          [-1, 284, 15, 15]         483,936\n",
      "     BatchNorm2d-603          [-1, 284, 15, 15]             568\n",
      "InvertedResidualBlock-604          [-1, 284, 15, 15]               0\n",
      "          Conv2d-605         [-1, 1704, 15, 15]       4,355,424\n",
      "     BatchNorm2d-606         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-607         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-608         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-609         [-1, 1704, 15, 15]          15,336\n",
      "     BatchNorm2d-610         [-1, 1704, 15, 15]           3,408\n",
      "            SiLU-611         [-1, 1704, 15, 15]               0\n",
      "        CNNBlock-612         [-1, 1704, 15, 15]               0\n",
      "AdaptiveAvgPool2d-613           [-1, 1704, 1, 1]               0\n",
      "          Conv2d-614             [-1, 71, 1, 1]         121,055\n",
      "            SiLU-615             [-1, 71, 1, 1]               0\n",
      "          Conv2d-616           [-1, 1704, 1, 1]         122,688\n",
      "         Sigmoid-617           [-1, 1704, 1, 1]               0\n",
      "SqueezeExcitation-618         [-1, 1704, 15, 15]               0\n",
      "          Conv2d-619          [-1, 468, 15, 15]         797,472\n",
      "     BatchNorm2d-620          [-1, 468, 15, 15]             936\n",
      "InvertedResidualBlock-621          [-1, 468, 15, 15]               0\n",
      "          Conv2d-622         [-1, 2808, 15, 15]      11,827,296\n",
      "     BatchNorm2d-623         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-624         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-625         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-626         [-1, 2808, 15, 15]          25,272\n",
      "     BatchNorm2d-627         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-628         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-629         [-1, 2808, 15, 15]               0\n",
      "AdaptiveAvgPool2d-630           [-1, 2808, 1, 1]               0\n",
      "          Conv2d-631            [-1, 117, 1, 1]         328,653\n",
      "            SiLU-632            [-1, 117, 1, 1]               0\n",
      "          Conv2d-633           [-1, 2808, 1, 1]         331,344\n",
      "         Sigmoid-634           [-1, 2808, 1, 1]               0\n",
      "SqueezeExcitation-635         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-636          [-1, 468, 15, 15]       1,314,144\n",
      "     BatchNorm2d-637          [-1, 468, 15, 15]             936\n",
      "InvertedResidualBlock-638          [-1, 468, 15, 15]               0\n",
      "          Conv2d-639         [-1, 2808, 15, 15]      11,827,296\n",
      "     BatchNorm2d-640         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-641         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-642         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-643         [-1, 2808, 15, 15]          25,272\n",
      "     BatchNorm2d-644         [-1, 2808, 15, 15]           5,616\n",
      "            SiLU-645         [-1, 2808, 15, 15]               0\n",
      "        CNNBlock-646         [-1, 2808, 15, 15]               0\n",
      "AdaptiveAvgPool2d-647           [-1, 2808, 1, 1]               0\n",
      "          Conv2d-648            [-1, 117, 1, 1]         328,653\n",
      "            SiLU-649            [-1, 117, 1, 1]               0\n",
      "          Conv2d-650           [-1, 2808, 1, 1]         331,344\n",
      "         Sigmoid-651           [-1, 2808, 1, 1]               0\n",
      "SqueezeExcitation-652         [-1, 2808, 15, 15]               0\n",
      "          Conv2d-653          [-1, 468, 15, 15]       1,314,144\n",
      "     BatchNorm2d-654          [-1, 468, 15, 15]             936\n",
      "InvertedResidualBlock-655          [-1, 468, 15, 15]               0\n",
      "          Conv2d-656         [-1, 1875, 15, 15]         877,500\n",
      "     BatchNorm2d-657         [-1, 1875, 15, 15]           3,750\n",
      "            SiLU-658         [-1, 1875, 15, 15]               0\n",
      "        CNNBlock-659         [-1, 1875, 15, 15]               0\n",
      "AdaptiveAvgPool2d-660           [-1, 1875, 1, 1]               0\n",
      "         Dropout-661                 [-1, 1875]               0\n",
      "          Linear-662                  [-1, 101]         189,476\n",
      "================================================================\n",
      "Total params: 95,571,391\n",
      "Trainable params: 95,571,391\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.38\n",
      "Forward/backward pass size (MB): 3034.46\n",
      "Params size (MB): 364.58\n",
      "Estimated Total Size (MB): 3401.42\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model shape ready\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet(\n",
    "    version=version,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(batch_size,3,res,res)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(summary(model, input_size=(3, res, res)))\n",
    "print(\"model shape ready\")\n",
    "\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee819dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs/EfficientNetb520211231002535\n",
      "grandstore_string:\n",
      "grandstore/caltech101_EfficientNetb520211231002535.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string:\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c876d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7ab1d1818f498c90041904cb8c1597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2093 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3260/3948193672.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mtop_accuracy_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOP_ACCURACY_TRACK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0maccwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                    )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3260/1740864069.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(max_epoch, top_accuracy_track, grandstore, minepoch, epochwindow, accwindow)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mtotaltrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 5\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=240,\n",
    "                    minepoch=120,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                  \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b45315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Run 200 epoch(s)\n",
      "Accuracy MIN: 9.666666666666666 / MAX: 70.0\n",
      "\n",
      "Top 5 performing epochs:\n",
      "#1 epoch 190\t||train_acc 70.00%\t||test 68.75%\n",
      "#2 epoch 183\t||train_acc 69.83%\t||test 68.75%\n",
      "#3 epoch 186\t||train_acc 69.33%\t||test 67.17%\n",
      "#4 epoch 164\t||train_acc 69.08%\t||test 68.67%\n",
      "#5 epoch 147\t||train_acc 69.00%\t||test 68.75%\n",
      "\n",
      "Last epoch:\n",
      "epoch 200\t||train_acc 68.50%\t||test 67.42%\n",
      "\n",
      "The model has parameters: 95571391\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {:.2f}%\\t||test {:.2f}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {:.2f}%\\t||test {:.2f}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writings done!\n",
      "Files at: grandstore/caltech101_EfficientNetb520211230103316.pkl\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "\n",
    "print(\"writings done!\")\n",
    "print(\"Files at: \"+grandstore_string)\n",
    "\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a341d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
