{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ResNet101_sp with 101 classes running on: caltech101\n",
      "Dataset size: Train: 6277, Valid: 1200, Test: 1200\n",
      "{'Faces': 0, 'Faces_easy': 1, 'Leopards': 2, 'Motorbikes': 3, 'accordion': 4, 'airplanes': 5, 'anchor': 6, 'ant': 7, 'barrel': 8, 'bass': 9, 'beaver': 10, 'binocular': 11, 'bonsai': 12, 'brain': 13, 'brontosaurus': 14, 'buddha': 15, 'butterfly': 16, 'camera': 17, 'cannon': 18, 'car_side': 19, 'ceiling_fan': 20, 'cellphone': 21, 'chair': 22, 'chandelier': 23, 'cougar_body': 24, 'cougar_face': 25, 'crab': 26, 'crayfish': 27, 'crocodile': 28, 'crocodile_head': 29, 'cup': 30, 'dalmatian': 31, 'dollar_bill': 32, 'dolphin': 33, 'dragonfly': 34, 'electric_guitar': 35, 'elephant': 36, 'emu': 37, 'euphonium': 38, 'ewer': 39, 'ferry': 40, 'flamingo': 41, 'flamingo_head': 42, 'garfield': 43, 'gerenuk': 44, 'gramophone': 45, 'grand_piano': 46, 'hawksbill': 47, 'headphone': 48, 'hedgehog': 49, 'helicopter': 50, 'ibis': 51, 'inline_skate': 52, 'joshua_tree': 53, 'kangaroo': 54, 'ketch': 55, 'lamp': 56, 'laptop': 57, 'llama': 58, 'lobster': 59, 'lotus': 60, 'mandolin': 61, 'mayfly': 62, 'menorah': 63, 'metronome': 64, 'minaret': 65, 'nautilus': 66, 'octopus': 67, 'okapi': 68, 'pagoda': 69, 'panda': 70, 'pigeon': 71, 'pizza': 72, 'platypus': 73, 'pyramid': 74, 'revolver': 75, 'rhino': 76, 'rooster': 77, 'saxophone': 78, 'schooner': 79, 'scissors': 80, 'scorpion': 81, 'sea_horse': 82, 'snoopy': 83, 'soccer_ball': 84, 'stapler': 85, 'starfish': 86, 'stegosaurus': 87, 'stop_sign': 88, 'strawberry': 89, 'sunflower': 90, 'tick': 91, 'trilobite': 92, 'umbrella': 93, 'watch': 94, 'water_lilly': 95, 'wheelchair': 96, 'wild_cat': 97, 'windsor_chair': 98, 'wrench': 99, 'yin_yang': 100}\n",
      "torch.Size([3, 224, 224])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "\n",
    "#vital params\n",
    "\n",
    " \n",
    "model_name=\"ResNet101_sp\"\n",
    "\n",
    "dataset_name=\"caltech101\"\n",
    "\n",
    "#hyperparameters\n",
    "batch_size=10\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "image_size=(224,224)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "elif dataset_name == \"caltech101\":\n",
    "    num_classes=101\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n",
    "\n",
    "\n",
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "elif dataset_name == \"caltech101\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/caltech101\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [7477, 1200])\n",
    "    train_set, test_set = data.random_split(train_set,[6277, 1200])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "    \n",
    "    \n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "print(head_train_set.class_to_idx)\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfdd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    " \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    " \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    " \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    " \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    " \n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    " \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(27040, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        \n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 52 * 52)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "        \n",
    " \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    " \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    " \n",
    "        return nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x=self.stn(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    " \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    " \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    " \n",
    "        return x\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    " \n",
    " \n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    " \n",
    " \n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    " \n",
    " \n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    " \n",
    " \n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n",
      "torch.Size([10, 101])\n",
      "model shape ready\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 218, 218]           1,184\n",
      "         MaxPool2d-2          [-1, 8, 109, 109]               0\n",
      "              ReLU-3          [-1, 8, 109, 109]               0\n",
      "            Conv2d-4         [-1, 10, 105, 105]           2,010\n",
      "         MaxPool2d-5           [-1, 10, 52, 52]               0\n",
      "              ReLU-6           [-1, 10, 52, 52]               0\n",
      "            Linear-7                   [-1, 32]         865,312\n",
      "              ReLU-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 6]             198\n",
      "           Conv2d-10         [-1, 64, 112, 112]           9,408\n",
      "      BatchNorm2d-11         [-1, 64, 112, 112]             128\n",
      "             ReLU-12         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]           4,096\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "             ReLU-16           [-1, 64, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-21          [-1, 256, 56, 56]             512\n",
      "           Conv2d-22          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
      "             ReLU-24          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-25          [-1, 256, 56, 56]               0\n",
      "           Conv2d-26           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-27           [-1, 64, 56, 56]             128\n",
      "             ReLU-28           [-1, 64, 56, 56]               0\n",
      "           Conv2d-29           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "           Conv2d-32          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-33          [-1, 256, 56, 56]             512\n",
      "             ReLU-34          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-35          [-1, 256, 56, 56]               0\n",
      "           Conv2d-36           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-37           [-1, 64, 56, 56]             128\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "           Conv2d-39           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-40           [-1, 64, 56, 56]             128\n",
      "             ReLU-41           [-1, 64, 56, 56]               0\n",
      "           Conv2d-42          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-43          [-1, 256, 56, 56]             512\n",
      "             ReLU-44          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-45          [-1, 256, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-47          [-1, 128, 56, 56]             256\n",
      "             ReLU-48          [-1, 128, 56, 56]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-53          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-54          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-55          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-56          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-57          [-1, 512, 28, 28]               0\n",
      "           Conv2d-58          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-59          [-1, 128, 28, 28]             256\n",
      "             ReLU-60          [-1, 128, 28, 28]               0\n",
      "           Conv2d-61          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-62          [-1, 128, 28, 28]             256\n",
      "             ReLU-63          [-1, 128, 28, 28]               0\n",
      "           Conv2d-64          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-65          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-66          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-67          [-1, 512, 28, 28]               0\n",
      "           Conv2d-68          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-69          [-1, 128, 28, 28]             256\n",
      "             ReLU-70          [-1, 128, 28, 28]               0\n",
      "           Conv2d-71          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-72          [-1, 128, 28, 28]             256\n",
      "             ReLU-73          [-1, 128, 28, 28]               0\n",
      "           Conv2d-74          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-75          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-76          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-77          [-1, 512, 28, 28]               0\n",
      "           Conv2d-78          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
      "             ReLU-80          [-1, 128, 28, 28]               0\n",
      "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
      "             ReLU-83          [-1, 128, 28, 28]               0\n",
      "           Conv2d-84          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-85          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-86          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-87          [-1, 512, 28, 28]               0\n",
      "           Conv2d-88          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-89          [-1, 256, 28, 28]             512\n",
      "             ReLU-90          [-1, 256, 28, 28]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-95         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-96         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-97         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-98         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-99         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-100          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-101          [-1, 256, 14, 14]             512\n",
      "            ReLU-102          [-1, 256, 14, 14]               0\n",
      "          Conv2d-103          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-104          [-1, 256, 14, 14]             512\n",
      "            ReLU-105          [-1, 256, 14, 14]               0\n",
      "          Conv2d-106         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-107         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-108         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-109         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-110          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-111          [-1, 256, 14, 14]             512\n",
      "            ReLU-112          [-1, 256, 14, 14]               0\n",
      "          Conv2d-113          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-114          [-1, 256, 14, 14]             512\n",
      "            ReLU-115          [-1, 256, 14, 14]               0\n",
      "          Conv2d-116         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-117         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-118         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-119         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-120          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
      "            ReLU-122          [-1, 256, 14, 14]               0\n",
      "          Conv2d-123          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-124          [-1, 256, 14, 14]             512\n",
      "            ReLU-125          [-1, 256, 14, 14]               0\n",
      "          Conv2d-126         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-127         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-128         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-129         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-130          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-131          [-1, 256, 14, 14]             512\n",
      "            ReLU-132          [-1, 256, 14, 14]               0\n",
      "          Conv2d-133          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-134          [-1, 256, 14, 14]             512\n",
      "            ReLU-135          [-1, 256, 14, 14]               0\n",
      "          Conv2d-136         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-137         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-138         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-139         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-140          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-141          [-1, 256, 14, 14]             512\n",
      "            ReLU-142          [-1, 256, 14, 14]               0\n",
      "          Conv2d-143          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-144          [-1, 256, 14, 14]             512\n",
      "            ReLU-145          [-1, 256, 14, 14]               0\n",
      "          Conv2d-146         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-147         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-148         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-149         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-150          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
      "            ReLU-152          [-1, 256, 14, 14]               0\n",
      "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
      "            ReLU-155          [-1, 256, 14, 14]               0\n",
      "          Conv2d-156         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-157         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-158         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-159         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-160          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-161          [-1, 256, 14, 14]             512\n",
      "            ReLU-162          [-1, 256, 14, 14]               0\n",
      "          Conv2d-163          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-164          [-1, 256, 14, 14]             512\n",
      "            ReLU-165          [-1, 256, 14, 14]               0\n",
      "          Conv2d-166         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-167         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-168         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-169         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-170          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-171          [-1, 256, 14, 14]             512\n",
      "            ReLU-172          [-1, 256, 14, 14]               0\n",
      "          Conv2d-173          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-174          [-1, 256, 14, 14]             512\n",
      "            ReLU-175          [-1, 256, 14, 14]               0\n",
      "          Conv2d-176         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-177         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-178         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-179         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-180          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-181          [-1, 256, 14, 14]             512\n",
      "            ReLU-182          [-1, 256, 14, 14]               0\n",
      "          Conv2d-183          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-184          [-1, 256, 14, 14]             512\n",
      "            ReLU-185          [-1, 256, 14, 14]               0\n",
      "          Conv2d-186         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-187         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-188         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-189         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-190          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
      "            ReLU-192          [-1, 256, 14, 14]               0\n",
      "          Conv2d-193          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-194          [-1, 256, 14, 14]             512\n",
      "            ReLU-195          [-1, 256, 14, 14]               0\n",
      "          Conv2d-196         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-197         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-198         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-199         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-200          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-201          [-1, 256, 14, 14]             512\n",
      "            ReLU-202          [-1, 256, 14, 14]               0\n",
      "          Conv2d-203          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-204          [-1, 256, 14, 14]             512\n",
      "            ReLU-205          [-1, 256, 14, 14]               0\n",
      "          Conv2d-206         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-207         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-208         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-209         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-210          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-211          [-1, 256, 14, 14]             512\n",
      "            ReLU-212          [-1, 256, 14, 14]               0\n",
      "          Conv2d-213          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-214          [-1, 256, 14, 14]             512\n",
      "            ReLU-215          [-1, 256, 14, 14]               0\n",
      "          Conv2d-216         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-217         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-218         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-219         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-220          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-221          [-1, 256, 14, 14]             512\n",
      "            ReLU-222          [-1, 256, 14, 14]               0\n",
      "          Conv2d-223          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-224          [-1, 256, 14, 14]             512\n",
      "            ReLU-225          [-1, 256, 14, 14]               0\n",
      "          Conv2d-226         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-227         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-228         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-229         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-230          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-231          [-1, 256, 14, 14]             512\n",
      "            ReLU-232          [-1, 256, 14, 14]               0\n",
      "          Conv2d-233          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-234          [-1, 256, 14, 14]             512\n",
      "            ReLU-235          [-1, 256, 14, 14]               0\n",
      "          Conv2d-236         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-237         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-238         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-239         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-240          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-241          [-1, 256, 14, 14]             512\n",
      "            ReLU-242          [-1, 256, 14, 14]               0\n",
      "          Conv2d-243          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-244          [-1, 256, 14, 14]             512\n",
      "            ReLU-245          [-1, 256, 14, 14]               0\n",
      "          Conv2d-246         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-247         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-248         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-249         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-250          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-251          [-1, 256, 14, 14]             512\n",
      "            ReLU-252          [-1, 256, 14, 14]               0\n",
      "          Conv2d-253          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-254          [-1, 256, 14, 14]             512\n",
      "            ReLU-255          [-1, 256, 14, 14]               0\n",
      "          Conv2d-256         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-257         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-258         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-259         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-260          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-261          [-1, 256, 14, 14]             512\n",
      "            ReLU-262          [-1, 256, 14, 14]               0\n",
      "          Conv2d-263          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-264          [-1, 256, 14, 14]             512\n",
      "            ReLU-265          [-1, 256, 14, 14]               0\n",
      "          Conv2d-266         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-267         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-268         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-269         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-270          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-271          [-1, 256, 14, 14]             512\n",
      "            ReLU-272          [-1, 256, 14, 14]               0\n",
      "          Conv2d-273          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-274          [-1, 256, 14, 14]             512\n",
      "            ReLU-275          [-1, 256, 14, 14]               0\n",
      "          Conv2d-276         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-277         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-278         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-279         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-280          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-281          [-1, 256, 14, 14]             512\n",
      "            ReLU-282          [-1, 256, 14, 14]               0\n",
      "          Conv2d-283          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-284          [-1, 256, 14, 14]             512\n",
      "            ReLU-285          [-1, 256, 14, 14]               0\n",
      "          Conv2d-286         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-287         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-288         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-289         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-290          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-291          [-1, 256, 14, 14]             512\n",
      "            ReLU-292          [-1, 256, 14, 14]               0\n",
      "          Conv2d-293          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-294          [-1, 256, 14, 14]             512\n",
      "            ReLU-295          [-1, 256, 14, 14]               0\n",
      "          Conv2d-296         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-297         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-298         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-299         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-300          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-301          [-1, 256, 14, 14]             512\n",
      "            ReLU-302          [-1, 256, 14, 14]               0\n",
      "          Conv2d-303          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-304          [-1, 256, 14, 14]             512\n",
      "            ReLU-305          [-1, 256, 14, 14]               0\n",
      "          Conv2d-306         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-307         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-308         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-309         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-310          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-311          [-1, 256, 14, 14]             512\n",
      "            ReLU-312          [-1, 256, 14, 14]               0\n",
      "          Conv2d-313          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-314          [-1, 256, 14, 14]             512\n",
      "            ReLU-315          [-1, 256, 14, 14]               0\n",
      "          Conv2d-316         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-317         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-318         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-319         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-320          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-321          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-322          [-1, 512, 14, 14]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-327           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-328           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-329           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-330           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-331           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-332            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-333            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-334            [-1, 512, 7, 7]               0\n",
      "          Conv2d-335            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-336            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-337            [-1, 512, 7, 7]               0\n",
      "          Conv2d-338           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-339           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-340           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-341           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-342            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-343            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-344            [-1, 512, 7, 7]               0\n",
      "          Conv2d-345            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-346            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-347            [-1, 512, 7, 7]               0\n",
      "          Conv2d-348           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-349           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-350           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-351           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-352           [-1, 2048, 1, 1]               0\n",
      "          Linear-353                  [-1, 101]         206,949\n",
      "================================================================\n",
      "Total params: 43,575,813\n",
      "Trainable params: 43,575,813\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 435.33\n",
      "Params size (MB): 166.23\n",
      "Estimated Total Size (MB): 602.13\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model = resnet101()\n",
    "model.fc=nn.Linear(2048,num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(batch_size,3,224,224)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(\"model shape ready\")\n",
    "print(summary(model, input_size=(3, 224, 224)))\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592a8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs//ResNet101_sp20211224110536\n",
      "grandstore_string\n",
      "grandstore/caltech101_ResNet101_sp20211224110536.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+\"/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3316dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86232036408f4595a47bb831119f107c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1534290f01741228b22e5468ea3d6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b02b4b07154d8e8c230d6968e799af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.250\n",
      "Training Loss: 8940249.8343 \tValidation Loss: 63708994.9609\n",
      "\n",
      "Running epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996da9a452834f3783488eca39156f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b720668cd04a0caf56d6c470263365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb37f3f482aa4a8f8547d306a81bb99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.250\n",
      "Training Loss: 8402543.1532 \tValidation Loss: 165497198.8281\n",
      "\n",
      "Running epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd36e3af17a9463fb59cb547bbdb2e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbef538986e1468a80b1e23d96d24c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea266da8323c4bed8dabdc6f6a09b9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.250\n",
      "Training Loss: 8376924.0370 \tValidation Loss: 286606250.4688\n",
      "\n",
      "Running epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0246fb0aebe1490692abca5cb9d9c524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 8\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=280,\n",
    "                    minepoch=200,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                 \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4013c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model of: ResNet101_sp running on: cifar10\n",
      "\n",
      "Total Run 120 epoch(s)\n",
      "Accuracy MIN: 38.97 / MAX: 85.42\n",
      "\n",
      "Top 5 performing epochs:\n",
      "#1 epoch 96\t||train_acc 85.42%\t||test 84.41%\n",
      "#2 epoch 112\t||train_acc 85.36%\t||test 84.94%\n",
      "#3 epoch 108\t||train_acc 85.32%\t||test 84.82%\n",
      "#4 epoch 114\t||train_acc 85.22%\t||test 84.67%\n",
      "#5 epoch 116\t||train_acc 85.19%\t||test 84.75%\n",
      "\n",
      "Last epoch:\n",
      "epoch 120\t||train_acc 85.12%\t||test 84.68%\n",
      "\n",
      "The model has parameters: 43389354\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model of: \"+model_name +\n",
    "      \" running on: \"+dataset_name+\"\\n\")\n",
    "\n",
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {}%\\t||test {}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {}%\\t||test {}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ddef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
