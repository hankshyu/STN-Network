{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1314c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auther: Tzu-Han Hsu\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) 2022, Anywhere Door Lab (ADL) and Tzu-Han Hsu\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "#    list of conditions and the following disclaimer.\n",
    "\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "#    this list of conditions and the following disclaimer in the documentation\n",
    "#    and/or other materials provided with the distribution.\n",
    "\n",
    "# 3. Neither the name of the copyright holder nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived from\n",
    "#    this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DenseNet161_sp with 10 classes running on: cifar10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "#vital params\n",
    "\n",
    " \n",
    "model_name=\"DenseNet161_sp\"\n",
    "\n",
    "dataset_name=\"cifar10\"\n",
    "\n",
    "#hyperparameters\n",
    "batch_size=20\n",
    "num_classes=-1\n",
    "learning_rate=0.00075\n",
    "input_size=784\n",
    "image_size=(224,224)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38958e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset size: Train: 40000, Valid: 10000, Test: 10000\n",
      "torch.Size([3, 224, 224])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd38adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "#from .utils import load_state_dict_from_url\n",
    "\n",
    "\n",
    "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
    "\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)  \n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features] \n",
    "        for name, layer in self.named_children():   \n",
    "            new_features = layer(*features) \n",
    "            features.append(new_features)  \n",
    "        return torch.cat(features, 1)   \n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=num_classes, memory_efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,  \n",
    "                num_input_features=num_features,    \n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,    \n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)  \n",
    "            num_features = num_features + num_layers * growth_rate \n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)  \n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2   \n",
    "\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(27040, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        \n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 52 * 52)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.stn(x)\n",
    "        features = self.features(x) \n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))   \n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out) \n",
    "        return out\n",
    "\n",
    "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
    "              **kwargs):\n",
    "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
    "    if pretrained:\n",
    "        _load_state_dict(model, model_urls[arch], progress)\n",
    "    return model\n",
    "\n",
    "\n",
    "def densenet121(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-121 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet161(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-161 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet161', 32, (6, 12, 32, 32), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet169(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-169 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet169', 32, (6, 12, 48, 32), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet201(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-201 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet201', 32, (6, 12, 64, 48), 64, pretrained, progress,\n",
    "                     **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64, 10])\n",
      "model shape ready\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 218, 218]           1,184\n",
      "         MaxPool2d-2          [-1, 8, 109, 109]               0\n",
      "              ReLU-3          [-1, 8, 109, 109]               0\n",
      "            Conv2d-4         [-1, 10, 105, 105]           2,010\n",
      "         MaxPool2d-5           [-1, 10, 52, 52]               0\n",
      "              ReLU-6           [-1, 10, 52, 52]               0\n",
      "            Linear-7                   [-1, 32]         865,312\n",
      "              ReLU-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 6]             198\n",
      "           Conv2d-10         [-1, 64, 112, 112]           9,408\n",
      "      BatchNorm2d-11         [-1, 64, 112, 112]             128\n",
      "             ReLU-12         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-13           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-14           [-1, 64, 56, 56]             128\n",
      "             ReLU-15           [-1, 64, 56, 56]               0\n",
      "           Conv2d-16          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
      "             ReLU-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 96, 56, 56]             192\n",
      "             ReLU-21           [-1, 96, 56, 56]               0\n",
      "           Conv2d-22          [-1, 128, 56, 56]          12,288\n",
      "      BatchNorm2d-23          [-1, 128, 56, 56]             256\n",
      "             ReLU-24          [-1, 128, 56, 56]               0\n",
      "           Conv2d-25           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-26          [-1, 128, 56, 56]             256\n",
      "             ReLU-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28          [-1, 128, 56, 56]          16,384\n",
      "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
      "             ReLU-30          [-1, 128, 56, 56]               0\n",
      "           Conv2d-31           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-32          [-1, 160, 56, 56]             320\n",
      "             ReLU-33          [-1, 160, 56, 56]               0\n",
      "           Conv2d-34          [-1, 128, 56, 56]          20,480\n",
      "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
      "             ReLU-36          [-1, 128, 56, 56]               0\n",
      "           Conv2d-37           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-38          [-1, 192, 56, 56]             384\n",
      "             ReLU-39          [-1, 192, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 56, 56]          24,576\n",
      "      BatchNorm2d-41          [-1, 128, 56, 56]             256\n",
      "             ReLU-42          [-1, 128, 56, 56]               0\n",
      "           Conv2d-43           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-44          [-1, 224, 56, 56]             448\n",
      "             ReLU-45          [-1, 224, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 56, 56]          28,672\n",
      "      BatchNorm2d-47          [-1, 128, 56, 56]             256\n",
      "             ReLU-48          [-1, 128, 56, 56]               0\n",
      "           Conv2d-49           [-1, 32, 56, 56]          36,864\n",
      "      _DenseBlock-50          [-1, 256, 56, 56]               0\n",
      "      BatchNorm2d-51          [-1, 256, 56, 56]             512\n",
      "             ReLU-52          [-1, 256, 56, 56]               0\n",
      "           Conv2d-53          [-1, 128, 56, 56]          32,768\n",
      "        AvgPool2d-54          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
      "             ReLU-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-61          [-1, 160, 28, 28]             320\n",
      "             ReLU-62          [-1, 160, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]          20,480\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-67          [-1, 192, 28, 28]             384\n",
      "             ReLU-68          [-1, 192, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          24,576\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-73          [-1, 224, 28, 28]             448\n",
      "             ReLU-74          [-1, 224, 28, 28]               0\n",
      "           Conv2d-75          [-1, 128, 28, 28]          28,672\n",
      "      BatchNorm2d-76          [-1, 128, 28, 28]             256\n",
      "             ReLU-77          [-1, 128, 28, 28]               0\n",
      "           Conv2d-78           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-79          [-1, 256, 28, 28]             512\n",
      "             ReLU-80          [-1, 256, 28, 28]               0\n",
      "           Conv2d-81          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
      "             ReLU-83          [-1, 128, 28, 28]               0\n",
      "           Conv2d-84           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-85          [-1, 288, 28, 28]             576\n",
      "             ReLU-86          [-1, 288, 28, 28]               0\n",
      "           Conv2d-87          [-1, 128, 28, 28]          36,864\n",
      "      BatchNorm2d-88          [-1, 128, 28, 28]             256\n",
      "             ReLU-89          [-1, 128, 28, 28]               0\n",
      "           Conv2d-90           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-91          [-1, 320, 28, 28]             640\n",
      "             ReLU-92          [-1, 320, 28, 28]               0\n",
      "           Conv2d-93          [-1, 128, 28, 28]          40,960\n",
      "      BatchNorm2d-94          [-1, 128, 28, 28]             256\n",
      "             ReLU-95          [-1, 128, 28, 28]               0\n",
      "           Conv2d-96           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-97          [-1, 352, 28, 28]             704\n",
      "             ReLU-98          [-1, 352, 28, 28]               0\n",
      "           Conv2d-99          [-1, 128, 28, 28]          45,056\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-103          [-1, 384, 28, 28]             768\n",
      "            ReLU-104          [-1, 384, 28, 28]               0\n",
      "          Conv2d-105          [-1, 128, 28, 28]          49,152\n",
      "     BatchNorm2d-106          [-1, 128, 28, 28]             256\n",
      "            ReLU-107          [-1, 128, 28, 28]               0\n",
      "          Conv2d-108           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-109          [-1, 416, 28, 28]             832\n",
      "            ReLU-110          [-1, 416, 28, 28]               0\n",
      "          Conv2d-111          [-1, 128, 28, 28]          53,248\n",
      "     BatchNorm2d-112          [-1, 128, 28, 28]             256\n",
      "            ReLU-113          [-1, 128, 28, 28]               0\n",
      "          Conv2d-114           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-115          [-1, 448, 28, 28]             896\n",
      "            ReLU-116          [-1, 448, 28, 28]               0\n",
      "          Conv2d-117          [-1, 128, 28, 28]          57,344\n",
      "     BatchNorm2d-118          [-1, 128, 28, 28]             256\n",
      "            ReLU-119          [-1, 128, 28, 28]               0\n",
      "          Conv2d-120           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-121          [-1, 480, 28, 28]             960\n",
      "            ReLU-122          [-1, 480, 28, 28]               0\n",
      "          Conv2d-123          [-1, 128, 28, 28]          61,440\n",
      "     BatchNorm2d-124          [-1, 128, 28, 28]             256\n",
      "            ReLU-125          [-1, 128, 28, 28]               0\n",
      "          Conv2d-126           [-1, 32, 28, 28]          36,864\n",
      "     _DenseBlock-127          [-1, 512, 28, 28]               0\n",
      "     BatchNorm2d-128          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-129          [-1, 512, 28, 28]               0\n",
      "          Conv2d-130          [-1, 256, 28, 28]         131,072\n",
      "       AvgPool2d-131          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-135          [-1, 128, 14, 14]             256\n",
      "            ReLU-136          [-1, 128, 14, 14]               0\n",
      "          Conv2d-137           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-138          [-1, 288, 14, 14]             576\n",
      "            ReLU-139          [-1, 288, 14, 14]               0\n",
      "          Conv2d-140          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-141          [-1, 128, 14, 14]             256\n",
      "            ReLU-142          [-1, 128, 14, 14]               0\n",
      "          Conv2d-143           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-144          [-1, 320, 14, 14]             640\n",
      "            ReLU-145          [-1, 320, 14, 14]               0\n",
      "          Conv2d-146          [-1, 128, 14, 14]          40,960\n",
      "     BatchNorm2d-147          [-1, 128, 14, 14]             256\n",
      "            ReLU-148          [-1, 128, 14, 14]               0\n",
      "          Conv2d-149           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-150          [-1, 352, 14, 14]             704\n",
      "            ReLU-151          [-1, 352, 14, 14]               0\n",
      "          Conv2d-152          [-1, 128, 14, 14]          45,056\n",
      "     BatchNorm2d-153          [-1, 128, 14, 14]             256\n",
      "            ReLU-154          [-1, 128, 14, 14]               0\n",
      "          Conv2d-155           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-156          [-1, 384, 14, 14]             768\n",
      "            ReLU-157          [-1, 384, 14, 14]               0\n",
      "          Conv2d-158          [-1, 128, 14, 14]          49,152\n",
      "     BatchNorm2d-159          [-1, 128, 14, 14]             256\n",
      "            ReLU-160          [-1, 128, 14, 14]               0\n",
      "          Conv2d-161           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-162          [-1, 416, 14, 14]             832\n",
      "            ReLU-163          [-1, 416, 14, 14]               0\n",
      "          Conv2d-164          [-1, 128, 14, 14]          53,248\n",
      "     BatchNorm2d-165          [-1, 128, 14, 14]             256\n",
      "            ReLU-166          [-1, 128, 14, 14]               0\n",
      "          Conv2d-167           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-168          [-1, 448, 14, 14]             896\n",
      "            ReLU-169          [-1, 448, 14, 14]               0\n",
      "          Conv2d-170          [-1, 128, 14, 14]          57,344\n",
      "     BatchNorm2d-171          [-1, 128, 14, 14]             256\n",
      "            ReLU-172          [-1, 128, 14, 14]               0\n",
      "          Conv2d-173           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-174          [-1, 480, 14, 14]             960\n",
      "            ReLU-175          [-1, 480, 14, 14]               0\n",
      "          Conv2d-176          [-1, 128, 14, 14]          61,440\n",
      "     BatchNorm2d-177          [-1, 128, 14, 14]             256\n",
      "            ReLU-178          [-1, 128, 14, 14]               0\n",
      "          Conv2d-179           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-180          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-181          [-1, 512, 14, 14]               0\n",
      "          Conv2d-182          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-183          [-1, 128, 14, 14]             256\n",
      "            ReLU-184          [-1, 128, 14, 14]               0\n",
      "          Conv2d-185           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-186          [-1, 544, 14, 14]           1,088\n",
      "            ReLU-187          [-1, 544, 14, 14]               0\n",
      "          Conv2d-188          [-1, 128, 14, 14]          69,632\n",
      "     BatchNorm2d-189          [-1, 128, 14, 14]             256\n",
      "            ReLU-190          [-1, 128, 14, 14]               0\n",
      "          Conv2d-191           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-192          [-1, 576, 14, 14]           1,152\n",
      "            ReLU-193          [-1, 576, 14, 14]               0\n",
      "          Conv2d-194          [-1, 128, 14, 14]          73,728\n",
      "     BatchNorm2d-195          [-1, 128, 14, 14]             256\n",
      "            ReLU-196          [-1, 128, 14, 14]               0\n",
      "          Conv2d-197           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-198          [-1, 608, 14, 14]           1,216\n",
      "            ReLU-199          [-1, 608, 14, 14]               0\n",
      "          Conv2d-200          [-1, 128, 14, 14]          77,824\n",
      "     BatchNorm2d-201          [-1, 128, 14, 14]             256\n",
      "            ReLU-202          [-1, 128, 14, 14]               0\n",
      "          Conv2d-203           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-204          [-1, 640, 14, 14]           1,280\n",
      "            ReLU-205          [-1, 640, 14, 14]               0\n",
      "          Conv2d-206          [-1, 128, 14, 14]          81,920\n",
      "     BatchNorm2d-207          [-1, 128, 14, 14]             256\n",
      "            ReLU-208          [-1, 128, 14, 14]               0\n",
      "          Conv2d-209           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-210          [-1, 672, 14, 14]           1,344\n",
      "            ReLU-211          [-1, 672, 14, 14]               0\n",
      "          Conv2d-212          [-1, 128, 14, 14]          86,016\n",
      "     BatchNorm2d-213          [-1, 128, 14, 14]             256\n",
      "            ReLU-214          [-1, 128, 14, 14]               0\n",
      "          Conv2d-215           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-216          [-1, 704, 14, 14]           1,408\n",
      "            ReLU-217          [-1, 704, 14, 14]               0\n",
      "          Conv2d-218          [-1, 128, 14, 14]          90,112\n",
      "     BatchNorm2d-219          [-1, 128, 14, 14]             256\n",
      "            ReLU-220          [-1, 128, 14, 14]               0\n",
      "          Conv2d-221           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-222          [-1, 736, 14, 14]           1,472\n",
      "            ReLU-223          [-1, 736, 14, 14]               0\n",
      "          Conv2d-224          [-1, 128, 14, 14]          94,208\n",
      "     BatchNorm2d-225          [-1, 128, 14, 14]             256\n",
      "            ReLU-226          [-1, 128, 14, 14]               0\n",
      "          Conv2d-227           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-228          [-1, 768, 14, 14]           1,536\n",
      "            ReLU-229          [-1, 768, 14, 14]               0\n",
      "          Conv2d-230          [-1, 128, 14, 14]          98,304\n",
      "     BatchNorm2d-231          [-1, 128, 14, 14]             256\n",
      "            ReLU-232          [-1, 128, 14, 14]               0\n",
      "          Conv2d-233           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-234          [-1, 800, 14, 14]           1,600\n",
      "            ReLU-235          [-1, 800, 14, 14]               0\n",
      "          Conv2d-236          [-1, 128, 14, 14]         102,400\n",
      "     BatchNorm2d-237          [-1, 128, 14, 14]             256\n",
      "            ReLU-238          [-1, 128, 14, 14]               0\n",
      "          Conv2d-239           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-240          [-1, 832, 14, 14]           1,664\n",
      "            ReLU-241          [-1, 832, 14, 14]               0\n",
      "          Conv2d-242          [-1, 128, 14, 14]         106,496\n",
      "     BatchNorm2d-243          [-1, 128, 14, 14]             256\n",
      "            ReLU-244          [-1, 128, 14, 14]               0\n",
      "          Conv2d-245           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-246          [-1, 864, 14, 14]           1,728\n",
      "            ReLU-247          [-1, 864, 14, 14]               0\n",
      "          Conv2d-248          [-1, 128, 14, 14]         110,592\n",
      "     BatchNorm2d-249          [-1, 128, 14, 14]             256\n",
      "            ReLU-250          [-1, 128, 14, 14]               0\n",
      "          Conv2d-251           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-252          [-1, 896, 14, 14]           1,792\n",
      "            ReLU-253          [-1, 896, 14, 14]               0\n",
      "          Conv2d-254          [-1, 128, 14, 14]         114,688\n",
      "     BatchNorm2d-255          [-1, 128, 14, 14]             256\n",
      "            ReLU-256          [-1, 128, 14, 14]               0\n",
      "          Conv2d-257           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-258          [-1, 928, 14, 14]           1,856\n",
      "            ReLU-259          [-1, 928, 14, 14]               0\n",
      "          Conv2d-260          [-1, 128, 14, 14]         118,784\n",
      "     BatchNorm2d-261          [-1, 128, 14, 14]             256\n",
      "            ReLU-262          [-1, 128, 14, 14]               0\n",
      "          Conv2d-263           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-264          [-1, 960, 14, 14]           1,920\n",
      "            ReLU-265          [-1, 960, 14, 14]               0\n",
      "          Conv2d-266          [-1, 128, 14, 14]         122,880\n",
      "     BatchNorm2d-267          [-1, 128, 14, 14]             256\n",
      "            ReLU-268          [-1, 128, 14, 14]               0\n",
      "          Conv2d-269           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-270          [-1, 992, 14, 14]           1,984\n",
      "            ReLU-271          [-1, 992, 14, 14]               0\n",
      "          Conv2d-272          [-1, 128, 14, 14]         126,976\n",
      "     BatchNorm2d-273          [-1, 128, 14, 14]             256\n",
      "            ReLU-274          [-1, 128, 14, 14]               0\n",
      "          Conv2d-275           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-276         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-277         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-278          [-1, 128, 14, 14]         131,072\n",
      "     BatchNorm2d-279          [-1, 128, 14, 14]             256\n",
      "            ReLU-280          [-1, 128, 14, 14]               0\n",
      "          Conv2d-281           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-282         [-1, 1056, 14, 14]           2,112\n",
      "            ReLU-283         [-1, 1056, 14, 14]               0\n",
      "          Conv2d-284          [-1, 128, 14, 14]         135,168\n",
      "     BatchNorm2d-285          [-1, 128, 14, 14]             256\n",
      "            ReLU-286          [-1, 128, 14, 14]               0\n",
      "          Conv2d-287           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-288         [-1, 1088, 14, 14]           2,176\n",
      "            ReLU-289         [-1, 1088, 14, 14]               0\n",
      "          Conv2d-290          [-1, 128, 14, 14]         139,264\n",
      "     BatchNorm2d-291          [-1, 128, 14, 14]             256\n",
      "            ReLU-292          [-1, 128, 14, 14]               0\n",
      "          Conv2d-293           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-294         [-1, 1120, 14, 14]           2,240\n",
      "            ReLU-295         [-1, 1120, 14, 14]               0\n",
      "          Conv2d-296          [-1, 128, 14, 14]         143,360\n",
      "     BatchNorm2d-297          [-1, 128, 14, 14]             256\n",
      "            ReLU-298          [-1, 128, 14, 14]               0\n",
      "          Conv2d-299           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-300         [-1, 1152, 14, 14]           2,304\n",
      "            ReLU-301         [-1, 1152, 14, 14]               0\n",
      "          Conv2d-302          [-1, 128, 14, 14]         147,456\n",
      "     BatchNorm2d-303          [-1, 128, 14, 14]             256\n",
      "            ReLU-304          [-1, 128, 14, 14]               0\n",
      "          Conv2d-305           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-306         [-1, 1184, 14, 14]           2,368\n",
      "            ReLU-307         [-1, 1184, 14, 14]               0\n",
      "          Conv2d-308          [-1, 128, 14, 14]         151,552\n",
      "     BatchNorm2d-309          [-1, 128, 14, 14]             256\n",
      "            ReLU-310          [-1, 128, 14, 14]               0\n",
      "          Conv2d-311           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-312         [-1, 1216, 14, 14]           2,432\n",
      "            ReLU-313         [-1, 1216, 14, 14]               0\n",
      "          Conv2d-314          [-1, 128, 14, 14]         155,648\n",
      "     BatchNorm2d-315          [-1, 128, 14, 14]             256\n",
      "            ReLU-316          [-1, 128, 14, 14]               0\n",
      "          Conv2d-317           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-318         [-1, 1248, 14, 14]           2,496\n",
      "            ReLU-319         [-1, 1248, 14, 14]               0\n",
      "          Conv2d-320          [-1, 128, 14, 14]         159,744\n",
      "     BatchNorm2d-321          [-1, 128, 14, 14]             256\n",
      "            ReLU-322          [-1, 128, 14, 14]               0\n",
      "          Conv2d-323           [-1, 32, 14, 14]          36,864\n",
      "     _DenseBlock-324         [-1, 1280, 14, 14]               0\n",
      "     BatchNorm2d-325         [-1, 1280, 14, 14]           2,560\n",
      "            ReLU-326         [-1, 1280, 14, 14]               0\n",
      "          Conv2d-327          [-1, 640, 14, 14]         819,200\n",
      "       AvgPool2d-328            [-1, 640, 7, 7]               0\n",
      "     BatchNorm2d-329            [-1, 640, 7, 7]           1,280\n",
      "            ReLU-330            [-1, 640, 7, 7]               0\n",
      "          Conv2d-331            [-1, 128, 7, 7]          81,920\n",
      "     BatchNorm2d-332            [-1, 128, 7, 7]             256\n",
      "            ReLU-333            [-1, 128, 7, 7]               0\n",
      "          Conv2d-334             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-335            [-1, 672, 7, 7]           1,344\n",
      "            ReLU-336            [-1, 672, 7, 7]               0\n",
      "          Conv2d-337            [-1, 128, 7, 7]          86,016\n",
      "     BatchNorm2d-338            [-1, 128, 7, 7]             256\n",
      "            ReLU-339            [-1, 128, 7, 7]               0\n",
      "          Conv2d-340             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-341            [-1, 704, 7, 7]           1,408\n",
      "            ReLU-342            [-1, 704, 7, 7]               0\n",
      "          Conv2d-343            [-1, 128, 7, 7]          90,112\n",
      "     BatchNorm2d-344            [-1, 128, 7, 7]             256\n",
      "            ReLU-345            [-1, 128, 7, 7]               0\n",
      "          Conv2d-346             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-347            [-1, 736, 7, 7]           1,472\n",
      "            ReLU-348            [-1, 736, 7, 7]               0\n",
      "          Conv2d-349            [-1, 128, 7, 7]          94,208\n",
      "     BatchNorm2d-350            [-1, 128, 7, 7]             256\n",
      "            ReLU-351            [-1, 128, 7, 7]               0\n",
      "          Conv2d-352             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-353            [-1, 768, 7, 7]           1,536\n",
      "            ReLU-354            [-1, 768, 7, 7]               0\n",
      "          Conv2d-355            [-1, 128, 7, 7]          98,304\n",
      "     BatchNorm2d-356            [-1, 128, 7, 7]             256\n",
      "            ReLU-357            [-1, 128, 7, 7]               0\n",
      "          Conv2d-358             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-359            [-1, 800, 7, 7]           1,600\n",
      "            ReLU-360            [-1, 800, 7, 7]               0\n",
      "          Conv2d-361            [-1, 128, 7, 7]         102,400\n",
      "     BatchNorm2d-362            [-1, 128, 7, 7]             256\n",
      "            ReLU-363            [-1, 128, 7, 7]               0\n",
      "          Conv2d-364             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-365            [-1, 832, 7, 7]           1,664\n",
      "            ReLU-366            [-1, 832, 7, 7]               0\n",
      "          Conv2d-367            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-368            [-1, 128, 7, 7]             256\n",
      "            ReLU-369            [-1, 128, 7, 7]               0\n",
      "          Conv2d-370             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-371            [-1, 864, 7, 7]           1,728\n",
      "            ReLU-372            [-1, 864, 7, 7]               0\n",
      "          Conv2d-373            [-1, 128, 7, 7]         110,592\n",
      "     BatchNorm2d-374            [-1, 128, 7, 7]             256\n",
      "            ReLU-375            [-1, 128, 7, 7]               0\n",
      "          Conv2d-376             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-377            [-1, 896, 7, 7]           1,792\n",
      "            ReLU-378            [-1, 896, 7, 7]               0\n",
      "          Conv2d-379            [-1, 128, 7, 7]         114,688\n",
      "     BatchNorm2d-380            [-1, 128, 7, 7]             256\n",
      "            ReLU-381            [-1, 128, 7, 7]               0\n",
      "          Conv2d-382             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-383            [-1, 928, 7, 7]           1,856\n",
      "            ReLU-384            [-1, 928, 7, 7]               0\n",
      "          Conv2d-385            [-1, 128, 7, 7]         118,784\n",
      "     BatchNorm2d-386            [-1, 128, 7, 7]             256\n",
      "            ReLU-387            [-1, 128, 7, 7]               0\n",
      "          Conv2d-388             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-389            [-1, 960, 7, 7]           1,920\n",
      "            ReLU-390            [-1, 960, 7, 7]               0\n",
      "          Conv2d-391            [-1, 128, 7, 7]         122,880\n",
      "     BatchNorm2d-392            [-1, 128, 7, 7]             256\n",
      "            ReLU-393            [-1, 128, 7, 7]               0\n",
      "          Conv2d-394             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-395            [-1, 992, 7, 7]           1,984\n",
      "            ReLU-396            [-1, 992, 7, 7]               0\n",
      "          Conv2d-397            [-1, 128, 7, 7]         126,976\n",
      "     BatchNorm2d-398            [-1, 128, 7, 7]             256\n",
      "            ReLU-399            [-1, 128, 7, 7]               0\n",
      "          Conv2d-400             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-401           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-402           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-403            [-1, 128, 7, 7]         131,072\n",
      "     BatchNorm2d-404            [-1, 128, 7, 7]             256\n",
      "            ReLU-405            [-1, 128, 7, 7]               0\n",
      "          Conv2d-406             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-407           [-1, 1056, 7, 7]           2,112\n",
      "            ReLU-408           [-1, 1056, 7, 7]               0\n",
      "          Conv2d-409            [-1, 128, 7, 7]         135,168\n",
      "     BatchNorm2d-410            [-1, 128, 7, 7]             256\n",
      "            ReLU-411            [-1, 128, 7, 7]               0\n",
      "          Conv2d-412             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-413           [-1, 1088, 7, 7]           2,176\n",
      "            ReLU-414           [-1, 1088, 7, 7]               0\n",
      "          Conv2d-415            [-1, 128, 7, 7]         139,264\n",
      "     BatchNorm2d-416            [-1, 128, 7, 7]             256\n",
      "            ReLU-417            [-1, 128, 7, 7]               0\n",
      "          Conv2d-418             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-419           [-1, 1120, 7, 7]           2,240\n",
      "            ReLU-420           [-1, 1120, 7, 7]               0\n",
      "          Conv2d-421            [-1, 128, 7, 7]         143,360\n",
      "     BatchNorm2d-422            [-1, 128, 7, 7]             256\n",
      "            ReLU-423            [-1, 128, 7, 7]               0\n",
      "          Conv2d-424             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-425           [-1, 1152, 7, 7]           2,304\n",
      "            ReLU-426           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-427            [-1, 128, 7, 7]         147,456\n",
      "     BatchNorm2d-428            [-1, 128, 7, 7]             256\n",
      "            ReLU-429            [-1, 128, 7, 7]               0\n",
      "          Conv2d-430             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-431           [-1, 1184, 7, 7]           2,368\n",
      "            ReLU-432           [-1, 1184, 7, 7]               0\n",
      "          Conv2d-433            [-1, 128, 7, 7]         151,552\n",
      "     BatchNorm2d-434            [-1, 128, 7, 7]             256\n",
      "            ReLU-435            [-1, 128, 7, 7]               0\n",
      "          Conv2d-436             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-437           [-1, 1216, 7, 7]           2,432\n",
      "            ReLU-438           [-1, 1216, 7, 7]               0\n",
      "          Conv2d-439            [-1, 128, 7, 7]         155,648\n",
      "     BatchNorm2d-440            [-1, 128, 7, 7]             256\n",
      "            ReLU-441            [-1, 128, 7, 7]               0\n",
      "          Conv2d-442             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-443           [-1, 1248, 7, 7]           2,496\n",
      "            ReLU-444           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-445            [-1, 128, 7, 7]         159,744\n",
      "     BatchNorm2d-446            [-1, 128, 7, 7]             256\n",
      "            ReLU-447            [-1, 128, 7, 7]               0\n",
      "          Conv2d-448             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-449           [-1, 1280, 7, 7]           2,560\n",
      "            ReLU-450           [-1, 1280, 7, 7]               0\n",
      "          Conv2d-451            [-1, 128, 7, 7]         163,840\n",
      "     BatchNorm2d-452            [-1, 128, 7, 7]             256\n",
      "            ReLU-453            [-1, 128, 7, 7]               0\n",
      "          Conv2d-454             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-455           [-1, 1312, 7, 7]           2,624\n",
      "            ReLU-456           [-1, 1312, 7, 7]               0\n",
      "          Conv2d-457            [-1, 128, 7, 7]         167,936\n",
      "     BatchNorm2d-458            [-1, 128, 7, 7]             256\n",
      "            ReLU-459            [-1, 128, 7, 7]               0\n",
      "          Conv2d-460             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-461           [-1, 1344, 7, 7]           2,688\n",
      "            ReLU-462           [-1, 1344, 7, 7]               0\n",
      "          Conv2d-463            [-1, 128, 7, 7]         172,032\n",
      "     BatchNorm2d-464            [-1, 128, 7, 7]             256\n",
      "            ReLU-465            [-1, 128, 7, 7]               0\n",
      "          Conv2d-466             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-467           [-1, 1376, 7, 7]           2,752\n",
      "            ReLU-468           [-1, 1376, 7, 7]               0\n",
      "          Conv2d-469            [-1, 128, 7, 7]         176,128\n",
      "     BatchNorm2d-470            [-1, 128, 7, 7]             256\n",
      "            ReLU-471            [-1, 128, 7, 7]               0\n",
      "          Conv2d-472             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-473           [-1, 1408, 7, 7]           2,816\n",
      "            ReLU-474           [-1, 1408, 7, 7]               0\n",
      "          Conv2d-475            [-1, 128, 7, 7]         180,224\n",
      "     BatchNorm2d-476            [-1, 128, 7, 7]             256\n",
      "            ReLU-477            [-1, 128, 7, 7]               0\n",
      "          Conv2d-478             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-479           [-1, 1440, 7, 7]           2,880\n",
      "            ReLU-480           [-1, 1440, 7, 7]               0\n",
      "          Conv2d-481            [-1, 128, 7, 7]         184,320\n",
      "     BatchNorm2d-482            [-1, 128, 7, 7]             256\n",
      "            ReLU-483            [-1, 128, 7, 7]               0\n",
      "          Conv2d-484             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-485           [-1, 1472, 7, 7]           2,944\n",
      "            ReLU-486           [-1, 1472, 7, 7]               0\n",
      "          Conv2d-487            [-1, 128, 7, 7]         188,416\n",
      "     BatchNorm2d-488            [-1, 128, 7, 7]             256\n",
      "            ReLU-489            [-1, 128, 7, 7]               0\n",
      "          Conv2d-490             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-491           [-1, 1504, 7, 7]           3,008\n",
      "            ReLU-492           [-1, 1504, 7, 7]               0\n",
      "          Conv2d-493            [-1, 128, 7, 7]         192,512\n",
      "     BatchNorm2d-494            [-1, 128, 7, 7]             256\n",
      "            ReLU-495            [-1, 128, 7, 7]               0\n",
      "          Conv2d-496             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-497           [-1, 1536, 7, 7]           3,072\n",
      "            ReLU-498           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-499            [-1, 128, 7, 7]         196,608\n",
      "     BatchNorm2d-500            [-1, 128, 7, 7]             256\n",
      "            ReLU-501            [-1, 128, 7, 7]               0\n",
      "          Conv2d-502             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-503           [-1, 1568, 7, 7]           3,136\n",
      "            ReLU-504           [-1, 1568, 7, 7]               0\n",
      "          Conv2d-505            [-1, 128, 7, 7]         200,704\n",
      "     BatchNorm2d-506            [-1, 128, 7, 7]             256\n",
      "            ReLU-507            [-1, 128, 7, 7]               0\n",
      "          Conv2d-508             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-509           [-1, 1600, 7, 7]           3,200\n",
      "            ReLU-510           [-1, 1600, 7, 7]               0\n",
      "          Conv2d-511            [-1, 128, 7, 7]         204,800\n",
      "     BatchNorm2d-512            [-1, 128, 7, 7]             256\n",
      "            ReLU-513            [-1, 128, 7, 7]               0\n",
      "          Conv2d-514             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-515           [-1, 1632, 7, 7]           3,264\n",
      "            ReLU-516           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-517            [-1, 128, 7, 7]         208,896\n",
      "     BatchNorm2d-518            [-1, 128, 7, 7]             256\n",
      "            ReLU-519            [-1, 128, 7, 7]               0\n",
      "          Conv2d-520             [-1, 32, 7, 7]          36,864\n",
      "     _DenseBlock-521           [-1, 1664, 7, 7]               0\n",
      "     BatchNorm2d-522           [-1, 1664, 7, 7]           3,328\n",
      "          Linear-523                   [-1, 10]          16,650\n",
      "================================================================\n",
      "Total params: 13,369,834\n",
      "Trainable params: 13,369,834\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 365.59\n",
      "Params size (MB): 51.00\n",
      "Estimated Total Size (MB): 417.17\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model = densenet161().to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(64,3,224,224)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(\"model shape ready\")\n",
    "print(summary(model, input_size=(3, 224, 224)))\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592a8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs//DenseNet161_sp20211031093211\n",
      "grandstore_string\n",
      "grandstore/cifar10_DenseNet161_sp20211031093211.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+\"/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3316dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ee72da29624bf58ebaabe9f8020974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acc987691624dcba5b7d68c0803df3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480c90f7a4f4232854eb8922bf35227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.160\n",
      "Training Loss: 93292325.3820 \tValidation Loss: 132131229.0918\n",
      "\n",
      "Running epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07999c0dd7b545a7b6f4b79c1008e6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9bda22419646b68e10fb1cb9d622f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83060227b784beb916f70f67923eb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.520\n",
      "Training Loss: 92492242.1382 \tValidation Loss: 21004649.2261\n",
      "\n",
      "Running epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b092c505c9c94b53b39fa66999a6a1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e01c46fcb7d44428ee47a9eed3e58d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf08bb5fbec24a429717258f3876df3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.030\n",
      "Training Loss: 92249397.7924 \tValidation Loss: 9893543.4058\n",
      "\n",
      "Running epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7164dbcf53543fdb1d8f44cb4d96358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dd4447529e4e5a92006ede5cc85653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc2ff20ecd145418903a7b5443835a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.590\n",
      "Training Loss: 92179946.3534 \tValidation Loss: 268162.3348\n",
      "\n",
      "Running epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0288b2fa80b14e8aa76671e24238994e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2f2873ce7c4cac87d66a2b73130fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed20686824b4506921c5b344b185cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.590\n",
      "Training Loss: 92170189.9814 \tValidation Loss: 6340953.5388\n",
      "\n",
      "Running epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf17eba5532f474b8db80bf220610647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4632b1df3444a9834e815f2d8e649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.520\n",
      "Training Loss: 92176111.5649 \tValidation Loss: 231562673.5352\n",
      "\n",
      "Running epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0aa2c74cfa4c7199c468d05ed973c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899f638fb5d04283ba677b7bac089007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c2166ddc01459e81a46e0c89c5e924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.590\n",
      "Training Loss: 92172556.0096 \tValidation Loss: 137952981.7578\n",
      "\n",
      "Running epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac8a685fdcc4c4eb3876efe15257c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7043ad5d446454dbbcff46cdbdb8638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.590\n",
      "Training Loss: 92162558.6162 \tValidation Loss: 47651212.1826\n",
      "\n",
      "Running epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0551fa1529a54d6894cd5ff0b882ccd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20740/1253722838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mtop_accuracy_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOP_ACCURACY_TRACK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0maccwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                    )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20740/1740864069.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(max_epoch, top_accuracy_track, grandstore, minepoch, epochwindow, accwindow)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mtotaltrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 5\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=120,\n",
    "                    minepoch=30,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                  \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4013c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Run 44 epoch(s)\n",
      "Accuracy MIN: 9.6 / MAX: 48.63\n",
      "\n",
      "Top 5 performing epochs:\n",
      "#1 epoch 1\t||train_acc 48.63%\t||test 48.41%\n",
      "#2 epoch 25\t||train_acc 10.3%\t||test 10.0%\n",
      "#3 epoch 24\t||train_acc 10.3%\t||test 10.0%\n",
      "#4 epoch 21\t||train_acc 10.3%\t||test 10.0%\n",
      "#5 epoch 20\t||train_acc 10.3%\t||test 10.0%\n",
      "\n",
      "Last epoch:\n",
      "epoch 44\t||train_acc 9.6%\t||test 10.0%\n",
      "\n",
      "The model has parameters: 13390324\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {}%\\t||test {}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {}%\\t||test {}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writings done!\n",
      "Files at: grandstore/cifar10_DenseNet121_sp20211030010454.pkl\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "\n",
    "print(\"writings done!\")\n",
    "print(\"Files at: \"+grandstore_string)\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ddef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
