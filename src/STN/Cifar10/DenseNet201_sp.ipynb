{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db038553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DenseNet201_sp with 10 classes running on: cifar10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import os\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "#vital params\n",
    "\n",
    " \n",
    "model_name=\"DenseNet201_sp\"\n",
    "\n",
    "dataset_name=\"cifar10\"\n",
    "\n",
    "#hyperparameters\n",
    "batch_size=20\n",
    "num_classes=-1\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "image_size=(224,224)\n",
    "\n",
    "\n",
    "if dataset_name == \"tsrd\":\n",
    "    num_classes=58\n",
    "elif dataset_name == \"cifar10\":\n",
    "    num_classes=10\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Model: \"+model_name +\" with {} classes\".format(num_classes)+\n",
    "      \" running on: \"+dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38958e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset size: Train: 40000, Valid: 10000, Test: 10000\n",
      "torch.Size([3, 224, 224])\n",
      "Datasets loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "# load data through imagefolder\n",
    "if dataset_name == \"tsrd\":\n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    train_dir = \"../../dataset/data\"\n",
    "    head_train_set = dset.ImageFolder(train_dir,transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(head_train_set, [5000, 998])\n",
    "    train_set, test_set = data.random_split(train_set,[4000, 1000])\n",
    "\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    val_dataloader=torch.utils.data.DataLoader(valid_set,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_set,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=True)\n",
    "elif dataset_name == \"cifar10\":\n",
    "    \n",
    "    main_transforms=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5] , std = [0.5, 0.5, 0.5]),\n",
    "\n",
    "    ])\n",
    "\n",
    "    bigtrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=main_transforms)\n",
    "    train_set, valid_set = data.random_split(bigtrain_set, [40000, 10000])\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=main_transforms)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(valid_set, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset size: Train: {}, Valid: {}, Test: {}\"\n",
    "      .format(len(train_set),len(valid_set),len(test_set)))\n",
    "\n",
    "print(train_set[0][0].shape)\n",
    "print(\"Datasets loaded and prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd38adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "#from .utils import load_state_dict_from_url\n",
    "\n",
    "\n",
    "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
    "\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)  \n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features] \n",
    "        for name, layer in self.named_children():   \n",
    "            new_features = layer(*features) \n",
    "            features.append(new_features)  \n",
    "        return torch.cat(features, 1)   \n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=num_classes, memory_efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,  \n",
    "                num_input_features=num_features,    \n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,    \n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)  \n",
    "            num_features = num_features + num_layers * growth_rate \n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)  \n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2   \n",
    "\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(27040, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        \n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 52 * 52)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.stn(x)\n",
    "        features = self.features(x) \n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))   \n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out) \n",
    "        return out\n",
    "\n",
    "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
    "              **kwargs):\n",
    "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
    "    if pretrained:\n",
    "        _load_state_dict(model, model_urls[arch], progress)\n",
    "    return model\n",
    "\n",
    "\n",
    "def densenet121(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-121 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet161(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-161 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet161', 32, (6, 12, 32, 32), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet169(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-169 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet169', 32, (6, 12, 48, 32), 64, pretrained, progress,\n",
    "                     **kwargs)\n",
    "\n",
    "\n",
    "def densenet201(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-201 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet201', 32, (6, 12, 64, 48), 64, pretrained, progress,\n",
    "                     **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe94e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:4044: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "C:\\Users\\2080Ti\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3982: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "model shape ready\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 218, 218]           1,184\n",
      "         MaxPool2d-2          [-1, 8, 109, 109]               0\n",
      "              ReLU-3          [-1, 8, 109, 109]               0\n",
      "            Conv2d-4         [-1, 10, 105, 105]           2,010\n",
      "         MaxPool2d-5           [-1, 10, 52, 52]               0\n",
      "              ReLU-6           [-1, 10, 52, 52]               0\n",
      "            Linear-7                   [-1, 32]         865,312\n",
      "              ReLU-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 6]             198\n",
      "           Conv2d-10         [-1, 64, 112, 112]           9,408\n",
      "      BatchNorm2d-11         [-1, 64, 112, 112]             128\n",
      "             ReLU-12         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-13           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-14           [-1, 64, 56, 56]             128\n",
      "             ReLU-15           [-1, 64, 56, 56]               0\n",
      "           Conv2d-16          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
      "             ReLU-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 96, 56, 56]             192\n",
      "             ReLU-21           [-1, 96, 56, 56]               0\n",
      "           Conv2d-22          [-1, 128, 56, 56]          12,288\n",
      "      BatchNorm2d-23          [-1, 128, 56, 56]             256\n",
      "             ReLU-24          [-1, 128, 56, 56]               0\n",
      "           Conv2d-25           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-26          [-1, 128, 56, 56]             256\n",
      "             ReLU-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28          [-1, 128, 56, 56]          16,384\n",
      "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
      "             ReLU-30          [-1, 128, 56, 56]               0\n",
      "           Conv2d-31           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-32          [-1, 160, 56, 56]             320\n",
      "             ReLU-33          [-1, 160, 56, 56]               0\n",
      "           Conv2d-34          [-1, 128, 56, 56]          20,480\n",
      "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
      "             ReLU-36          [-1, 128, 56, 56]               0\n",
      "           Conv2d-37           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-38          [-1, 192, 56, 56]             384\n",
      "             ReLU-39          [-1, 192, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 56, 56]          24,576\n",
      "      BatchNorm2d-41          [-1, 128, 56, 56]             256\n",
      "             ReLU-42          [-1, 128, 56, 56]               0\n",
      "           Conv2d-43           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-44          [-1, 224, 56, 56]             448\n",
      "             ReLU-45          [-1, 224, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 56, 56]          28,672\n",
      "      BatchNorm2d-47          [-1, 128, 56, 56]             256\n",
      "             ReLU-48          [-1, 128, 56, 56]               0\n",
      "           Conv2d-49           [-1, 32, 56, 56]          36,864\n",
      "      _DenseBlock-50          [-1, 256, 56, 56]               0\n",
      "      BatchNorm2d-51          [-1, 256, 56, 56]             512\n",
      "             ReLU-52          [-1, 256, 56, 56]               0\n",
      "           Conv2d-53          [-1, 128, 56, 56]          32,768\n",
      "        AvgPool2d-54          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
      "             ReLU-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-61          [-1, 160, 28, 28]             320\n",
      "             ReLU-62          [-1, 160, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]          20,480\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-67          [-1, 192, 28, 28]             384\n",
      "             ReLU-68          [-1, 192, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          24,576\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-73          [-1, 224, 28, 28]             448\n",
      "             ReLU-74          [-1, 224, 28, 28]               0\n",
      "           Conv2d-75          [-1, 128, 28, 28]          28,672\n",
      "      BatchNorm2d-76          [-1, 128, 28, 28]             256\n",
      "             ReLU-77          [-1, 128, 28, 28]               0\n",
      "           Conv2d-78           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-79          [-1, 256, 28, 28]             512\n",
      "             ReLU-80          [-1, 256, 28, 28]               0\n",
      "           Conv2d-81          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
      "             ReLU-83          [-1, 128, 28, 28]               0\n",
      "           Conv2d-84           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-85          [-1, 288, 28, 28]             576\n",
      "             ReLU-86          [-1, 288, 28, 28]               0\n",
      "           Conv2d-87          [-1, 128, 28, 28]          36,864\n",
      "      BatchNorm2d-88          [-1, 128, 28, 28]             256\n",
      "             ReLU-89          [-1, 128, 28, 28]               0\n",
      "           Conv2d-90           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-91          [-1, 320, 28, 28]             640\n",
      "             ReLU-92          [-1, 320, 28, 28]               0\n",
      "           Conv2d-93          [-1, 128, 28, 28]          40,960\n",
      "      BatchNorm2d-94          [-1, 128, 28, 28]             256\n",
      "             ReLU-95          [-1, 128, 28, 28]               0\n",
      "           Conv2d-96           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-97          [-1, 352, 28, 28]             704\n",
      "             ReLU-98          [-1, 352, 28, 28]               0\n",
      "           Conv2d-99          [-1, 128, 28, 28]          45,056\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-103          [-1, 384, 28, 28]             768\n",
      "            ReLU-104          [-1, 384, 28, 28]               0\n",
      "          Conv2d-105          [-1, 128, 28, 28]          49,152\n",
      "     BatchNorm2d-106          [-1, 128, 28, 28]             256\n",
      "            ReLU-107          [-1, 128, 28, 28]               0\n",
      "          Conv2d-108           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-109          [-1, 416, 28, 28]             832\n",
      "            ReLU-110          [-1, 416, 28, 28]               0\n",
      "          Conv2d-111          [-1, 128, 28, 28]          53,248\n",
      "     BatchNorm2d-112          [-1, 128, 28, 28]             256\n",
      "            ReLU-113          [-1, 128, 28, 28]               0\n",
      "          Conv2d-114           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-115          [-1, 448, 28, 28]             896\n",
      "            ReLU-116          [-1, 448, 28, 28]               0\n",
      "          Conv2d-117          [-1, 128, 28, 28]          57,344\n",
      "     BatchNorm2d-118          [-1, 128, 28, 28]             256\n",
      "            ReLU-119          [-1, 128, 28, 28]               0\n",
      "          Conv2d-120           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-121          [-1, 480, 28, 28]             960\n",
      "            ReLU-122          [-1, 480, 28, 28]               0\n",
      "          Conv2d-123          [-1, 128, 28, 28]          61,440\n",
      "     BatchNorm2d-124          [-1, 128, 28, 28]             256\n",
      "            ReLU-125          [-1, 128, 28, 28]               0\n",
      "          Conv2d-126           [-1, 32, 28, 28]          36,864\n",
      "     _DenseBlock-127          [-1, 512, 28, 28]               0\n",
      "     BatchNorm2d-128          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-129          [-1, 512, 28, 28]               0\n",
      "          Conv2d-130          [-1, 256, 28, 28]         131,072\n",
      "       AvgPool2d-131          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-135          [-1, 128, 14, 14]             256\n",
      "            ReLU-136          [-1, 128, 14, 14]               0\n",
      "          Conv2d-137           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-138          [-1, 288, 14, 14]             576\n",
      "            ReLU-139          [-1, 288, 14, 14]               0\n",
      "          Conv2d-140          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-141          [-1, 128, 14, 14]             256\n",
      "            ReLU-142          [-1, 128, 14, 14]               0\n",
      "          Conv2d-143           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-144          [-1, 320, 14, 14]             640\n",
      "            ReLU-145          [-1, 320, 14, 14]               0\n",
      "          Conv2d-146          [-1, 128, 14, 14]          40,960\n",
      "     BatchNorm2d-147          [-1, 128, 14, 14]             256\n",
      "            ReLU-148          [-1, 128, 14, 14]               0\n",
      "          Conv2d-149           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-150          [-1, 352, 14, 14]             704\n",
      "            ReLU-151          [-1, 352, 14, 14]               0\n",
      "          Conv2d-152          [-1, 128, 14, 14]          45,056\n",
      "     BatchNorm2d-153          [-1, 128, 14, 14]             256\n",
      "            ReLU-154          [-1, 128, 14, 14]               0\n",
      "          Conv2d-155           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-156          [-1, 384, 14, 14]             768\n",
      "            ReLU-157          [-1, 384, 14, 14]               0\n",
      "          Conv2d-158          [-1, 128, 14, 14]          49,152\n",
      "     BatchNorm2d-159          [-1, 128, 14, 14]             256\n",
      "            ReLU-160          [-1, 128, 14, 14]               0\n",
      "          Conv2d-161           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-162          [-1, 416, 14, 14]             832\n",
      "            ReLU-163          [-1, 416, 14, 14]               0\n",
      "          Conv2d-164          [-1, 128, 14, 14]          53,248\n",
      "     BatchNorm2d-165          [-1, 128, 14, 14]             256\n",
      "            ReLU-166          [-1, 128, 14, 14]               0\n",
      "          Conv2d-167           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-168          [-1, 448, 14, 14]             896\n",
      "            ReLU-169          [-1, 448, 14, 14]               0\n",
      "          Conv2d-170          [-1, 128, 14, 14]          57,344\n",
      "     BatchNorm2d-171          [-1, 128, 14, 14]             256\n",
      "            ReLU-172          [-1, 128, 14, 14]               0\n",
      "          Conv2d-173           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-174          [-1, 480, 14, 14]             960\n",
      "            ReLU-175          [-1, 480, 14, 14]               0\n",
      "          Conv2d-176          [-1, 128, 14, 14]          61,440\n",
      "     BatchNorm2d-177          [-1, 128, 14, 14]             256\n",
      "            ReLU-178          [-1, 128, 14, 14]               0\n",
      "          Conv2d-179           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-180          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-181          [-1, 512, 14, 14]               0\n",
      "          Conv2d-182          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-183          [-1, 128, 14, 14]             256\n",
      "            ReLU-184          [-1, 128, 14, 14]               0\n",
      "          Conv2d-185           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-186          [-1, 544, 14, 14]           1,088\n",
      "            ReLU-187          [-1, 544, 14, 14]               0\n",
      "          Conv2d-188          [-1, 128, 14, 14]          69,632\n",
      "     BatchNorm2d-189          [-1, 128, 14, 14]             256\n",
      "            ReLU-190          [-1, 128, 14, 14]               0\n",
      "          Conv2d-191           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-192          [-1, 576, 14, 14]           1,152\n",
      "            ReLU-193          [-1, 576, 14, 14]               0\n",
      "          Conv2d-194          [-1, 128, 14, 14]          73,728\n",
      "     BatchNorm2d-195          [-1, 128, 14, 14]             256\n",
      "            ReLU-196          [-1, 128, 14, 14]               0\n",
      "          Conv2d-197           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-198          [-1, 608, 14, 14]           1,216\n",
      "            ReLU-199          [-1, 608, 14, 14]               0\n",
      "          Conv2d-200          [-1, 128, 14, 14]          77,824\n",
      "     BatchNorm2d-201          [-1, 128, 14, 14]             256\n",
      "            ReLU-202          [-1, 128, 14, 14]               0\n",
      "          Conv2d-203           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-204          [-1, 640, 14, 14]           1,280\n",
      "            ReLU-205          [-1, 640, 14, 14]               0\n",
      "          Conv2d-206          [-1, 128, 14, 14]          81,920\n",
      "     BatchNorm2d-207          [-1, 128, 14, 14]             256\n",
      "            ReLU-208          [-1, 128, 14, 14]               0\n",
      "          Conv2d-209           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-210          [-1, 672, 14, 14]           1,344\n",
      "            ReLU-211          [-1, 672, 14, 14]               0\n",
      "          Conv2d-212          [-1, 128, 14, 14]          86,016\n",
      "     BatchNorm2d-213          [-1, 128, 14, 14]             256\n",
      "            ReLU-214          [-1, 128, 14, 14]               0\n",
      "          Conv2d-215           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-216          [-1, 704, 14, 14]           1,408\n",
      "            ReLU-217          [-1, 704, 14, 14]               0\n",
      "          Conv2d-218          [-1, 128, 14, 14]          90,112\n",
      "     BatchNorm2d-219          [-1, 128, 14, 14]             256\n",
      "            ReLU-220          [-1, 128, 14, 14]               0\n",
      "          Conv2d-221           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-222          [-1, 736, 14, 14]           1,472\n",
      "            ReLU-223          [-1, 736, 14, 14]               0\n",
      "          Conv2d-224          [-1, 128, 14, 14]          94,208\n",
      "     BatchNorm2d-225          [-1, 128, 14, 14]             256\n",
      "            ReLU-226          [-1, 128, 14, 14]               0\n",
      "          Conv2d-227           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-228          [-1, 768, 14, 14]           1,536\n",
      "            ReLU-229          [-1, 768, 14, 14]               0\n",
      "          Conv2d-230          [-1, 128, 14, 14]          98,304\n",
      "     BatchNorm2d-231          [-1, 128, 14, 14]             256\n",
      "            ReLU-232          [-1, 128, 14, 14]               0\n",
      "          Conv2d-233           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-234          [-1, 800, 14, 14]           1,600\n",
      "            ReLU-235          [-1, 800, 14, 14]               0\n",
      "          Conv2d-236          [-1, 128, 14, 14]         102,400\n",
      "     BatchNorm2d-237          [-1, 128, 14, 14]             256\n",
      "            ReLU-238          [-1, 128, 14, 14]               0\n",
      "          Conv2d-239           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-240          [-1, 832, 14, 14]           1,664\n",
      "            ReLU-241          [-1, 832, 14, 14]               0\n",
      "          Conv2d-242          [-1, 128, 14, 14]         106,496\n",
      "     BatchNorm2d-243          [-1, 128, 14, 14]             256\n",
      "            ReLU-244          [-1, 128, 14, 14]               0\n",
      "          Conv2d-245           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-246          [-1, 864, 14, 14]           1,728\n",
      "            ReLU-247          [-1, 864, 14, 14]               0\n",
      "          Conv2d-248          [-1, 128, 14, 14]         110,592\n",
      "     BatchNorm2d-249          [-1, 128, 14, 14]             256\n",
      "            ReLU-250          [-1, 128, 14, 14]               0\n",
      "          Conv2d-251           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-252          [-1, 896, 14, 14]           1,792\n",
      "            ReLU-253          [-1, 896, 14, 14]               0\n",
      "          Conv2d-254          [-1, 128, 14, 14]         114,688\n",
      "     BatchNorm2d-255          [-1, 128, 14, 14]             256\n",
      "            ReLU-256          [-1, 128, 14, 14]               0\n",
      "          Conv2d-257           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-258          [-1, 928, 14, 14]           1,856\n",
      "            ReLU-259          [-1, 928, 14, 14]               0\n",
      "          Conv2d-260          [-1, 128, 14, 14]         118,784\n",
      "     BatchNorm2d-261          [-1, 128, 14, 14]             256\n",
      "            ReLU-262          [-1, 128, 14, 14]               0\n",
      "          Conv2d-263           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-264          [-1, 960, 14, 14]           1,920\n",
      "            ReLU-265          [-1, 960, 14, 14]               0\n",
      "          Conv2d-266          [-1, 128, 14, 14]         122,880\n",
      "     BatchNorm2d-267          [-1, 128, 14, 14]             256\n",
      "            ReLU-268          [-1, 128, 14, 14]               0\n",
      "          Conv2d-269           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-270          [-1, 992, 14, 14]           1,984\n",
      "            ReLU-271          [-1, 992, 14, 14]               0\n",
      "          Conv2d-272          [-1, 128, 14, 14]         126,976\n",
      "     BatchNorm2d-273          [-1, 128, 14, 14]             256\n",
      "            ReLU-274          [-1, 128, 14, 14]               0\n",
      "          Conv2d-275           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-276         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-277         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-278          [-1, 128, 14, 14]         131,072\n",
      "     BatchNorm2d-279          [-1, 128, 14, 14]             256\n",
      "            ReLU-280          [-1, 128, 14, 14]               0\n",
      "          Conv2d-281           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-282         [-1, 1056, 14, 14]           2,112\n",
      "            ReLU-283         [-1, 1056, 14, 14]               0\n",
      "          Conv2d-284          [-1, 128, 14, 14]         135,168\n",
      "     BatchNorm2d-285          [-1, 128, 14, 14]             256\n",
      "            ReLU-286          [-1, 128, 14, 14]               0\n",
      "          Conv2d-287           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-288         [-1, 1088, 14, 14]           2,176\n",
      "            ReLU-289         [-1, 1088, 14, 14]               0\n",
      "          Conv2d-290          [-1, 128, 14, 14]         139,264\n",
      "     BatchNorm2d-291          [-1, 128, 14, 14]             256\n",
      "            ReLU-292          [-1, 128, 14, 14]               0\n",
      "          Conv2d-293           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-294         [-1, 1120, 14, 14]           2,240\n",
      "            ReLU-295         [-1, 1120, 14, 14]               0\n",
      "          Conv2d-296          [-1, 128, 14, 14]         143,360\n",
      "     BatchNorm2d-297          [-1, 128, 14, 14]             256\n",
      "            ReLU-298          [-1, 128, 14, 14]               0\n",
      "          Conv2d-299           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-300         [-1, 1152, 14, 14]           2,304\n",
      "            ReLU-301         [-1, 1152, 14, 14]               0\n",
      "          Conv2d-302          [-1, 128, 14, 14]         147,456\n",
      "     BatchNorm2d-303          [-1, 128, 14, 14]             256\n",
      "            ReLU-304          [-1, 128, 14, 14]               0\n",
      "          Conv2d-305           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-306         [-1, 1184, 14, 14]           2,368\n",
      "            ReLU-307         [-1, 1184, 14, 14]               0\n",
      "          Conv2d-308          [-1, 128, 14, 14]         151,552\n",
      "     BatchNorm2d-309          [-1, 128, 14, 14]             256\n",
      "            ReLU-310          [-1, 128, 14, 14]               0\n",
      "          Conv2d-311           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-312         [-1, 1216, 14, 14]           2,432\n",
      "            ReLU-313         [-1, 1216, 14, 14]               0\n",
      "          Conv2d-314          [-1, 128, 14, 14]         155,648\n",
      "     BatchNorm2d-315          [-1, 128, 14, 14]             256\n",
      "            ReLU-316          [-1, 128, 14, 14]               0\n",
      "          Conv2d-317           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-318         [-1, 1248, 14, 14]           2,496\n",
      "            ReLU-319         [-1, 1248, 14, 14]               0\n",
      "          Conv2d-320          [-1, 128, 14, 14]         159,744\n",
      "     BatchNorm2d-321          [-1, 128, 14, 14]             256\n",
      "            ReLU-322          [-1, 128, 14, 14]               0\n",
      "          Conv2d-323           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-324         [-1, 1280, 14, 14]           2,560\n",
      "            ReLU-325         [-1, 1280, 14, 14]               0\n",
      "          Conv2d-326          [-1, 128, 14, 14]         163,840\n",
      "     BatchNorm2d-327          [-1, 128, 14, 14]             256\n",
      "            ReLU-328          [-1, 128, 14, 14]               0\n",
      "          Conv2d-329           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-330         [-1, 1312, 14, 14]           2,624\n",
      "            ReLU-331         [-1, 1312, 14, 14]               0\n",
      "          Conv2d-332          [-1, 128, 14, 14]         167,936\n",
      "     BatchNorm2d-333          [-1, 128, 14, 14]             256\n",
      "            ReLU-334          [-1, 128, 14, 14]               0\n",
      "          Conv2d-335           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-336         [-1, 1344, 14, 14]           2,688\n",
      "            ReLU-337         [-1, 1344, 14, 14]               0\n",
      "          Conv2d-338          [-1, 128, 14, 14]         172,032\n",
      "     BatchNorm2d-339          [-1, 128, 14, 14]             256\n",
      "            ReLU-340          [-1, 128, 14, 14]               0\n",
      "          Conv2d-341           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-342         [-1, 1376, 14, 14]           2,752\n",
      "            ReLU-343         [-1, 1376, 14, 14]               0\n",
      "          Conv2d-344          [-1, 128, 14, 14]         176,128\n",
      "     BatchNorm2d-345          [-1, 128, 14, 14]             256\n",
      "            ReLU-346          [-1, 128, 14, 14]               0\n",
      "          Conv2d-347           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-348         [-1, 1408, 14, 14]           2,816\n",
      "            ReLU-349         [-1, 1408, 14, 14]               0\n",
      "          Conv2d-350          [-1, 128, 14, 14]         180,224\n",
      "     BatchNorm2d-351          [-1, 128, 14, 14]             256\n",
      "            ReLU-352          [-1, 128, 14, 14]               0\n",
      "          Conv2d-353           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-354         [-1, 1440, 14, 14]           2,880\n",
      "            ReLU-355         [-1, 1440, 14, 14]               0\n",
      "          Conv2d-356          [-1, 128, 14, 14]         184,320\n",
      "     BatchNorm2d-357          [-1, 128, 14, 14]             256\n",
      "            ReLU-358          [-1, 128, 14, 14]               0\n",
      "          Conv2d-359           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-360         [-1, 1472, 14, 14]           2,944\n",
      "            ReLU-361         [-1, 1472, 14, 14]               0\n",
      "          Conv2d-362          [-1, 128, 14, 14]         188,416\n",
      "     BatchNorm2d-363          [-1, 128, 14, 14]             256\n",
      "            ReLU-364          [-1, 128, 14, 14]               0\n",
      "          Conv2d-365           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-366         [-1, 1504, 14, 14]           3,008\n",
      "            ReLU-367         [-1, 1504, 14, 14]               0\n",
      "          Conv2d-368          [-1, 128, 14, 14]         192,512\n",
      "     BatchNorm2d-369          [-1, 128, 14, 14]             256\n",
      "            ReLU-370          [-1, 128, 14, 14]               0\n",
      "          Conv2d-371           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-372         [-1, 1536, 14, 14]           3,072\n",
      "            ReLU-373         [-1, 1536, 14, 14]               0\n",
      "          Conv2d-374          [-1, 128, 14, 14]         196,608\n",
      "     BatchNorm2d-375          [-1, 128, 14, 14]             256\n",
      "            ReLU-376          [-1, 128, 14, 14]               0\n",
      "          Conv2d-377           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-378         [-1, 1568, 14, 14]           3,136\n",
      "            ReLU-379         [-1, 1568, 14, 14]               0\n",
      "          Conv2d-380          [-1, 128, 14, 14]         200,704\n",
      "     BatchNorm2d-381          [-1, 128, 14, 14]             256\n",
      "            ReLU-382          [-1, 128, 14, 14]               0\n",
      "          Conv2d-383           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-384         [-1, 1600, 14, 14]           3,200\n",
      "            ReLU-385         [-1, 1600, 14, 14]               0\n",
      "          Conv2d-386          [-1, 128, 14, 14]         204,800\n",
      "     BatchNorm2d-387          [-1, 128, 14, 14]             256\n",
      "            ReLU-388          [-1, 128, 14, 14]               0\n",
      "          Conv2d-389           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-390         [-1, 1632, 14, 14]           3,264\n",
      "            ReLU-391         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-392          [-1, 128, 14, 14]         208,896\n",
      "     BatchNorm2d-393          [-1, 128, 14, 14]             256\n",
      "            ReLU-394          [-1, 128, 14, 14]               0\n",
      "          Conv2d-395           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-396         [-1, 1664, 14, 14]           3,328\n",
      "            ReLU-397         [-1, 1664, 14, 14]               0\n",
      "          Conv2d-398          [-1, 128, 14, 14]         212,992\n",
      "     BatchNorm2d-399          [-1, 128, 14, 14]             256\n",
      "            ReLU-400          [-1, 128, 14, 14]               0\n",
      "          Conv2d-401           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-402         [-1, 1696, 14, 14]           3,392\n",
      "            ReLU-403         [-1, 1696, 14, 14]               0\n",
      "          Conv2d-404          [-1, 128, 14, 14]         217,088\n",
      "     BatchNorm2d-405          [-1, 128, 14, 14]             256\n",
      "            ReLU-406          [-1, 128, 14, 14]               0\n",
      "          Conv2d-407           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-408         [-1, 1728, 14, 14]           3,456\n",
      "            ReLU-409         [-1, 1728, 14, 14]               0\n",
      "          Conv2d-410          [-1, 128, 14, 14]         221,184\n",
      "     BatchNorm2d-411          [-1, 128, 14, 14]             256\n",
      "            ReLU-412          [-1, 128, 14, 14]               0\n",
      "          Conv2d-413           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-414         [-1, 1760, 14, 14]           3,520\n",
      "            ReLU-415         [-1, 1760, 14, 14]               0\n",
      "          Conv2d-416          [-1, 128, 14, 14]         225,280\n",
      "     BatchNorm2d-417          [-1, 128, 14, 14]             256\n",
      "            ReLU-418          [-1, 128, 14, 14]               0\n",
      "          Conv2d-419           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-420         [-1, 1792, 14, 14]           3,584\n",
      "            ReLU-421         [-1, 1792, 14, 14]               0\n",
      "          Conv2d-422          [-1, 128, 14, 14]         229,376\n",
      "     BatchNorm2d-423          [-1, 128, 14, 14]             256\n",
      "            ReLU-424          [-1, 128, 14, 14]               0\n",
      "          Conv2d-425           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-426         [-1, 1824, 14, 14]           3,648\n",
      "            ReLU-427         [-1, 1824, 14, 14]               0\n",
      "          Conv2d-428          [-1, 128, 14, 14]         233,472\n",
      "     BatchNorm2d-429          [-1, 128, 14, 14]             256\n",
      "            ReLU-430          [-1, 128, 14, 14]               0\n",
      "          Conv2d-431           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-432         [-1, 1856, 14, 14]           3,712\n",
      "            ReLU-433         [-1, 1856, 14, 14]               0\n",
      "          Conv2d-434          [-1, 128, 14, 14]         237,568\n",
      "     BatchNorm2d-435          [-1, 128, 14, 14]             256\n",
      "            ReLU-436          [-1, 128, 14, 14]               0\n",
      "          Conv2d-437           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-438         [-1, 1888, 14, 14]           3,776\n",
      "            ReLU-439         [-1, 1888, 14, 14]               0\n",
      "          Conv2d-440          [-1, 128, 14, 14]         241,664\n",
      "     BatchNorm2d-441          [-1, 128, 14, 14]             256\n",
      "            ReLU-442          [-1, 128, 14, 14]               0\n",
      "          Conv2d-443           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-444         [-1, 1920, 14, 14]           3,840\n",
      "            ReLU-445         [-1, 1920, 14, 14]               0\n",
      "          Conv2d-446          [-1, 128, 14, 14]         245,760\n",
      "     BatchNorm2d-447          [-1, 128, 14, 14]             256\n",
      "            ReLU-448          [-1, 128, 14, 14]               0\n",
      "          Conv2d-449           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-450         [-1, 1952, 14, 14]           3,904\n",
      "            ReLU-451         [-1, 1952, 14, 14]               0\n",
      "          Conv2d-452          [-1, 128, 14, 14]         249,856\n",
      "     BatchNorm2d-453          [-1, 128, 14, 14]             256\n",
      "            ReLU-454          [-1, 128, 14, 14]               0\n",
      "          Conv2d-455           [-1, 32, 14, 14]          36,864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-456         [-1, 1984, 14, 14]           3,968\n",
      "            ReLU-457         [-1, 1984, 14, 14]               0\n",
      "          Conv2d-458          [-1, 128, 14, 14]         253,952\n",
      "     BatchNorm2d-459          [-1, 128, 14, 14]             256\n",
      "            ReLU-460          [-1, 128, 14, 14]               0\n",
      "          Conv2d-461           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-462         [-1, 2016, 14, 14]           4,032\n",
      "            ReLU-463         [-1, 2016, 14, 14]               0\n",
      "          Conv2d-464          [-1, 128, 14, 14]         258,048\n",
      "     BatchNorm2d-465          [-1, 128, 14, 14]             256\n",
      "            ReLU-466          [-1, 128, 14, 14]               0\n",
      "          Conv2d-467           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-468         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-469         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-470          [-1, 128, 14, 14]         262,144\n",
      "     BatchNorm2d-471          [-1, 128, 14, 14]             256\n",
      "            ReLU-472          [-1, 128, 14, 14]               0\n",
      "          Conv2d-473           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-474         [-1, 2080, 14, 14]           4,160\n",
      "            ReLU-475         [-1, 2080, 14, 14]               0\n",
      "          Conv2d-476          [-1, 128, 14, 14]         266,240\n",
      "     BatchNorm2d-477          [-1, 128, 14, 14]             256\n",
      "            ReLU-478          [-1, 128, 14, 14]               0\n",
      "          Conv2d-479           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-480         [-1, 2112, 14, 14]           4,224\n",
      "            ReLU-481         [-1, 2112, 14, 14]               0\n",
      "          Conv2d-482          [-1, 128, 14, 14]         270,336\n",
      "     BatchNorm2d-483          [-1, 128, 14, 14]             256\n",
      "            ReLU-484          [-1, 128, 14, 14]               0\n",
      "          Conv2d-485           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-486         [-1, 2144, 14, 14]           4,288\n",
      "            ReLU-487         [-1, 2144, 14, 14]               0\n",
      "          Conv2d-488          [-1, 128, 14, 14]         274,432\n",
      "     BatchNorm2d-489          [-1, 128, 14, 14]             256\n",
      "            ReLU-490          [-1, 128, 14, 14]               0\n",
      "          Conv2d-491           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-492         [-1, 2176, 14, 14]           4,352\n",
      "            ReLU-493         [-1, 2176, 14, 14]               0\n",
      "          Conv2d-494          [-1, 128, 14, 14]         278,528\n",
      "     BatchNorm2d-495          [-1, 128, 14, 14]             256\n",
      "            ReLU-496          [-1, 128, 14, 14]               0\n",
      "          Conv2d-497           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-498         [-1, 2208, 14, 14]           4,416\n",
      "            ReLU-499         [-1, 2208, 14, 14]               0\n",
      "          Conv2d-500          [-1, 128, 14, 14]         282,624\n",
      "     BatchNorm2d-501          [-1, 128, 14, 14]             256\n",
      "            ReLU-502          [-1, 128, 14, 14]               0\n",
      "          Conv2d-503           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-504         [-1, 2240, 14, 14]           4,480\n",
      "            ReLU-505         [-1, 2240, 14, 14]               0\n",
      "          Conv2d-506          [-1, 128, 14, 14]         286,720\n",
      "     BatchNorm2d-507          [-1, 128, 14, 14]             256\n",
      "            ReLU-508          [-1, 128, 14, 14]               0\n",
      "          Conv2d-509           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-510         [-1, 2272, 14, 14]           4,544\n",
      "            ReLU-511         [-1, 2272, 14, 14]               0\n",
      "          Conv2d-512          [-1, 128, 14, 14]         290,816\n",
      "     BatchNorm2d-513          [-1, 128, 14, 14]             256\n",
      "            ReLU-514          [-1, 128, 14, 14]               0\n",
      "          Conv2d-515           [-1, 32, 14, 14]          36,864\n",
      "     _DenseBlock-516         [-1, 2304, 14, 14]               0\n",
      "     BatchNorm2d-517         [-1, 2304, 14, 14]           4,608\n",
      "            ReLU-518         [-1, 2304, 14, 14]               0\n",
      "          Conv2d-519         [-1, 1152, 14, 14]       2,654,208\n",
      "       AvgPool2d-520           [-1, 1152, 7, 7]               0\n",
      "     BatchNorm2d-521           [-1, 1152, 7, 7]           2,304\n",
      "            ReLU-522           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-523            [-1, 128, 7, 7]         147,456\n",
      "     BatchNorm2d-524            [-1, 128, 7, 7]             256\n",
      "            ReLU-525            [-1, 128, 7, 7]               0\n",
      "          Conv2d-526             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-527           [-1, 1184, 7, 7]           2,368\n",
      "            ReLU-528           [-1, 1184, 7, 7]               0\n",
      "          Conv2d-529            [-1, 128, 7, 7]         151,552\n",
      "     BatchNorm2d-530            [-1, 128, 7, 7]             256\n",
      "            ReLU-531            [-1, 128, 7, 7]               0\n",
      "          Conv2d-532             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-533           [-1, 1216, 7, 7]           2,432\n",
      "            ReLU-534           [-1, 1216, 7, 7]               0\n",
      "          Conv2d-535            [-1, 128, 7, 7]         155,648\n",
      "     BatchNorm2d-536            [-1, 128, 7, 7]             256\n",
      "            ReLU-537            [-1, 128, 7, 7]               0\n",
      "          Conv2d-538             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-539           [-1, 1248, 7, 7]           2,496\n",
      "            ReLU-540           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-541            [-1, 128, 7, 7]         159,744\n",
      "     BatchNorm2d-542            [-1, 128, 7, 7]             256\n",
      "            ReLU-543            [-1, 128, 7, 7]               0\n",
      "          Conv2d-544             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-545           [-1, 1280, 7, 7]           2,560\n",
      "            ReLU-546           [-1, 1280, 7, 7]               0\n",
      "          Conv2d-547            [-1, 128, 7, 7]         163,840\n",
      "     BatchNorm2d-548            [-1, 128, 7, 7]             256\n",
      "            ReLU-549            [-1, 128, 7, 7]               0\n",
      "          Conv2d-550             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-551           [-1, 1312, 7, 7]           2,624\n",
      "            ReLU-552           [-1, 1312, 7, 7]               0\n",
      "          Conv2d-553            [-1, 128, 7, 7]         167,936\n",
      "     BatchNorm2d-554            [-1, 128, 7, 7]             256\n",
      "            ReLU-555            [-1, 128, 7, 7]               0\n",
      "          Conv2d-556             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-557           [-1, 1344, 7, 7]           2,688\n",
      "            ReLU-558           [-1, 1344, 7, 7]               0\n",
      "          Conv2d-559            [-1, 128, 7, 7]         172,032\n",
      "     BatchNorm2d-560            [-1, 128, 7, 7]             256\n",
      "            ReLU-561            [-1, 128, 7, 7]               0\n",
      "          Conv2d-562             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-563           [-1, 1376, 7, 7]           2,752\n",
      "            ReLU-564           [-1, 1376, 7, 7]               0\n",
      "          Conv2d-565            [-1, 128, 7, 7]         176,128\n",
      "     BatchNorm2d-566            [-1, 128, 7, 7]             256\n",
      "            ReLU-567            [-1, 128, 7, 7]               0\n",
      "          Conv2d-568             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-569           [-1, 1408, 7, 7]           2,816\n",
      "            ReLU-570           [-1, 1408, 7, 7]               0\n",
      "          Conv2d-571            [-1, 128, 7, 7]         180,224\n",
      "     BatchNorm2d-572            [-1, 128, 7, 7]             256\n",
      "            ReLU-573            [-1, 128, 7, 7]               0\n",
      "          Conv2d-574             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-575           [-1, 1440, 7, 7]           2,880\n",
      "            ReLU-576           [-1, 1440, 7, 7]               0\n",
      "          Conv2d-577            [-1, 128, 7, 7]         184,320\n",
      "     BatchNorm2d-578            [-1, 128, 7, 7]             256\n",
      "            ReLU-579            [-1, 128, 7, 7]               0\n",
      "          Conv2d-580             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-581           [-1, 1472, 7, 7]           2,944\n",
      "            ReLU-582           [-1, 1472, 7, 7]               0\n",
      "          Conv2d-583            [-1, 128, 7, 7]         188,416\n",
      "     BatchNorm2d-584            [-1, 128, 7, 7]             256\n",
      "            ReLU-585            [-1, 128, 7, 7]               0\n",
      "          Conv2d-586             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-587           [-1, 1504, 7, 7]           3,008\n",
      "            ReLU-588           [-1, 1504, 7, 7]               0\n",
      "          Conv2d-589            [-1, 128, 7, 7]         192,512\n",
      "     BatchNorm2d-590            [-1, 128, 7, 7]             256\n",
      "            ReLU-591            [-1, 128, 7, 7]               0\n",
      "          Conv2d-592             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-593           [-1, 1536, 7, 7]           3,072\n",
      "            ReLU-594           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-595            [-1, 128, 7, 7]         196,608\n",
      "     BatchNorm2d-596            [-1, 128, 7, 7]             256\n",
      "            ReLU-597            [-1, 128, 7, 7]               0\n",
      "          Conv2d-598             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-599           [-1, 1568, 7, 7]           3,136\n",
      "            ReLU-600           [-1, 1568, 7, 7]               0\n",
      "          Conv2d-601            [-1, 128, 7, 7]         200,704\n",
      "     BatchNorm2d-602            [-1, 128, 7, 7]             256\n",
      "            ReLU-603            [-1, 128, 7, 7]               0\n",
      "          Conv2d-604             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-605           [-1, 1600, 7, 7]           3,200\n",
      "            ReLU-606           [-1, 1600, 7, 7]               0\n",
      "          Conv2d-607            [-1, 128, 7, 7]         204,800\n",
      "     BatchNorm2d-608            [-1, 128, 7, 7]             256\n",
      "            ReLU-609            [-1, 128, 7, 7]               0\n",
      "          Conv2d-610             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-611           [-1, 1632, 7, 7]           3,264\n",
      "            ReLU-612           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-613            [-1, 128, 7, 7]         208,896\n",
      "     BatchNorm2d-614            [-1, 128, 7, 7]             256\n",
      "            ReLU-615            [-1, 128, 7, 7]               0\n",
      "          Conv2d-616             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-617           [-1, 1664, 7, 7]           3,328\n",
      "            ReLU-618           [-1, 1664, 7, 7]               0\n",
      "          Conv2d-619            [-1, 128, 7, 7]         212,992\n",
      "     BatchNorm2d-620            [-1, 128, 7, 7]             256\n",
      "            ReLU-621            [-1, 128, 7, 7]               0\n",
      "          Conv2d-622             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-623           [-1, 1696, 7, 7]           3,392\n",
      "            ReLU-624           [-1, 1696, 7, 7]               0\n",
      "          Conv2d-625            [-1, 128, 7, 7]         217,088\n",
      "     BatchNorm2d-626            [-1, 128, 7, 7]             256\n",
      "            ReLU-627            [-1, 128, 7, 7]               0\n",
      "          Conv2d-628             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-629           [-1, 1728, 7, 7]           3,456\n",
      "            ReLU-630           [-1, 1728, 7, 7]               0\n",
      "          Conv2d-631            [-1, 128, 7, 7]         221,184\n",
      "     BatchNorm2d-632            [-1, 128, 7, 7]             256\n",
      "            ReLU-633            [-1, 128, 7, 7]               0\n",
      "          Conv2d-634             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-635           [-1, 1760, 7, 7]           3,520\n",
      "            ReLU-636           [-1, 1760, 7, 7]               0\n",
      "          Conv2d-637            [-1, 128, 7, 7]         225,280\n",
      "     BatchNorm2d-638            [-1, 128, 7, 7]             256\n",
      "            ReLU-639            [-1, 128, 7, 7]               0\n",
      "          Conv2d-640             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-641           [-1, 1792, 7, 7]           3,584\n",
      "            ReLU-642           [-1, 1792, 7, 7]               0\n",
      "          Conv2d-643            [-1, 128, 7, 7]         229,376\n",
      "     BatchNorm2d-644            [-1, 128, 7, 7]             256\n",
      "            ReLU-645            [-1, 128, 7, 7]               0\n",
      "          Conv2d-646             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-647           [-1, 1824, 7, 7]           3,648\n",
      "            ReLU-648           [-1, 1824, 7, 7]               0\n",
      "          Conv2d-649            [-1, 128, 7, 7]         233,472\n",
      "     BatchNorm2d-650            [-1, 128, 7, 7]             256\n",
      "            ReLU-651            [-1, 128, 7, 7]               0\n",
      "          Conv2d-652             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-653           [-1, 1856, 7, 7]           3,712\n",
      "            ReLU-654           [-1, 1856, 7, 7]               0\n",
      "          Conv2d-655            [-1, 128, 7, 7]         237,568\n",
      "     BatchNorm2d-656            [-1, 128, 7, 7]             256\n",
      "            ReLU-657            [-1, 128, 7, 7]               0\n",
      "          Conv2d-658             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-659           [-1, 1888, 7, 7]           3,776\n",
      "            ReLU-660           [-1, 1888, 7, 7]               0\n",
      "          Conv2d-661            [-1, 128, 7, 7]         241,664\n",
      "     BatchNorm2d-662            [-1, 128, 7, 7]             256\n",
      "            ReLU-663            [-1, 128, 7, 7]               0\n",
      "          Conv2d-664             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-665           [-1, 1920, 7, 7]           3,840\n",
      "            ReLU-666           [-1, 1920, 7, 7]               0\n",
      "          Conv2d-667            [-1, 128, 7, 7]         245,760\n",
      "     BatchNorm2d-668            [-1, 128, 7, 7]             256\n",
      "            ReLU-669            [-1, 128, 7, 7]               0\n",
      "          Conv2d-670             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-671           [-1, 1952, 7, 7]           3,904\n",
      "            ReLU-672           [-1, 1952, 7, 7]               0\n",
      "          Conv2d-673            [-1, 128, 7, 7]         249,856\n",
      "     BatchNorm2d-674            [-1, 128, 7, 7]             256\n",
      "            ReLU-675            [-1, 128, 7, 7]               0\n",
      "          Conv2d-676             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-677           [-1, 1984, 7, 7]           3,968\n",
      "            ReLU-678           [-1, 1984, 7, 7]               0\n",
      "          Conv2d-679            [-1, 128, 7, 7]         253,952\n",
      "     BatchNorm2d-680            [-1, 128, 7, 7]             256\n",
      "            ReLU-681            [-1, 128, 7, 7]               0\n",
      "          Conv2d-682             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-683           [-1, 2016, 7, 7]           4,032\n",
      "            ReLU-684           [-1, 2016, 7, 7]               0\n",
      "          Conv2d-685            [-1, 128, 7, 7]         258,048\n",
      "     BatchNorm2d-686            [-1, 128, 7, 7]             256\n",
      "            ReLU-687            [-1, 128, 7, 7]               0\n",
      "          Conv2d-688             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-689           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-690           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-691            [-1, 128, 7, 7]         262,144\n",
      "     BatchNorm2d-692            [-1, 128, 7, 7]             256\n",
      "            ReLU-693            [-1, 128, 7, 7]               0\n",
      "          Conv2d-694             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-695           [-1, 2080, 7, 7]           4,160\n",
      "            ReLU-696           [-1, 2080, 7, 7]               0\n",
      "          Conv2d-697            [-1, 128, 7, 7]         266,240\n",
      "     BatchNorm2d-698            [-1, 128, 7, 7]             256\n",
      "            ReLU-699            [-1, 128, 7, 7]               0\n",
      "          Conv2d-700             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-701           [-1, 2112, 7, 7]           4,224\n",
      "            ReLU-702           [-1, 2112, 7, 7]               0\n",
      "          Conv2d-703            [-1, 128, 7, 7]         270,336\n",
      "     BatchNorm2d-704            [-1, 128, 7, 7]             256\n",
      "            ReLU-705            [-1, 128, 7, 7]               0\n",
      "          Conv2d-706             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-707           [-1, 2144, 7, 7]           4,288\n",
      "            ReLU-708           [-1, 2144, 7, 7]               0\n",
      "          Conv2d-709            [-1, 128, 7, 7]         274,432\n",
      "     BatchNorm2d-710            [-1, 128, 7, 7]             256\n",
      "            ReLU-711            [-1, 128, 7, 7]               0\n",
      "          Conv2d-712             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-713           [-1, 2176, 7, 7]           4,352\n",
      "            ReLU-714           [-1, 2176, 7, 7]               0\n",
      "          Conv2d-715            [-1, 128, 7, 7]         278,528\n",
      "     BatchNorm2d-716            [-1, 128, 7, 7]             256\n",
      "            ReLU-717            [-1, 128, 7, 7]               0\n",
      "          Conv2d-718             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-719           [-1, 2208, 7, 7]           4,416\n",
      "            ReLU-720           [-1, 2208, 7, 7]               0\n",
      "          Conv2d-721            [-1, 128, 7, 7]         282,624\n",
      "     BatchNorm2d-722            [-1, 128, 7, 7]             256\n",
      "            ReLU-723            [-1, 128, 7, 7]               0\n",
      "          Conv2d-724             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-725           [-1, 2240, 7, 7]           4,480\n",
      "            ReLU-726           [-1, 2240, 7, 7]               0\n",
      "          Conv2d-727            [-1, 128, 7, 7]         286,720\n",
      "     BatchNorm2d-728            [-1, 128, 7, 7]             256\n",
      "            ReLU-729            [-1, 128, 7, 7]               0\n",
      "          Conv2d-730             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-731           [-1, 2272, 7, 7]           4,544\n",
      "            ReLU-732           [-1, 2272, 7, 7]               0\n",
      "          Conv2d-733            [-1, 128, 7, 7]         290,816\n",
      "     BatchNorm2d-734            [-1, 128, 7, 7]             256\n",
      "            ReLU-735            [-1, 128, 7, 7]               0\n",
      "          Conv2d-736             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-737           [-1, 2304, 7, 7]           4,608\n",
      "            ReLU-738           [-1, 2304, 7, 7]               0\n",
      "          Conv2d-739            [-1, 128, 7, 7]         294,912\n",
      "     BatchNorm2d-740            [-1, 128, 7, 7]             256\n",
      "            ReLU-741            [-1, 128, 7, 7]               0\n",
      "          Conv2d-742             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-743           [-1, 2336, 7, 7]           4,672\n",
      "            ReLU-744           [-1, 2336, 7, 7]               0\n",
      "          Conv2d-745            [-1, 128, 7, 7]         299,008\n",
      "     BatchNorm2d-746            [-1, 128, 7, 7]             256\n",
      "            ReLU-747            [-1, 128, 7, 7]               0\n",
      "          Conv2d-748             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-749           [-1, 2368, 7, 7]           4,736\n",
      "            ReLU-750           [-1, 2368, 7, 7]               0\n",
      "          Conv2d-751            [-1, 128, 7, 7]         303,104\n",
      "     BatchNorm2d-752            [-1, 128, 7, 7]             256\n",
      "            ReLU-753            [-1, 128, 7, 7]               0\n",
      "          Conv2d-754             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-755           [-1, 2400, 7, 7]           4,800\n",
      "            ReLU-756           [-1, 2400, 7, 7]               0\n",
      "          Conv2d-757            [-1, 128, 7, 7]         307,200\n",
      "     BatchNorm2d-758            [-1, 128, 7, 7]             256\n",
      "            ReLU-759            [-1, 128, 7, 7]               0\n",
      "          Conv2d-760             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-761           [-1, 2432, 7, 7]           4,864\n",
      "            ReLU-762           [-1, 2432, 7, 7]               0\n",
      "          Conv2d-763            [-1, 128, 7, 7]         311,296\n",
      "     BatchNorm2d-764            [-1, 128, 7, 7]             256\n",
      "            ReLU-765            [-1, 128, 7, 7]               0\n",
      "          Conv2d-766             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-767           [-1, 2464, 7, 7]           4,928\n",
      "            ReLU-768           [-1, 2464, 7, 7]               0\n",
      "          Conv2d-769            [-1, 128, 7, 7]         315,392\n",
      "     BatchNorm2d-770            [-1, 128, 7, 7]             256\n",
      "            ReLU-771            [-1, 128, 7, 7]               0\n",
      "          Conv2d-772             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-773           [-1, 2496, 7, 7]           4,992\n",
      "            ReLU-774           [-1, 2496, 7, 7]               0\n",
      "          Conv2d-775            [-1, 128, 7, 7]         319,488\n",
      "     BatchNorm2d-776            [-1, 128, 7, 7]             256\n",
      "            ReLU-777            [-1, 128, 7, 7]               0\n",
      "          Conv2d-778             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-779           [-1, 2528, 7, 7]           5,056\n",
      "            ReLU-780           [-1, 2528, 7, 7]               0\n",
      "          Conv2d-781            [-1, 128, 7, 7]         323,584\n",
      "     BatchNorm2d-782            [-1, 128, 7, 7]             256\n",
      "            ReLU-783            [-1, 128, 7, 7]               0\n",
      "          Conv2d-784             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-785           [-1, 2560, 7, 7]           5,120\n",
      "            ReLU-786           [-1, 2560, 7, 7]               0\n",
      "          Conv2d-787            [-1, 128, 7, 7]         327,680\n",
      "     BatchNorm2d-788            [-1, 128, 7, 7]             256\n",
      "            ReLU-789            [-1, 128, 7, 7]               0\n",
      "          Conv2d-790             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-791           [-1, 2592, 7, 7]           5,184\n",
      "            ReLU-792           [-1, 2592, 7, 7]               0\n",
      "          Conv2d-793            [-1, 128, 7, 7]         331,776\n",
      "     BatchNorm2d-794            [-1, 128, 7, 7]             256\n",
      "            ReLU-795            [-1, 128, 7, 7]               0\n",
      "          Conv2d-796             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-797           [-1, 2624, 7, 7]           5,248\n",
      "            ReLU-798           [-1, 2624, 7, 7]               0\n",
      "          Conv2d-799            [-1, 128, 7, 7]         335,872\n",
      "     BatchNorm2d-800            [-1, 128, 7, 7]             256\n",
      "            ReLU-801            [-1, 128, 7, 7]               0\n",
      "          Conv2d-802             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-803           [-1, 2656, 7, 7]           5,312\n",
      "            ReLU-804           [-1, 2656, 7, 7]               0\n",
      "          Conv2d-805            [-1, 128, 7, 7]         339,968\n",
      "     BatchNorm2d-806            [-1, 128, 7, 7]             256\n",
      "            ReLU-807            [-1, 128, 7, 7]               0\n",
      "          Conv2d-808             [-1, 32, 7, 7]          36,864\n",
      "     _DenseBlock-809           [-1, 2688, 7, 7]               0\n",
      "     BatchNorm2d-810           [-1, 2688, 7, 7]           5,376\n",
      "          Linear-811                   [-1, 10]          26,890\n",
      "================================================================\n",
      "Total params: 31,544,298\n",
      "Trainable params: 31,544,298\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 605.42\n",
      "Params size (MB): 120.33\n",
      "Estimated Total Size (MB): 726.33\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "model = densenet201().to(device)\n",
    "\n",
    "\n",
    "\n",
    "#pretesting model for shape\n",
    "x=torch.randn(64,3,224,224)\n",
    "x=x.to(device)\n",
    "print(x.shape)\n",
    "print(model(x).shape)\n",
    "print(\"model shape ready\")\n",
    "print(summary(model, input_size=(3, 224, 224)))\n",
    "#initailise network\n",
    "\n",
    "\n",
    "#loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(\"model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592a8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test defined\n",
      "early stop defined\n"
     ]
    }
   ],
   "source": [
    "# This is the testing part\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)\n",
    "\n",
    "def test(model, test_loader, istest= False, doprint=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Prediction\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss=criterion(output,target)\n",
    "            \n",
    "            # Compute loss & accuracy\n",
    "            test_loss+=loss.item()*data.size(0)\n",
    "\n",
    "            \n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
    "            \n",
    "            #print(\"pred={} , target={} , judge={}\".format(pred.item(),target.item(),pred.eq(target.view_as(pred)).sum().item()))\n",
    "\n",
    "            \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "    # Log testing info\n",
    "    if istest and doprint:\n",
    "        \n",
    "        print('Loss: {}   Accuracy: {}/{} ({:.3f}%)'.format(test_loss,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "        print(\"Total parameters: {}\".format(get_n_params(model)))\n",
    "    elif doprint:\n",
    "        print('Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100.000 * correct / len(test_loader.dataset)))\n",
    "    return 100.000 * correct / len(test_loader.dataset)\n",
    "        \n",
    "\n",
    "print(\"test defined\")\n",
    "\n",
    "def testshouldearlystop(acclist,minepoch,epochwindow,accwindow):\n",
    "    runlen=len(acclist)\n",
    "    if(runlen<minepoch):\n",
    "        return False\n",
    "    elif(acclist[-1]>acclist[-2]):\n",
    "        return False\n",
    "    \n",
    "    watchwindow=acclist[-epochwindow:]\n",
    "    shouldjump=True\n",
    "    sum=0\n",
    "    for i in watchwindow:\n",
    "        sum+=i\n",
    "    avg = sum/epochwindow\n",
    "    for i in watchwindow:\n",
    "        if abs(i-avg)>(accwindow):\n",
    "            shouldjump=False\n",
    "    return shouldjump\n",
    "print(\"early stop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49606c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard_string:\n",
      "runs//DenseNet201_sp20211104101623\n",
      "grandstore_string\n",
      "grandstore/cifar10_DenseNet201_sp20211104101623.pkl\n"
     ]
    }
   ],
   "source": [
    "now=datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "tensorboard_string=\"runs/\"+\"/\"+model_name+dt_string\n",
    "grandstore_string=\"grandstore/\"+dataset_name+\"_\"+model_name+dt_string+\".pkl\"\n",
    "print(\"tensorboard_string:\")\n",
    "print(tensorboard_string)\n",
    "print(\"grandstore_string\")\n",
    "print(grandstore_string)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(tensorboard_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3316dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the training part\n",
    "\n",
    "# Grand_store={\n",
    "#     'total_epoch_run':-1\n",
    "#     'topmodels':-1\n",
    "#     'lastmodel':-1\n",
    "#     'acclog':[]\n",
    "#     'maxacc':-1\n",
    "#     'minacc':101\n",
    "# }\n",
    "# train_epoch={\n",
    "#     \"numofepoch\":-1\n",
    "#     \"accuracy\":-1\n",
    "#     \"model_state\":model.state_dict(),\n",
    "#     \"optim_state\":optimizer.state_dict(),\n",
    "#     \"totaltrain_loss\":totaltrain_loss,\n",
    "#     \"totalvalid_loss\":totalvalid_loss\n",
    "# }\n",
    "\n",
    "def training(max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "             minepoch=30,epochwindow=10,accwindow=0.35):\n",
    "\n",
    "    grandstore['total_epoch_run']=0\n",
    "    grandstore['topmodels']=[]\n",
    "    grandstore['acclog']=[]\n",
    "    grandstore['maxacc']=-1\n",
    "    grandstore['minacc']=101\n",
    "    \n",
    "    for epoch in range(0,max_epoch):\n",
    "        \n",
    "        grandstore['total_epoch_run']=epoch+1\n",
    "        \n",
    "        train_epoch={\n",
    "        \"numofepoch\":grandstore['total_epoch_run']\n",
    "        }\n",
    "    \n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        print(\"Running epoch: {}\".format(epoch+1))\n",
    "\n",
    "        model.train()\n",
    "        totaltrain_loss=0\n",
    "        \n",
    "        #this is the training part\n",
    "        for data,target in tqdm(train_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            totaltrain_loss += train_loss\n",
    "\n",
    "        #this is the validation part\n",
    "        model.eval()\n",
    "        totalvalid_loss=0;\n",
    "        correct = 0\n",
    "        for data,target in tqdm(val_dataloader):\n",
    "            data=data.to(device=device)\n",
    "            target=target.to(device=device)\n",
    "            output=model(data)\n",
    "            loss=criterion(output,target)\n",
    "            valid_loss=loss.item()*data.size(0)\n",
    "            #train_loss = train_loss/len(train_dataloader.dataset)\n",
    "            #valid_loss = valid_loss/len(val_dataloader.dataset)\n",
    "            totalvalid_loss+=valid_loss\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in t\n",
    "        \n",
    "\n",
    "        training_accuracy=100. * correct / len(val_dataloader.dataset)\n",
    "        train_epoch[\"accuracy\"]=training_accuracy\n",
    "        train_epoch[\"totaltrain_loss\"]=totaltrain_loss\n",
    "        train_epoch[\"totalvalid_loss\"]=totalvalid_loss\n",
    "        \n",
    "        #writings to the GrandStore\n",
    "        \n",
    "        grandstore['acclog'].append(training_accuracy)\n",
    "        \n",
    "        if training_accuracy < grandstore['minacc']:\n",
    "            grandstore['minacc'] = training_accuracy\n",
    "            \n",
    "        if training_accuracy > grandstore['maxacc']:\n",
    "            grandstore['maxacc'] = training_accuracy\n",
    "        \n",
    "\n",
    "        if epoch < top_accuracy_track:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'].append((training_accuracy,thisepochtestresult,epoch+1,train_epoch))\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        elif training_accuracy > grandstore['topmodels'][0][0]:\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['topmodels'][0]=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            #if error print this\n",
    "            grandstore['topmodels'].sort()\n",
    "\n",
    "        if epoch == (max_epoch-1):\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "                     \n",
    "        writer.add_scalar('Training Loss',totaltrain_loss,global_step = epoch)\n",
    "        writer.add_scalar('Valid Loss',totalvalid_loss,global_step = epoch)\n",
    "        writer.add_scalar('Accuracy',training_accuracy,global_step = epoch)\n",
    "        \n",
    "        print('Accuracy: {:.3f}'.format(training_accuracy))\n",
    "        print('Training Loss: {:.4f} \\tValidation Loss: {:.4f}\\n'.format(totaltrain_loss, totalvalid_loss))\n",
    "        \n",
    "        #early stopping criteria\n",
    "        if(testshouldearlystop(acclist=grandstore['acclog'],\n",
    "                               minepoch = minepoch,\n",
    "                               epochwindow = epochwindow,\n",
    "                               accwindow = accwindow)):\n",
    "            print(\"early stop occured!!\")\n",
    "            thisepochtestresult=test(model,test_dataloader,istest = True,doprint=False)\n",
    "            grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)\n",
    "            return grandstore\n",
    "    \n",
    "    return grandstore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ec9b554224917a9158120fcb5fd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d01b23722b24d4bbadfaab738d73f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5187e0e5abef4d88963b16e244c93eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.480\n",
      "Training Loss: 93909715.5866 \tValidation Loss: 10572777.3914\n",
      "\n",
      "Running epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe24794dd1e4478b34aeac50a3fb73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6029ff838ba44b5abddfd51f7d5d1ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c0dfc613c341d7a6cffd00c8b7c00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOP_ACCURACY_TRACK = 5\n",
    "# max_epoch=120, top_accuracy_track=3, grandstore={},\n",
    "# minepoch=30,epochwindow=10,accwindow=0.35\n",
    "\n",
    "Grandstore=training(max_epoch=120,\n",
    "                    minepoch=30,\n",
    "                    top_accuracy_track=TOP_ACCURACY_TRACK,\n",
    "                    epochwindow=10,\n",
    "                    accwindow=0.25                  \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Run {} epoch(s)\".format(Grandstore['total_epoch_run']))\n",
    "\n",
    "plt.plot(*[range(1,Grandstore['total_epoch_run']+1)],Grandstore['acclog'])\n",
    "print(\"Accuracy MIN: {} / MAX: {}\".format(Grandstore['minacc'],Grandstore['maxacc']))\n",
    "print()\n",
    "print(\"Top {} performing epochs:\".format(TOP_ACCURACY_TRACK))\n",
    "\n",
    "\n",
    "gstm=Grandstore['topmodels']\n",
    "for i in range(TOP_ACCURACY_TRACK):\n",
    "    easy=gstm[TOP_ACCURACY_TRACK-i-1]\n",
    "    print(\"#{} epoch {}\\t||train_acc {}%\\t||test {}%\".format(i+1,easy[2],easy[0],easy[1]))\n",
    "print()\n",
    "print(\"Last epoch:\")\n",
    "lsmd=Grandstore['lastmodel']\n",
    "print(\"epoch {}\\t||train_acc {}%\\t||test {}%\".format(Grandstore['total_epoch_run'],lsmd[0],lsmd[1]))\n",
    "      \n",
    "print()\n",
    "print(\"The model has parameters: {}\".format(get_n_params(model)))\n",
    "#grandstore['lastmodel']=((training_accuracy,train_epoch,thisepochtestresult))\n",
    "# grandstore['lastmodel']=(training_accuracy,thisepochtestresult,epoch+1,train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac30dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writings done!\n",
      "Files at: grandstore/cifar10_DenseNet121_sp20211030010454.pkl\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "f1=open(grandstore_string,\"wb\")\n",
    "pickle.dump(Grandstore,f1)\n",
    "f1.close()\n",
    "\n",
    "print(\"writings done!\")\n",
    "print(\"Files at: \"+grandstore_string)\n",
    "# with open(grandstore_string, 'rb') as file:\n",
    "#     myvar = pickle.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ddef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
